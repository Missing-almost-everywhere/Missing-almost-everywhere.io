{
  "hash": "070256d3f2a4d86180a2bfd2a92b3ce9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Mixture of K scaled and shifted t-distributions\"\nformat:\n  html:\n    code-fold: true\n---\n\n\n\n# Introduktion\nIn this project, I will examine a mixture of K scaled and shifted t-distributions.\n\nThe scaled and shifted t-distributions have the following PDF.\n$$\nf(x|\\mu,\\sigma,\\nu)=\\frac{\\Gamma((\\nu+1)/2)}{\\sqrt{\\pi\\nu\\sigma^2}\\Gamma(\\nu/2)}(1+\\frac{(x-\\mu)^2}{\\nu\\sigma^2})^{-(\\nu+1)/2}\n$$\n\n$\\mu\\in \t\\mathbb{R}$, $\\sigma>0$ and $\\nu>0$\n\nWith the mixed distribution.\n\n$$\\sum_{k=1}^{K} \\pi_k f(x|\\mu_k,\\sigma_k,\\nu_k)$$\n\n# A littel theory\nThe EM algorithm consists of a two-step optimization process to maximize the likelihood. In the Expectation (E) step, the posterior probabilities are calculated. In the Maximization (M) step, the likelihood is maximized to find the parameters, given the posterior probabilities.\n\n\n## Posterior probabilities (Expectation step)\nThe posterior probabilities correspond to the probability that an outcome is drawn from a given distribution.\n$$\nP(Z=\\pi_k|x_i)=\\frac{P(x_i|Z=\\pi_k)P(Z_i=\\pi_k)}{P(x_i)}=\\frac{\\pi_kf(x_i|\\mu_k,\\sigma_k,\\nu_k)}{\\sum_{k=1}^{K}{\\pi_k}f(x_i|\\mu_k,\\sigma_k,\\nu_k)}=w(x_i)\n$$\n\n\n\n## Maximization step M step:\nIn the M step, the coefficients $\\{\\pi_k,\\theta_k\\}$  are estimated, where $\\theta_k=\\{\\mu_k,\\sigma_k,\\nu_k\\}$\n\n\n# Log Likelyhood\nThe simplification of writing makes it possible to express the density in the form $f(x|\\pi,\\theta)\\sum_{k=1}^K \\pi_kf(x|\\theta_k)$\nwhere $\\theta_i=\\{\\mu_i,\\sigma_i^2,\\nu_i\\}$. \n\n\nMaking the the likelihood\n$$\nL(\\pi_k,\\theta_k)=\\sum_{i=1}^{N}\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k)\n$$\n\nand the negative log likelihood.\n$$\nl(\\pi_k,\\theta_k)=-\\sum_{i=1}^{N} log(\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k))\n$$\n\nFor the estimation of $\\pi_k$ Lagrange optimization can be used on the negative log-likelihood. It should be noted that  $\\sum_{k=1}^{K}\\pi_k =1$. corresponds to the linear constraint.\n\nThe Lagrange function becomes\n$$\n\\mathcal{L}(\\pi_k)=-\\sum_{i=1}^{N} log(\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k))-(\\lambda\\sum_{k=1}^{K}\\pi_k-1)\n$$\n\nLagrange optimization requires the objective function to be convex. Convexity can be checked using the second derivative.\n\nFist derivative.\n$$\n\\frac{\\partial l(\\pi_k,\\theta_k)}{\\partial \\pi_k}=-\\sum_{i=1}^{n}\\frac{f(x_i|\\theta_k)}{\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k)}\n$$\n\nSecond derivative.\n$$\n\\frac{\\partial^2 l(\\pi_k,\\theta_k)}{\\partial^2 \\pi_k}=\\sum_{i=1}^{n}\\frac{f(x_i|\\theta_k)^2}{(\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k))^2}>0\n$$\n\nSince the second derivative of the negative log-likelihood is positive for all values of $X$, the function is convex. This follows from the fact that $f(x_i,\\theta_k),\\pi_k>0$ by the definition of probabilities.\n\n### Solving the lagrancian.\n\n$$\n\\mathcal{L}(\\pi_k)=-\\sum_{i=1}^{N} log(\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k))-(\\lambda\\sum_{k=1}^{K}\\pi_k-1)\n$$\n\nFist derivative.\n$$\n\\frac{\\partial\\mathcal{L}(\\pi_k)}{\\partial \\pi_k}=\\sum_{i=1}^{N}\\frac{f(x_i|\\theta_k)}{\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k)} +\\lambda =\\sum_{i=1}^{N}w(x_i)\\frac{1}{\\pi_k} +\\lambda =0\n$$\n\nThe left side is the same as the weight, which is the posterior probabilities multiplied by $\\pi_k$. \n\nThis gives a condition for the optimal value.\n$$\n\\sum_{i=1}^{N}w(x_i)+\\pi_k\\lambda = 0 => -\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i) =\\lambda\n$$\n\nBut since $\\sum_{i=1}^{N}w(x_i)+\\pi_k\\lambda = 0$  must hold for all $k$\nand that $\\sum_{k=1}^{K}w(x_i)=1$, since these are the posterior probabilities.\nOne can derive.\n$$\n\\begin{aligned}\n0=\\sum_{k=1}^{K}(\\sum_{i=1}^{N}w(x_i)+\\pi_k\\lambda )=\\sum_{k=1}^{K}\\sum_{i=1}^{N}w(x_i)+\\sum_{k=1}^{K}\\pi_k\\lambda =\\sum_{k=1}^{K}\\sum_{i=1}^{N}w(x_i)+\\lambda \\\\ = \\sum_{k=1}^{K}\\sum_{i=1}^{N}w(x_i) -\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i)=\\sum_{i=1}^{N}\\sum_{k=1}^{K}w(x_i)\n-\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i) \\\\\n\\sum_{i=1}^{N}\\sum_{k=1}^{K}w(x_i)\n-\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i)=N-\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i)\\\\ => \\hat{\\pi}_k= \\frac{1}{N}\\sum_{i=1}^{N}w(x_i)\n\\end{aligned}\n$$\n\nThe estimate for the probability of drawing from a given distribution is the average of the weights.\n\n## Estiamtion of $\\theta_k=\\{\\mu_k,\\sigma_k,\\nu_k\\}$\nThe estimation of the parameters $\\theta_k={\\mu_k,\\sigma_k,\\nu_k}$\nis done by minimizing the negative log-likelihood.\n\nAs demonstrated below, the problem is that there is no closed-form solution. This arises from the fact that there is a sum in the denominator of the gradient.\n\n\n# Gradient of $\\theta_k$\n$$\n\\frac{\\partial l(\\theta_k)  }{\\partial \\theta_k} =- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\theta_k)}\\frac{\\partial f(x_i|\\theta_k)  }{\\partial \\theta_k}\n$$\n\nNote that in the above, it is almost the sum of the weights, so it can be useful to express$\\frac{\\partial f(x_i|\\theta_k)}{\\partial\\theta_k}$ in terms of $f(x_i|\\theta_k)$,  if possible.\n\nSince $\\theta_k$ is a simplified notation for the parameters $\\{\\mu_k,\\sigma_k,\\nu_k\\}$, which are the parameters of interest to estimate, the derivatives with respect to these parameters will be calculated and substituted into the formula above.\n\nIn the next section, \n$\\frac{\\partial f(x_i|\\theta_k)  }{\\partial \\theta_k}$ will be found with respect to \n$\\{\\mu_k,\\sigma_k,\\nu_k\\}$  and substituted into $\\frac{\\partial l(\\theta_k)  }{\\partial \\theta_k}$ to give a formula for $\\frac{\\partial l(\\theta_k)  }{\\partial \\theta_k}$\nto provide a formula for $\\frac{\\partial l(\\theta_k)  }{\\partial \\theta_k}$.\n\n\n# Finding the gradient of negativ log likelyhood for $\\nu_k$\n\n$$\n\\begin{aligned}\n\\frac{\\partial f(x_i|\\mu_k,\\sigma_k,\\nu_k)}{\\partial \\mu_k}=(\\nu_k+1)(\\frac{x_i-\\mu_k}{\\nu_k\\sigma_k^2})(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{-1}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\ =(\\nu_k+1)\\frac{x_i-\\mu_k}{\\nu_k\\sigma_k^2+(x_i-\\mu_k)^2}f(x_i|\\mu_k,\\sigma_k,\\nu_k)\n\\end{aligned}\n$$\n\nLeading to.\n\n$$\n\\begin{aligned}\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\mu_k} =- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)}\\frac{\\partial f(x_i|\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\mu_k}\\\\ =-(\\nu_k+1)\\sum_{i=1}^{N}w(x_i)\\frac{x_i-\\mu_k}{\\nu_k\\sigma_k^2+(x_i-\\mu_k)^2}\n\\end{aligned}\n$$\n\nNote that a closed form does not seem possible since $x_i$\nappears in the denominator, and the sum cannot be split. However, the gradient can still be used for faster implementation.\n\n## Finding the gradient of negativ log likelyhood for $\\sigma_k$\n\n\n$$\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k} =- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_k,\\sigma_k\\nu_k)}\\frac{\\partial f(x_i|\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k}\n$$\n\n$$\n\\begin{aligned}\n\\frac{\\partial f(x_i|\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k}=\\frac{-1}{\\sigma}f(x_i|\\mu_k,\\sigma_k,\\nu_k)+\\frac{(\\nu_k+1)(x-\\mu_k)^2}{\\nu_k\\sigma_k^3}\\frac{\\nu_k\\sigma_k^2}{\\nu_k\\sigma_k^2+(x-\\mu_k)^2}f(x_i|\\mu_k,\\sigma_k,\\nu_k)=\\\\\n\\frac{1}{\\sigma_k}f(x_i|\\mu_k,\\sigma_k,\\nu_k)(-1+\\frac{(v_k+1)(x_i-\\mu_k)^2}{\\nu_k\\sigma_k +(x_i-\\mu_k)^2})\n\\end{aligned}\n$$\n\nThis leads to the gradient\n\n$$\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k} = -\\sum_{i=1}^{N}w(x_i)(-1+\\frac{(\\nu_k+1)(x_i-\\mu_k)^2}{\\nu_k\\sigma_k +(x_i-\\mu_k)^2})\\frac{1}{\\sigma_k}\n$$\n\nSince the denominator cannot be split, the expression cannot be simplified.\n\n$$\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)}{\\partial \\nu_k}=- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_i,\\sigma_i\\nu_i)} \\frac{\\partial f(x|\\mu_k,\\sigma_k,\\nu_k) }{\\partial \\nu_k}\n$$\n\nIt can be useful to know that\n$$\n\\begin{aligned}\n\\frac{\\partial \\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}}{\\partial \\nu} = \\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma(\\nu_k/2)}-\\frac{\\Gamma((\\nu_k+1)/2)}{2\\Gamma(\\nu_k/2)^2}\\psi(\\nu_k/2)\\\\ =\\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma((\\nu_k+1)/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}-\\frac{\\psi(\\nu_k/2)}{2\\Gamma(\\nu_k/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}\n\\end{aligned}\n$$\n\nGiving the derivative.\n\n$$\n\\begin{align*}\n\\frac{\\partial f(x_i|\\mu_k,\\sigma_k^2,\\nu_k) }{\\partial \\nu_k}=-\\frac{1}{2\\nu_k}\\frac{\\Gamma((\\nu_k+1)/2)}{\\sqrt{\\pi\\nu_k\\sigma_k^2}\\Gamma(\\nu_k/2)}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{\\frac{-(\\nu_k+1)}{2}}\\\\\n+\\frac{(x_i-\\mu_k)^2(\\nu_k+1)}{\\nu_k\\sigma^2+(x_i-\\mu_k)^2}\\frac{1}{2\\nu_k}\\frac{\\Gamma((\\nu_k+1)/2)}{\\sqrt{\\pi\\nu_k\\sigma_k^2}\\Gamma(\\nu_k/2)}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{\\frac{-(\\nu_k+1)}{2}-1}\n\\\\\n+\\frac{\\partial \\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}}{\\partial \\nu_k}\\frac{1}{\\sqrt{\\pi\\nu_k\\sigma_k^2}}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k \\sigma_k^2})^{\\frac{-(\\nu_k+1)}{2}}\n\\\\ = \\\\\n-\\frac{1}{2\\nu_k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\\n+\\frac{(x_i-\\mu_k)^2(\\nu_k+1)}{\\nu_k\\sigma_k^2+(x_i-\\mu_k)^2}\\frac{1}{2\\nu_k}f(x|\\mu_k,\\sigma_k^2,\\nu_k)\n\\\\\n\\\\ +\\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma((\\nu_k+1)/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}\\frac{1}{\\sqrt{\\pi\\nu_k\\sigma_k^2}}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{-\\frac{\\nu_k+1}{2}}\\\\\n-\\frac{\\psi(\\nu_k/2)}{2\\Gamma(\\nu_k/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}\\frac{1}{\\sqrt{\\pi\\nu_k\\sigma_k^2}}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{-\\frac{\\nu_k+1}{2}}\\\\\n=\\\\\n-\\frac{1}{2\\nu_k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\\n+\\frac{(x_i-\\mu_k)^2(\\nu_k+1)}{\\nu_k\\sigma^2+(x_i-\\mu_k)^2}\\frac{1}{2\\nu_k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\\n+\\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma((\\nu_k+1)/2)}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)-\\frac{\\psi(\\nu_k/2)}{2\\Gamma(\\nu_k/2)}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\n\\end{align*}\n$$\n\nGiving the gradient of the likelihood.\n\n$$\n\\begin{aligned}\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)}{\\partial \\nu_k}=- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_i,\\sigma_i\\nu_i)} \\frac{\\partial f(x_i|\\mu_k,\\sigma_k^2,\\nu_k) }{\\partial \\nu_k}\\\\ = \n-\\sum_{i=1}^{N}\\frac{-1}{2}w(x_i)+\\frac{(\\nu_k+1)}{2}\\frac{(x_i-\\mu_k)^2}{ \\nu_k^2\\sigma_k^2}w(x_i)+\\frac{\\psi((\\nu+1)/2)}{2\\Gamma((\\nu+1)/2)}w(x_i)-\\frac{\\psi(\\nu/2)}{2\\Gamma(\\nu/2)}w(x_i)\\\\\n\\end{aligned}\n$$\n\nAll the gradients are computed. The goal is to obtain the maximum likelihood, but there is no closed-form solution. For some special cases, the functions might be approximated nicely. However, the gradients can still be used for gradient descent, which is the method I have chosen.\n\n# Gradient decent.\nGradient descent is used in the M step to optimize the parameters given the weights.\n\nTwo problems arise when using gradient descent. These are described below.\n\n## Undershooting the gradient out of domain.\nIf gradient descent undershoots and sets $\\nu_i<0$ ore $\\sigma_i<0$, the density is not defined. This is handled by setting a minimum value for the parameters $\\nu_i$ and $\\sigma_i$. This means that the minimum value for these parameters is not zero but a value close to zero.\n\nUndershooting can also cause problems when using built-in optimization tools. A pro tip is that if you encounter NaN values as output during optimization, it’s a good idea to start debugging with this issue in mind. In my experience, there is not always a built-in check for these cases.\n\n## Gradient explotion\nThis does not fix the problem of gradient explosion. What happens is that the gradient decent may take too large a step in either direction. To make matters worse, the sum in the expression for the gradients can cause the gradients to grow as the sample size increases, which can lead to gradient explosion.\n\nTo combat this, I have tried different strategies.\nBelow are the two method i tried described.\n\n\n## Normalising the gradient\n\nWhen analyzing the gradient, it's important to note that the sum is not divided by the size of $X$(denoted as $N$). This means that the absolute value of the gradient grows with $N$. If gradient descent is used with a fixed learning rate $\\alpha$, problems may arise where the gradient either undershoots or overshoots. In an implementation setting, this issue is very apparent. However, in real-world problems where the true parameters are unknown, it is impossible to know if the estimated values are accurate.\n\nThere are two methods to address this issue: one is to check the norm of the gradient to see if you are near a local optimum, and the other is to increase the number of iterations. I have chosen to normalize the gradient by dividing by $N$. This corresponds to choosing $\\alpha$ based on the size of $X$.\n\n\n## Gradient clipping\nGradient clipping works by setting a maximum value for the norm of the gradient. Specifically, if $|\\nabla l(X,\\pi,\\theta)|>c$ then the gradient is scaled to $\\nabla l=c*\\frac{\\nabla l}{|\\nabla l|}$. This allows you to set a maximum value for the norm of the gradient step.\n\nSince this method is based on the norm of the entire gradient, it would be beneficial to vary the clipping based on the type of parameter $\\{\\mu,\\sigma,\\nu\\}$., especially for $\\mu$. I have implemented a version where the clipping is based on different parameters.\n\n\nIn testing, it seems that gradient clipping is easy to use and works better. Therefore, in the implementation, gradient clipping is used.\n\nBelow is a summary of the gradient descent approach\n#Gradient decent.\nGradient descent is used in the M step to optimize the parameters given the weights.\n\nThere is no closed-form solution for many of the gradients. To make matters worse, the sum in the expression for the gradients can cause the gradients to grow as the sample size increases, potentially leading to exploding gradients. If gradient descent undershoots and sets \n$\\nu_i<0$ or $\\sigma_i<0$, the density is not defined. I have addressed the issue of undershooting by setting a minimum value for the parameters \n$\\nu_i$ and $\\sigma_i$. This means that the minimum is not zero but a value close to zero.\n\nHowever, this does not solve the problem of overshooting gradients. The solution to this issue is normalization. One strategy is to normalize the gradient by the sample size \nN. Another solution is to use gradient clipping. Gradient clipping works by setting a maximum value for the norm of the gradient. Specifically, if $|\\nabla l|>c$, then the gradient is set to \n$\\nabla l=c*\\frac{\\nabla l}{|\\nabla l|}$. This allows you to set a maximum value for the norm of the gradient step. Since this method is based on the norm of the entire gradient, it would be beneficial to vary the clipping based on the type of parameter $\\{\\mu,\\sigma,\\nu\\}$, especially for $\\mu$.\n\nI have implemented a version where the clipping is based on different parameter types. Specifically, if the gradient matrix column corresponding to a given parameter is normalized by its corresponding vector and scaled by a given constant for that parameter. If this explanation is confusing, refer to the function Gradient_clipping_vec in the Rcpp file, which is fairly self-explanatory.\n\nAdditionally, I have included a check to see if a steep step leads to smaller values in the objective function. If not, the gradient is scaled again by $\\alpha$.\n\n### Implentatsion notes for gradient decent\nSince the EM algorithm is an iterative process and R handles loops poorly, Rcpp is used to run C++ code for faster implementation. This is a common approach in many libraries.\n\n## Combining it all to one EM function.\nSince the EM algorithm is a two-step optimization process, it is not strictly necessary to find the smallest value in the M step. The critical aspect is that both steps lead to a decrease in the negative likelihood.\n\nIn terms of implementation, this affects the settings used for running gradient descent. For example, setting a maximum number of iterations too high can result in very long runtimes.\n\nI have not found any definitive guidelines for tuning these parameters in general settings, so I have made it possible for the end user to adjust them as needed.\n\n## Initialization\nThe EM algorithm requires starting parameters, and if they are too far off, it can affect the runtime. I have chosen to use the hard clustering technique, k-means++, which provides a set of partitions. In each of these partitions, the scaled and shifted t-distributions are fitted and used as the starting parameters.\n\nK-means++ works similarly to k-means, with the primary difference being in the initialization process. The goal of k-means is to minimize the objective function:\n\n$\\underset{\\mu}{\\arg\\min}\\sum_{k=1}^{K}\\sum_{i=1}^{N}|x_i-\\mu_k|^2$\nK-means++ improves on this by initially selecting one center randomly and then iteratively assigning points to the nearest center and updating the centers. It can be shown that this process will decrease the objective function, making it a greedy algorithm.\n\n\n# Potential problems\nK-means has the potential problem of local minima. This can be illustrated in two dimensions by assigning points to the four corners of a square and using three of them as starting positions, which can lead to poor initializations.\n\nThe issue of non-uniqueness for optima should not be problematic in our case, as we are dealing with continuous distributions (or at least it seems unlikely).\n\nK-means++ is also sensitive to outliers. This problem is present in the structure of the likelihood for mixture models as well. If there is a small cluster of outliers, the fitted t-distribution can become very sharp, giving a high likelihood to those points. This should be mitigated by ensuring that the probability of drawing from this distribution is very small. However, there is no guarantee that this is always the case. Essentially, this means that outliers may not be treated as such but rather as draws from a separate distribution.\n\nDespite these issues, the method based on k-means++ still seems like the best option. Otherwise, one can set the starting parameters manually.\n\nFor estimating the individual t-distributions, we use the condition \n$\\nu >2$. Given this, we can use the following results:\n\n$\\mathbb{E}[X]=\\mu$ and $Var(X)=\\sigma^2\\frac{\\nu}{\\nu-2}\\rightarrow 2\\frac{ Var(X)}{Var(X)-\\sigma^2}=\\nu$\nore \n$Var(X)=\\sigma^2\\frac{\\nu}{\\nu-2}\\rightarrow \\sqrt{Var(X)\\frac{\\nu-2}{\\nu}}=\\sigma$\n\nSince this is only for initialization, if the parameters for the t-distributions are overshot, the EM algorithm should correct for it. The advantage is that with a good estimate of the parameters \n$\\mu$ and \n$\\nu$, and $\\sigma$, standard optimization can be used to estimate the last parameter based on the log likelihood.\n\nThis method often fails, so it may be better to use grid search or manual tuning.\n\n## GridSearch Initialization\nIn most cases, the method of using K-means partitions to estimate $\\mu$ and using the proportions to estimate \n$\\pi$ seems reasonable. So, I have implemented a version where GridSearch is used for the parameters $\\sigma$ and $\\nu$, which is what caused problems before. By doing this, one gets the advantage of the simplicity and relative speed of using K-means while obtaining usable starting parameters for $\\sigma$ and $\\nu$.\n\n# Measurs\nIf one wants to compare models, the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) are available.\n\n\n# Code \nLibrays used.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required libraries\nlibrary(this.path) # find where you are\n\n# Note current script location\ncurrent_script_directory <- this.dir()\n# library's to Rcpp\nlibrary(\"Rcpp\")\nlibrary(\"RcppArmadillo\")\nlibrary(\"RcppGSL\")\nsourceCpp(\"compstat.cpp\")\n\nlibrary(ClusterR)\n#KMeans_rcpp #kmeans++ works well and is in Rcpp\n\n# for fiting scaled and shifted t-distribution\nlibrary(fitdistrplus)\nlibrary(MASS)\n\n\n# for paraleel gridseach in the intilasation\nlibrary(doParallel)\nlibrary(foreach)\n```\n:::\n\n\n\nMy own implented code i R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# build function in R\nlibrary(\"metRology\")\nlibrary(\"extraDistr\")\n\n#function for generating a vector of draws from a mixture T distribuations\nDraw_mixture_componet <- function(Pi=c(0.2, 0.5, 0.3),\n                                  Mu=c(0,25,50),\n                                  Sigma=c(1,2,1),\n                                  Nu=c(3,4,4),\n                                  N_samples=1000){\n  #sample mixture componets\n  mixture_componoets_sample <- sample(x = 1:length(Pi), size = N_samples, replace = TRUE, prob = Pi)\n  \n  draws=c(rep(0,N_samples))\n  for (i in 1:N_samples){\n    d_i=sample(x = 1:length(Pi), size = 1, replace = TRUE, prob = Pi) #draw_index = d_i\n    draws[i]=rt.scaled(n = 1, df = Nu[d_i], mean = Mu[d_i], sd = Sigma[d_i])\n  }\n  return(draws)\n}\n\n# itnernall function of initialation\ninitialize_if_na <- function(var,# variabels\n                             K# numer of partions\n                             ) {\n  if (any(is.na(var))) {\n    return(rep(NA, K))\n  }\n  return(var)\n}\n# estimation of PI by taking proportion of partions\nEstiame_Pi_from_partions= function(Partions_vec,K){\n  Pi=c(rep(0,K))\n  for (i in 1:K){\n    Mid_vec=Partions_vec==i\n    Pi[i]=sum(Mid_vec)/length(Partions_vec)\n    }\n  return(Pi)\n}\n\n# This is not the best parameter, but for now its better than guissing.\n# it seam to work well for well seperated distribuations\nIntiall_parameter_optimasation<-function(X#Data from parametasion\n    ){\n  Mu<-mean(X)\n  variance=var(X)\n  fn1<-function(s){sum(-(dlst(X, df=abs(s), mu = Mu, sigma = sqrt(variance*(abs(s)-2)/abs(s)) , log = TRUE)))}\n  result <- optim(par = 5, fn = fn1, method = \"L-BFGS-B\",\n  lower = c(2+0.05))\n \n  Nu_val=(abs(result$par))\n  Sigma_val=sqrt(variance*(Nu_val-2)/Nu_val) \n  \n  return(c(Mu,Sigma_val,Nu_val))\n}\n\n\n\n\nIntiall_parameter<-function(X,#Data in vector form\n                        Pi=NA,#Problillaty vector\n                        Mu=NA,# mean vector\n                        Sigma=NA,#Sigma vector \n                        Nu =NA, # NU vector\n                        K # Number of distribuations\n                  ){\n  if(any(c(is.na(Pi),is.na(Mu),is.na(Sigma),is.na(Nu)))){\n    # get partions\n    KMeans_objet=KMeans_rcpp(as.matrix(X), clusters = K, num_init = 30, initializer = 'kmeans++')\n  }\n  if(any(is.na(Pi))){\n    #Estimate Pi\n    Pi=Estiame_Pi_from_partions(KMeans_objet$clusters,K)\n  }\n  \n  if(any(c(is.na(Mu),is.na(Sigma),is.na(Nu)))){\n    #Make sure their is vector \n    Mu=initialize_if_na(Mu,K)\n    Sigma=initialize_if_na(Sigma,K)\n    Nu=initialize_if_na(Nu,K)\n    for (i in 1:K){\n      partin_data=X[KMeans_objet$clusters==i]\n      partin_parameter=Intiall_parameter_optimasation(partin_data)\n      if(is.na(Mu[i])){\n        Mu[i]=partin_parameter[1]\n      }\n      if(is.na(Sigma[i])){\n        Sigma[i]=partin_parameter[2]\n      }\n      if(is.na(Nu[i])){\n        Nu[i]=partin_parameter[3]\n      }\n    }\n  }\n  ret_obj=list(Mu = Mu,\n               Sigma = Sigma,\n               Nu=Nu,\n               Pi=Pi)\n  return(ret_obj)\n}\n\n\n\n\n\n\n# use kmeans to Pi estimate pi and mu\nIntiall_parameter_grid=function(X,Pi,Mu,Sigma_grid,Nu_grid,K){\n  if(any(c(is.na(Pi),is.na(Mu)))){\n    # get partions\n    KMeans_objet=KMeans_rcpp(as.matrix(X), clusters = K, num_init = 30, initializer = 'kmeans++')\n  }\n  if(any(is.na(Pi))){\n    #Estimate Pi\n    Pi=Estiame_Pi_from_partions(KMeans_objet$clusters,K)\n  }\n  \n  if(any(is.na(Mu))){\n    #Make sure their is vector \n    Mu=initialize_if_na(Mu,K)\n    for (i in 1:K){\n      partin_data=X[KMeans_objet$clusters==i]\n      \n      if(is.na(Mu[i])){\n        Mu[i]=mean(partin_data)\n      }\n    }\n  }\n  #preform gridseach but only over paramters Sigma and nu\n  \n  #Finding combinations\n  #Sigma_grid=as.numeric(seq(2,5))\n  #make list with comnations \n  Sigma_grid_combinations <- expand.grid(rep(list(Sigma_grid), K))\n  #makes List of list whith every combantion in the seq Sigma_grid\n  list_of_combinations_Sigma <- split(as.matrix(Sigma_grid_combinations), seq(nrow(Sigma_grid_combinations)))\n  \n  #Nu_grid=as.numeric(seq(2,10))\n  Nu_grid_combinations<-expand.grid(rep(list(Nu_grid), K))\n  list_of_combinations_Nu <- split(as.matrix(Nu_grid_combinations), seq(nrow(Nu_grid_combinations)))\n  \n  \n  grid_for_seach=expand.grid(list_of_combinations_Sigma,list_of_combinations_Nu)\n  \n  \n  # This can be paralised but, it mean define all the functions from Rcpp in R so they can be importet into the clusters\n  #souch rcpp can export function to paralles so redefine functions\n  \n  # # Number of cores to use\n  # num_cores <- detectCores() - 1 # supose to be nice to let one stand for other stuff\n  # # Create a cluster\n  # cl <- makeCluster(num_cores)\n  # registerDoParallel(cl)\n  # clusterExport(cl, c(\"loglikelyhood_t_mix\", \"X\", \"Pi\", \"Mu\", \"grid_for_seach\"))\n  # \n  # results <- foreach(i = 1:nrow(grid_for_seach), .combine = rbind) %dopar% {\n  # row <- grid_for_seach[i, ]\n  # var1 <- as.vector(unlist((row[1]))) \n  # var2 <-  as.vector(unlist((row[2])))#\n  # \n  # log_likelihood_val <- den_test(X, Pi, Mu, c(5,5,5), c(4,4,4)) # function can be importet to clusters so the method wont work\n  # c(var1 = var1, var2 = var2, log_likelihood_val = log_likelihood_val) \n  # }\n  # stopCluster(cl)\n  \n  compute_log_likelihood <- function(row) {\n  var1 <- as.vector(unlist(row[1]))\n  var2 <- as.vector(unlist(row[2]))\n  loglikelyhood_t_mix(X, Pi, Mu, var1, var2)\n  }\n  \n  log_likelihood_values <- sapply(1:nrow(grid_for_seach), function(i) {\n    compute_log_likelihood(grid_for_seach[i, ])\n    })\n  max_log_likelihood <- max(log_likelihood_values)\n  max_index <- which.max(log_likelihood_values)\n  Sigma=as.vector(unlist(grid_for_seach[max_index,]$Var1))\n  Nu=as.vector(unlist(grid_for_seach[max_index,]$Var2))\n  ret_obj=list(Mu = Mu,\n               Sigma = Sigma,\n               Nu=Nu,\n               Pi=Pi,\n               likelyhood=max_log_likelihood,\n               Numer_of_combination=nrow(grid_for_seach))\n  return(ret_obj)\n}\n\n\n\n\n#Kmeans is used to run make intiall partions, when Scaled shifted t distribuation is fitted on each partions.\n# This method works well when the distribuations is well seperated, but not if mixture distributuin is to close.\n#basically what happen if the partions is to close what happens is that partions do not look like t distribution, and can somtime even look like uniform distribution (basically far of), this make the estimation of the ustabel and sometimes lead to error, the solution is to come with some manuall inputs \n\nEM_Mix_T_Dist<-function(X,#Data in vector form\n                        Pi=NA,#Problillaty vector\n                        Mu=NA,# mean vector\n                        Sigma=NA,#Sigma vector \n                        Nu =NA, # NU vector\n                        K, # Number of distribuations\n                        Clipping_vector=c(10,5,2),# Max value for clipping  for parameters (Mu,Sigma,Ni)\n                        Max_iter=100,# maxium number of interations for the EM algorithm\n                        alpha=0.02,# scaling for gradient decent\n                        max_iter_gradient=3,# maxium number of times in graident for each iteratin\n                        norm_gradient = 0.1, # stop criteria for gradient decent\n                        Start_method_optim=T){\n  #Checking for intiations process\n  #if no argument is given for Pi , Mu , Sigma and NU is given then a gues is made\n  if(Start_method_optim==T){\n    start_para=Intiall_parameter(X,Pi,Mu,Sigma,Nu,K)\n  }\n  else{\n    start_para=Intiall_parameter_grid(X,Pi,Mu,Sigma_grid=as.numeric(seq(1,15,2)),as.numeric(seq(2,10,2)),K)\n  }\n  Pi=start_para$Pi\n  Mu=start_para$Mu\n  Sigma=start_para$Sigma\n  Nu=start_para$Nu\n  \n  if(any(c(is.na(Pi),is.na(Mu),is.na(Sigma),is.na(Nu)))){\n    print(\"The intilasaions failed\")\n    return(NA)\n  } \n  #Run EM\n  EM_partion=EM_t_distribution(X,Pi,Mu,Sigma,Nu,Clipping_vector = Clipping_vector,Max_iter=Max_iter,alpha=alpha,max_iter_gradient=max_iter_gradient,norm_gradient = norm_gradient)\n  \n  #Make return object\n  likelyhood=likelyhood_t_mix(X,EM_partion[,4],EM_partion[,1],EM_partion[,2],EM_partion[,3])\n  loglikelyhood=loglikelyhood_t_mix(X,EM_partion[,4],EM_partion[,1],EM_partion[,2],EM_partion[,3])\n  AIC=2*K-2*loglikelyhood\n  BIC=K*log(length(X))-2*loglikelyhood\n  ret_obj=list(Mu = EM_partion[,1],\n               Sigma = EM_partion[,2],\n               Nu=EM_partion[,3],\n               Pi=EM_partion[,4],\n               AIC=AIC,\n               BIC=BIC,\n               likelyhood=likelyhood,\n               loglikelyhood=loglikelyhood,\n               weights=Weights_of_X(X,EM_partion[,4],EM_partion[,1],EM_partion[,2],EM_partion[,3]))\n  return(ret_obj)\n}\n```\n:::\n\n\n\nRcpp library\n\n<details>\n<summary>Click to expand/collapse the C++ code</summary>\n\n\n```cpp\n#include <RcppArmadillo.h>\n#include <Rmath.h>\n#include <cmath>\n#include <RcppGSL.h>\n#include <gsl/gsl_sf_psi.h> \n\n// [[Rcpp::depends(RcppArmadillo)]]\nusing namespace Rcpp;\nusing namespace arma;\nusing namespace std;\n\n\n// [[Rcpp::export]]\ndouble T_density(double x,\n                 double Mu,\n                 double Sigma,\n                 double Nu){\n  return (Rf_gammafn((Nu + 1) / 2.0) / (Rf_gammafn(Nu / 2.0)* sqrt(M_PI * Nu * pow(Sigma, 2))))*pow((1+pow((x-Mu),2)/(Nu*pow(Sigma,2))),-(Nu+1)/2.0);\n}\n\n// [[Rcpp::export]]\ndouble Mix_T_density_x(double x,\n                     vec Pi, \n                     vec Mu,\n                     vec Sigma,\n                     vec Nu){\n  vec dens_of_x_for_each_t(Pi.n_elem);\n  for (int i =0; i< Pi.n_elem;++i){\n    dens_of_x_for_each_t[i]=Pi[i]*T_density(x,Mu[i],Sigma[i],Nu[i]);\n    }\n  double densdens_of_X=sum(dens_of_x_for_each_t);\n  return densdens_of_X;\n}\n\n\n\n// [[Rcpp::export]]\nvec Mix_T_density(vec X,\n                     vec Pi, \n                     vec Mu,\n                     vec Sigma,\n                     vec Nu){\n  vec Dens_for_each_x(X.n_elem);\n  \n  for (int j=0; j<X.n_elem;++j){\n    Dens_for_each_x[j] = Mix_T_density_x(X[j],Pi,Mu,Sigma,Nu);\n  }\n  return (Dens_for_each_x);\n}\n\n\n// [[Rcpp::export]]\ndouble likelyhood_t_mix(vec X,\n                  vec Pi,\n                  vec mu,\n                  vec Sigma,\n                  vec Nu){\n  return prod(Mix_T_density(X,Pi,mu,Sigma,Nu));\n}\n\n// [[Rcpp::export]]\ndouble loglikelyhood_t_mix(vec X,\n                        vec Pi,\n                        vec mu,\n                        vec Sigma,\n                        vec Nu){\n  vec densities = Mix_T_density(X, Pi, mu, Sigma, Nu);\n  double log_likelihood = sum(log(densities));\n  return log_likelihood;\n}\n\n\n// [[Rcpp::export]]\nmat Weights_of_X(vec X,\n                 vec Pi,\n                 vec mu,\n                 vec Sigma,\n                 vec Nu){\n  //Row's is a given outcome Pi is the probiallty that it come from the j density\n  mat Weigths(X.n_elem,Pi.n_elem);\n  for (int i =0; i < X.n_elem; ++i){\n    vec like_x_i(Pi.n_elem); // make vector\n    for (int j=0;j<Pi.n_elem;++j){\n      like_x_i[j]=Pi[j]*T_density(X[i],mu[j],Sigma[j],Nu[j]);\n    }\n    for (int k=0;k<Pi.n_elem;++k){\n    Weigths(i,k)=like_x_i[k]/sum(like_x_i);\n    }\n  }\n  return Weigths;\n}\n\n\n\n// [[Rcpp::export]]\nvec get_Pi(mat Weigths){\n  if (Weigths.n_rows == 0) {\n    Rcpp::stop(\"Input matrix has no rows\");\n  }\n  return sum(Weigths, 0).t() / Weigths.n_rows;\n}\n\n\n\n// [[Rcpp::depends(RcppGSL)]]\n// [[Rcpp::export]]\nmat gradient(vec X,\n             vec Pi,\n             vec mu,\n             vec Sigma,\n             vec Nu,\n             string normalise =\"no_normaliztion\"){\n  // col mu ,Sigma, Nu\n  mat Gradiets(Pi.n_elem,3);\n  // find weights\n  mat w_x = Weights_of_X(X,Pi,mu,Sigma,Nu);\n  \n  \n  // mu\n  for (int i =0; i < mu.n_elem; ++i){\n    double mu_i =0;\n    for (int j =0; j < X.n_elem; ++j){\n      // mising weight\n      mu_i=mu_i+w_x(j,i)*(X[j]-mu[i])/(Nu[i]*pow(Sigma[i],2)+pow(X[j]-mu[i],2));\n    }\n    mu_i = mu_i*(Nu[i]+1);\n    mu_i =-mu_i; \n    Gradiets(i,0)=mu_i;\n  // sigma\n    double sigma_i=0;\n    for (int j =0; j < X.n_elem; ++j){\n      sigma_i= sigma_i+w_x(j,i)*(-1+(Nu[i]+1)*pow(X[j]-mu[i],2)/(Sigma[i]*Nu[i]+pow(X[j]-mu[i],2)));\n      }\n    sigma_i=sigma_i/Sigma[i];\n    sigma_i=-sigma_i;\n    Gradiets(i,1)=sigma_i;\n    // nu\n    // declare varibels for less coput\n    double Gamma_v_plus_one= tgamma((Nu[i]+1)/2);\n    double psi_v_plus_one= gsl_sf_psi((Nu[i]+1)/2);\n    double Gamma_v = tgamma(Nu[i]/2);\n    double psi_v = gsl_sf_psi(Nu[i]/2);\n    double nu_i=0;\n    for (int j =0;j < X.n_elem;++j){\n      nu_i=nu_i-0.5*w_x(j,i)+((Nu[i]+1)/2)*((pow(X[j]-mu[i],2))/(pow(Sigma[i]*Nu[i],2)))*w_x(j,i)+((psi_v_plus_one)/(2*Gamma_v_plus_one))*w_x(j,i)-((psi_v)/(2*Gamma_v))*w_x(j,i);\n      }\n    nu_i=-nu_i;\n    Gradiets(i,2)=nu_i;\n  }\n    // Gradient normalsation\n  if (normalise ==\"second_norm\"){\n    Gradiets=Gradiets/norm(Gradiets,2); // second option based on dataX.n_elem;\n    }\n  if (normalise ==\"N_element\"){\n    Gradiets=Gradiets/X.n_elem;\n    }\n  if (normalise ==\"Element_wise_Normalsation\"){\n    Gradiets=Gradiets.col(0)/norm(Gradiets.col(0),2);\n    Gradiets=Gradiets.col(1)/norm(Gradiets.col(1),2);\n    Gradiets=Gradiets.col(2)/norm(Gradiets.col(2),2);\n  }\n  return Gradiets;\n}\n\n// [[Rcpp::export]]\nvec Maximum_of_two_vector(vec v1,vec v2){\n  vec v_out(v1.n_elem);\n  for(int i =0;i<v1.n_elem;++i){\n    v_out[i]= max(v1[i],v2[i]);\n  }\n  return v_out;\n}\n\n\n//using overload to define to function based on the case where in\n\nmat Gradient_clipping_vec(mat gradient,vec C_levels=vec(3,1.0)){\n  for (int i =0;i<gradient.n_cols;++i){\n    if(norm(gradient.col(i),2)>C_levels[i]){\n      gradient.col(i)=C_levels[i]*gradient.col(i)/norm(gradient.col(i),2);\n      }\n    }\n  return(gradient);\n}\nmat Gradient_clipping(mat gradient,double C_level=1){\n  if(norm(gradient,2)>C_level){\n    gradient=C_level*gradient/norm(gradient,2);\n  }\n  return(gradient);\n}\n// Chosen not to export graident clibbing to R since i cant export to function with the same name.\n\n/*\nI this section i have made multiple version of gradient decent.\nIt could be combinede in to one function. I have keept it as threes versions so, since in the final EM one is gona be called as the standart.\nSo insted of investing af lot time to writing it, this makes the flow faster.\n*/\n// [[Rcpp::export]]\nmat gradient_decent_no_check(vec X,\n                                   vec Pi,\n                                   vec mu,\n                                   vec Sigma,\n                                   vec Nu,\n                                   double alpha=0.02,\n                                   double Max_iter=1000,\n                                   double norm_gradient = 0.5,\n                                   string normalise_method =\"no_normaliztion\"//this should be changed\n                               )\n  {\n  mat parameters(Pi.n_elem,3); \n  // Set parameters value in matrix form (esay to update)\n  parameters.col(0)=mu;\n  parameters.col(1)=Sigma;\n  parameters.col(2)=Nu;\n  mat gradient_in_point = gradient(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2),normalise_method); \n  \n  // Sigma_lower_bound_vec\n  /*\n  This is smart since it the if one overshoot the domain of the Gamma function is not definede \n  */\n  double parameter_min= 0.01;\n  vec Sigma_lower_bound_vec(Sigma.n_elem,fill::ones);\n  Sigma_lower_bound_vec=parameter_min*Sigma_lower_bound_vec;\n  vec Nu_lower_bound_vec(Nu.n_elem,fill::ones);\n  Nu_lower_bound_vec=parameter_min*Nu_lower_bound_vec;\n  \n  // loop throug gradient decent\n  for (int i=0;i<Max_iter;++i){\n    if (norm(vectorise(gradient(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2),\"no_normaliztion\")),2)<norm_gradient){\n      return parameters;\n    }\n    parameters=parameters-alpha*gradient_in_point;\n    // If undershoot set to minimum Note is is importante that the check is done before the gradient is updated, otherwise the gradient will not be able to be evaluated\n    parameters.col(1)=Maximum_of_two_vector(parameters.col(1),Sigma_lower_bound_vec);\n    parameters.col(2)=Maximum_of_two_vector(parameters.col(2),Nu_lower_bound_vec);\n    // update parametes\n    gradient_in_point = gradient(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2),normalise_method);\n    \n  }\n  return parameters;\n}\n\n\n// [[Rcpp::export]]\nmat gradient_decent_with_check(vec X,\n                             vec Pi,\n                             vec mu,\n                             vec Sigma,\n                             vec Nu,\n                             double alpha=0.02,\n                             double Max_iter=1000,\n                             double norm_gradient = 0.5,\n                             string normalise_method =\"no_normaliztion\",//this should be changed\n                             double max_it_in_check= 10\n                            )\n{\n  mat parameters(Pi.n_elem,3); \n  // Set parameters value in matrix form (esay to update)\n  parameters.col(0)=mu;\n  parameters.col(1)=Sigma;\n  parameters.col(2)=Nu;\n  mat gradient_in_point = gradient(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2),normalise_method); \n  // make teperay variabel\n  mat tem_parameters=parameters;\n  // Sigma_lower_bound_ve\n  double parameter_min= 0.01;\n  vec Sigma_lower_bound_vec(Sigma.n_elem,fill::ones);\n  Sigma_lower_bound_vec=parameter_min*Sigma_lower_bound_vec;\n  vec Nu_lower_bound_vec(Nu.n_elem,fill::ones);\n  Nu_lower_bound_vec=parameter_min*Nu_lower_bound_vec;\n  // loop throug gradient decent\n  for (int i=0;i<Max_iter;++i){\n    if (norm(vectorise(gradient(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2),\"no_normaliztion\")),2)<norm_gradient){\n      return parameters;\n    }\n    for(int j=0;j<max_it_in_check;j++){\n      if(likelyhood_t_mix(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2))<likelyhood_t_mix(X,Pi,tem_parameters.col(0),tem_parameters.col(1),tem_parameters.col(2))){\n        break;\n      }\n      tem_parameters=parameters-alpha*gradient_in_point;\n      gradient_in_point=alpha*gradient_in_point;\n    }\n    parameters=tem_parameters;\n    // If undershoot set to minimum Note is is importante that the check is done before the gradient is updated, otherwise the gradient will not be able to be evaluated\n    parameters.col(1)=Maximum_of_two_vector(parameters.col(1),Sigma_lower_bound_vec);\n    parameters.col(2)=Maximum_of_two_vector(parameters.col(2),Nu_lower_bound_vec);\n    // update parametes\n    gradient_in_point = gradient(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2),normalise_method);\n    \n  }\n  return parameters;\n}\n\n// need to be made\n// [[Rcpp::export]]\nmat gradient_decent_T_mix_model(vec X,\n                               vec Pi,\n                               vec mu,\n                               vec Sigma,\n                               vec Nu,\n                               double alpha=0.02,\n                               double Max_iter=1000,\n                               double norm_gradient = 0.5,\n                               string type_decent =\"Normal\",\n                               string normalise_method =\"no_normaliztion\",//this should be changed\n                               double max_it_in_check= 10\n                               \n)\n{\n  mat parameters(Pi.n_elem,3); \n  // Set parameters value in matrix form (esay to update)\n  parameters.col(0)=mu;\n  parameters.col(1)=Sigma;\n  parameters.col(2)=Nu;\n  mat gradient_in_point = gradient(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2),normalise_method); \n  // make teperay variabel\n  mat tem_parameters=parameters;\n  // Sigma_lower_bound_ve\n  double parameter_min= 0.01;\n  vec Sigma_lower_bound_vec(Sigma.n_elem,fill::ones);\n  Sigma_lower_bound_vec=parameter_min*Sigma_lower_bound_vec;\n  vec Nu_lower_bound_vec(Nu.n_elem,fill::ones);\n  Nu_lower_bound_vec=parameter_min*Nu_lower_bound_vec;\n  // loop throug gradient decent\n  for (int i=0;i<Max_iter;++i){\n    if (norm(vectorise(gradient(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2),\"no_normaliztion\")),2)<norm_gradient){\n      return parameters;\n    }\n    for(int j=0;j<max_it_in_check;j++){\n      if(likelyhood_t_mix(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2))<likelyhood_t_mix(X,Pi,tem_parameters.col(0),tem_parameters.col(1),tem_parameters.col(2))){\n        break;\n      }\n      tem_parameters=parameters-alpha*gradient_in_point;\n      gradient_in_point=alpha*gradient_in_point;\n    }\n    parameters=tem_parameters;\n    // If undershoot set to minimum Note is is importante that the check is done before the gradient is updated, otherwise the gradient will not be able to be evaluated\n    parameters.col(1)=Maximum_of_two_vector(parameters.col(1),Sigma_lower_bound_vec);\n    parameters.col(2)=Maximum_of_two_vector(parameters.col(2),Nu_lower_bound_vec);\n    // update parametes\n    gradient_in_point = gradient(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2),normalise_method);\n    \n  }\n  return parameters;\n}\n\n// Gradient decent whit clipping\n//\n// [[Rcpp::export]]\nmat gradient_decent_with_clipping(vec X,\n                                vec Pi,\n                                vec mu,\n                                vec Sigma,\n                                vec Nu,\n                                vec Clipping_vector,\n                                double alpha=0.02,\n                                int Max_iter=1000,\n                                double norm_gradient = 0.5,\n                                int max_it_in_check= 10\n)\n{\n  // make mat for parameters\n  mat parameters(Pi.n_elem,3); \n  \n  // Set parameters value in matrix form (esay to update)\n  parameters.col(0)=mu;\n  parameters.col(1)=Sigma;\n  parameters.col(2)=Nu;\n  mat gradient_in_point = gradient(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2),\"no_normaliztion\"); \n  \n  // make teperay variabel\n  mat tem_parameters=parameters;\n  \n  // Make lover bound for parameter\n  double parameter_min= 0.01;\n  vec Sigma_lower_bound_vec(Sigma.n_elem,fill::ones);\n  Sigma_lower_bound_vec=parameter_min*Sigma_lower_bound_vec;\n  vec Nu_lower_bound_vec(Nu.n_elem,fill::ones);\n  Nu_lower_bound_vec=parameter_min*Nu_lower_bound_vec;\n  \n  // loop throug gradient decent\n  for (int i=0;i<Max_iter;++i){\n    // make check for gradient size \n    if (norm(vectorise(gradient(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2),\"no_normaliztion\")),2)<norm_gradient){\n      return parameters;\n    }\n    //graident clipping prcedorure\n    //mat Gradient_clipping_vec(mat gradient,vec C_levels=vec(3,1.0))\n    gradient_in_point=Gradient_clipping_vec(gradient_in_point,Clipping_vector);\n    \n    //check if gradient result in small likelyhood.\n    for(int j=0;j<max_it_in_check;j++){\n      if(likelyhood_t_mix(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2))<likelyhood_t_mix(X,Pi,tem_parameters.col(0),tem_parameters.col(1),tem_parameters.col(2))){\n        break;\n      }\n      tem_parameters=parameters-alpha*gradient_in_point;\n      gradient_in_point=alpha*gradient_in_point; // this contiues aplies alpha to the graiden \n    }\n    parameters=tem_parameters;\n    // If undershoot set to minimum Note is is importante that the check is done before the gradient is updated, otherwise the gradient will not be able to be evaluated\n    parameters.col(1)=Maximum_of_two_vector(parameters.col(1),Sigma_lower_bound_vec);\n    parameters.col(2)=Maximum_of_two_vector(parameters.col(2),Nu_lower_bound_vec);\n    // update parametes\n    gradient_in_point = gradient(X,Pi,parameters.col(0),parameters.col(1),parameters.col(2),\"no_normaliztion\");\n    \n  }\n  return parameters;\n}\n\n\n\n\n\n\n// [[Rcpp::export]]\nmat optimize_using_gradient_decent(vec X,\n                                   vec Pi,\n                                   vec mu,\n                                   vec Sigma,\n                                   vec Nu,\n                                   double alpha=0.02,\n                                   int Max_iter=1000,\n                                   double norm_gradient = 0.5,\n                                   string normalise_method =\"no_normaliztion\",//this should be changed\n                                   bool Check_gradient_decrease = true,\n                                   int max_it_in_check= 100){\n  if(Check_gradient_decrease==false){\n    return(gradient_decent_no_check(X,Pi,mu,Sigma,Nu,alpha,Max_iter,norm_gradient,normalise_method));\n  }\n  else{\n    return(gradient_decent_with_check(X,Pi,mu,Sigma,Nu,alpha,Max_iter,norm_gradient,normalise_method,max_it_in_check));\n  }\n  \n}\n\n\n// [[Rcpp::export]]\nmat EM_t_distribution(vec X,\n                      vec Pi,\n                      vec mu,\n                      vec Sigma,\n                      vec Nu,\n                      vec Clipping_vector,\n                      int Max_iter = 10000,\n                      double alpha=0.02,\n                      int max_iter_gradient=100,\n                      double norm_gradient = 0.001\n){\n  mat parameters(Pi.n_elem,4); \n  // Set parameters value in matrix form (esay to update)\n  \n  parameters.col(0)=mu;\n  parameters.col(1)=Sigma;\n  parameters.col(2)=Nu;\n  \n  mat w_x=Weights_of_X(X,Pi,mu,Sigma,Nu);\n  parameters.col(3)=get_Pi(w_x);\n  for (int i=0;i<Max_iter;++i){\n    w_x=Weights_of_X(X,parameters.col(3),parameters.col(0),parameters.col(1),parameters.col(2));\n    parameters.col(3)=get_Pi(w_x);\n    parameters.cols(0,2)=gradient_decent_with_clipping(X,parameters.col(3),parameters.col(0),parameters.col(1),parameters.col(2),Clipping_vector,alpha,Max_iter,norm_gradient,max_iter_gradient);\n  }\n  \n  return parameters;\n}\n\n```\n\n\n\n<details>\n\nCode for visulsations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#code for visualasation\nlibrary(ggplot2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.4.2\n```\n\n\n:::\n\n```{.r .cell-code}\nPlot_mix_t_distribution <- function(X,Pi,Mu,Sigma,Nu,bins=30){\n  plot_fun<-function(x){sapply(x,function(x){Mix_T_density_x(x,Pi,Mu,Sigma,Nu)})}\n  plot <- ggplot(data.frame(X), aes(x = X)) +\n    geom_histogram(aes(y = ..density..), bins = bins, color = \"black\", alpha = 0.7) +\n    stat_function(fun = plot_fun, geom = \"line\",color=\"red\")+\n    labs(title = \"Histogram with Mixture of t-Distributions\", x = \"X\", y = \"Density\") +\n    theme_minimal()  \n  return(plot)\n}\n```\n:::\n\n\n\n\n\n# Test\nBelow, I have created a small test of the application with a mixture of three well-separated distributions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# test\n\ndraws=Draw_mixture_componet()\n\nmodel<-EM_Mix_T_Dist(draws,K=3)\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins=50)\n```\n\n::: {.cell-output-display}\n![](Mixture-of-K-scaled-and-shifted-t-distributions_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nAs can be seen, the application performs very well.\n\n# A Look into When Initialization Fails\nThe initialization of the distribusion is too close together, as illustrated below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(3123)\ndraws=Draw_mixture_componet(Mu=c(-1,4,10))\n\nmodel<-EM_Mix_T_Dist(draws,K=3)\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins=50)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 101 rows containing missing values or values outside the scale range\n(`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Mixture-of-K-scaled-and-shifted-t-distributions_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\nAbove, I have drawn from a mixture model with three components and fitted a three-component mixture model using automated initialization, even though there are three distinct peaks. The initialization fails.\n\nBelow, there is a second example, again with three components, but in this case, it is not easy to tell if there are two or three components.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Make \ndraws=Draw_mixture_componet(Mu=c(0,2,10))\n\nhist(draws,breaks = 100)\n```\n\n::: {.cell-output-display}\n![](Mixture-of-K-scaled-and-shifted-t-distributions_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nKMeans_objet=KMeans_rcpp(as.matrix(draws), clusters = 3, num_init = 90, initializer = 'kmeans++')\npar(mfrow = c(1, 3))\nhist(draws[KMeans_objet$clusters==1],breaks = 30)\nhist(draws[KMeans_objet$clusters==2],breaks = 30)\nhist(draws[KMeans_objet$clusters==3],breaks = 30)\n```\n\n::: {.cell-output-display}\n![](Mixture-of-K-scaled-and-shifted-t-distributions_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n\n#if one runs the stadart implentasion here the starting guase would fail.\n#model<-EM_Mix_T_Dist(draws,K=3)\n```\n:::\n\n\n\nIn the above, the histograms of the fractions generated from the K-means++ are shown. Cluster 1 is the well-separated cluster. The rest of the partitions do not resemble a T-distribution, which seems to lead to the high values of $\\nu$\n\nThe above is generated from mixing three t-distributions but could look like two distributions. My implementation for standardization will, in this case, fail. It will return since the value of gradient becomes to high do to large value of $\\nu$.\n\nBelow, I showcase how one can manually tune the input and run the method.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPi_test=Estiame_Pi_from_partions(KMeans_objet$clusters,K=3)\nmu_test=c(mean(draws[KMeans_objet$clusters==1]),mean(draws[KMeans_objet$clusters==2]),mean(draws[KMeans_objet$clusters==3]))\n\n\n# here is to example on how to hand fit the values \n\npar(mfrow = c(1, 2))\nPlot_mix_t_distribution(draws,Pi_test,mu_test,c(1,2,3),c(2.5,2.5,4),bins=100)\n```\n\n::: {.cell-output-display}\n![](Mixture-of-K-scaled-and-shifted-t-distributions_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nPlot_mix_t_distribution(draws,c(0.2,0.4,0.4),mu_test,c(1,1.7,3),c(2.5,2.5,4),bins=100)\n```\n\n::: {.cell-output-display}\n![](Mixture-of-K-scaled-and-shifted-t-distributions_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\n\nLastly, we examine what the implementation yields after manual tuning is used as the starting values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPi_test=Estiame_Pi_from_partions(KMeans_objet$clusters,K=3)\nmu_test=c(mean(draws[KMeans_objet$clusters==1]),mean(draws[KMeans_objet$clusters==2]),mean(draws[KMeans_objet$clusters==3]))\n\nmodel<-EM_Mix_T_Dist(draws,K=3,Pi=c(0.2,0.4,0.4),Mu=mu_test,Sigma=c(1,1.7,3),Nu=c(2.5,2.5,4),,Max_iter = 100, max_iter_gradient=3)\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins = 100)\n```\n\n::: {.cell-output-display}\n![](Mixture-of-K-scaled-and-shifted-t-distributions_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nAs can be seen the Em algorithm still strugels here.\n\nIf one runs the implication with $K=2$ their is no problems.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel<-EM_Mix_T_Dist(draws,K=2)\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins = 100)\n```\n\n::: {.cell-output-display}\n![](Mixture-of-K-scaled-and-shifted-t-distributions_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## Test with second initialization\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(3123)\ndraws=Draw_mixture_componet(Mu=c(-1,4,10))\n\nmodel<-EM_Mix_T_Dist(draws,K=3,Start_method_optim = F)\n\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins=50)\n```\n\n::: {.cell-output-display}\n![](Mixture-of-K-scaled-and-shifted-t-distributions_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nThe method based on the grid search is computational expensiv but can give good result.\n\n\n\n## Why the initialization fails.\nIf the starting values are too extreme, such as $\\nu_i=50000$, the gradient cannot be computed properly due to issues with the gamma function or its derivative. As it stands, the initialization can produce such problematic values if the data from k-means is not well-behaved. If the distribution is not well-separated, the hard clustering technique will not capture enough of the distribution in the decreasing part as the function moves out. This can lead to excessively high values of $\\nu$.\n\n# Aplicantion on real data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mixsmsn)\n\n# Load the dataset\n\n\ndata_real_test<-as.vector( unlist(faithful$eruptions))\n\n\nparamenter=Intiall_parameter_grid(data_real_test,Pi=NA,Mu=NA,Sigma=seq(0.1,4,0.5),Nu=seq(1,3,0.5),K=2)\n\n\n\nmodel<-EM_Mix_T_Dist(data_real_test,Pi=paramenter$Pi,Mu=paramenter$Mu,Sigma = paramenter$Sigma,Nu=paramenter$Nu,K=2,max_iter_gradient = 40,Max_iter =200 )\nPlot_mix_t_distribution(data_real_test,model$Pi,model$Mu,model$Sigma,model$Nu,bins=50)\n```\n\n::: {.cell-output-display}\n![](Mixture-of-K-scaled-and-shifted-t-distributions_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nAbove, I have applied the method to real-world data from the Old Faithful geyser in Yellowstone National Park. The y-axis represents the eruption height. I had to manually feed the grid optimization into the function to get some decent starting values, and I had to run the application more times than the standard, but overall, the method works.\n\nPro-tip: Potentially, one can speed up the process by hand-tuning the grid search.\n\n\n# Further improvements\n\n## Model selection\nFurther improvements should be made to the initialization process, as this would enable automatic model selection based on AIC or BIC. \n\n## confidence bands\nImplementing confidence bands using a parametric bootstrap approach would be beneficial. Their is small complication since $\\pi_i$ label ambiguity. For faster implementation, the bootstrap process could utilize a \"hot start,\" meaning setting the starting values for the parameters to the estimated values.\n\n### Initialization\nThe initialization could run in parallel for both the grid search and the optimization method. It seems like R won’t import functions defined in a separate Rcpp file to the cluster. There are two options: define the necessary functions and dependent functions directly in R via the RcppFunction, or do the parallelization in Rcpp. Doing the parallelization directly in Rcpp would be better.\n\n### Gradient decent.\nIt would be nice if the gradient descent could use more iterations in the later steps. This is fairly easy to implement, but for an end-user and for me, it's hard to set a good standard setting. Potentially, some of the same effect can be achieved by increasing the number of iterations since if the weights don't change, it would almost be the same.\n\n\n\nFor now the project is closed.\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}