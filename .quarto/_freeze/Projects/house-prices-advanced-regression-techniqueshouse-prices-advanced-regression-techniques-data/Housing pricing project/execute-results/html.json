{
  "hash": "5f5e9f115f0c7f4edad796cb0ce949b0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"House Pricing Project\"\nformat: \n  html:\n    code-fold: true\nfilters:\n  - shinylive\n---\n\n\nThis is practics case based on some data from [kaggle](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview)\n\n\nI mostly made this to play around with some data and learn new methods.\nWhen working with data, like many others, I like to use either R Markdown or Jupyter Notebook.\nI also enjoy writing small notes and comments so that if I need to revisit the project later, I can easily see what I did.\nThis approach is really helpful in terms of reproducibility. Unfortunately, people often share notebooks without adding any text or comments.\n\nWhile I’ve written out some comments here, I don’t go too deep. However, I do show parts of my workflow.\nThe code is written in R, and I consider it what I call \"one-time code.\" It’s not meant to be read by others.\nIt’s not optimized either—I’ve simply chosen the fastest code I could think of or find to solve a problem. I ende op using Chat-gbt for some code aswell then correcting the mistakes.\nThe methology and the models chosen is all me.\n\n# Metric\nThe goal is to achieve the lowest score on the out-of-sample prediction set based on a specific metric.\n\nLet $P_{id}$ represent the price of a given house identified by $id$.  \nThe metric used for scoring is the Root Mean Squared Deviation (RMSD) of logarithmic prices:\n\n\n$$RMSD(\\log(\\hat{P}_{id})) = \\sqrt{\\frac{\\sum_{id} (\\log(P_{id}) - \\log(\\hat{P}_{id}))^2}{N}}$$\n\nThe logarithm is applied to ensure that expensive houses are not prioritized disproportionately. Without this adjustment, the model might prioritize expensive houses because the same proportional change in price would result in a larger numerical difference for higher-priced properties.\n\n(While this argument makes sense, it seems like the ratio of expensive to \"cheap\" houses in the dataset might also affect this outcome.)  \n(Nevertheless, this is the metric used by all other candidates, so if I want to compare my score with theirs, I must use it.)\n\nWhen conducting the analysis, the square root is typically omitted. This simplification does not affect the results, as the square root is a monotonically increasing function. Therefore, for model selection, the choice remains unaffected.\n\n# Data explorations data/cleaning.\nTheir is detail data description [here](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data), on krangler af in textfile with the data.\n\nI have copied the description of the covariates.\n\n<details>\n<summary>Click to expand/collapse varibel descriptions</summary>\n\n\n\n-   SalePrice - the property's sale price in dollars. This is the target variable that you're trying to \n\nPredictor varibels\n\n-   MSSubClass: The building class\n-   MSZoning: The general zoning classification\n-   LotFrontage: Linear feet of street connected to property\n-   LotArea: Lot size in square feet\n-   Street: Type of road access\n-   Alley: Type of alley access\n-   LotShape: General shape of property\n-   LandContour: Flatness of the property\n-   Utilities: Type of utilities available\n-   LotConfig: Lot configuration\n-   LandSlope: Slope of property\n-   Neighborhood: Physical locations within Ames city limits\n-   Condition1: Proximity to main road or railroad\n-   Condition2: Proximity to main road or railroad (if a second is present)\n-   BldgType: Type of dwelling\n-   HouseStyle: Style of dwelling\n-   OverallQual: Overall material and finish quality\n-   OverallCond: Overall condition rating\n-   YearBuilt: Original construction date\n-   YearRemodAdd: Remodel date\n-   RoofStyle: Type of roof\n-   RoofMatl: Roof material\n-   Exterior1st: Exterior covering on house\n-   Exterior2nd: Exterior covering on house (if more than one material)\n-   MasVnrType: Masonry veneer type\n-   MasVnrArea: Masonry veneer area in square feet\n-   ExterQual: Exterior material quality\n-   ExterCond: Present condition of the material on the exterior\n-   Foundation: Type of foundation\n-   BsmtQual: Height of the basement\n-   BsmtCond: General condition of the basement\n-   BsmtExposure: Walkout or garden level basement walls\n-   BsmtFinType1: Quality of basement finished area\n-   BsmtFinSF1: Type 1 finished square feet\n-   BsmtFinType2: Quality of second finished area (if present)\n-   BsmtFinSF2: Type 2 finished square feet\n-   BsmtUnfSF: Unfinished square feet of basement area\n-   TotalBsmtSF: Total square feet of basement area\n-   Heating: Type of heating\n-   HeatingQC: Heating quality and condition\n-   CentralAir: Central air conditioning\n-   Electrical: Electrical system\n-   1stFlrSF: First Floor square feet\n-   2ndFlrSF: Second floor square feet\n-   LowQualFinSF: Low quality finished square feet (all floors)\n-   GrLivArea: Above grade (ground) living area square feet\n-   BsmtFullBath: Basement full bathrooms\n-   BsmtHalfBath: Basement half bathrooms\n-   FullBath: Full bathrooms above grade\n-   HalfBath: Half baths above grade\n-   Bedroom: Number of bedrooms above basement level\n-   Kitchen: Number of kitchens\n-   KitchenQual: Kitchen quality\n-   TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n-   Functional: Home functionality rating\n-   Fireplaces: Number of fireplaces\n-   FireplaceQu: Fireplace quality\n-   GarageType: Garage location\n-   GarageYrBlt: Year garage was built\n-   GarageFinish: Interior finish of the garage\n-   GarageCars: Size of garage in car capacity\n-   GarageArea: Size of garage in square feet\n-   GarageQual: Garage quality\n-   GarageCond: Garage condition\n-   PavedDrive: Paved driveway\n-   WoodDeckSF: Wood deck area in square feet\n-   OpenPorchSF: Open porch area in square feet\n-   EnclosedPorch: Enclosed porch area in square feet\n-   3SsnPorch: Three season porch area in square feet\n-   ScreenPorch: Screen porch area in square feet\n-   PoolArea: Pool area in square feet\n-   PoolQC: Pool quality\n-   Fence: Fence quality\n-   MiscFeature: Miscellaneous feature not covered in other categories\n-   MiscVal: $Value of miscellaneous feature\n-   MoSold: Month Sold\n-   YrSold: Year Sold\n-   SaleType: Type of sale\n-   SaleCondition: Condition of sale\n\n</details>\n\n\n## Comments on Available Data\nAll the houses are located in the Ames city area.\nThere is information on the month and year sold. (In both the training and test sets, the years have the same values, meaning there’s no need to estimate values forward in time. This is a bit unusual—what is the purpose of the model in such a case?)\nMost of the data is categorical or discrete in nature.\nSome of the data has a natural order or rank.\nWhen examining the available data, it’s worth noting that there is no information on earlier sale prices—why is this missing?\nInformation on \"time on market\" is also missing.\n\n## Data cleaning and exploration\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\ndf=as.data.frame(read.csv(\"train.csv\"))\n\n#split <- sample(1:nrow(df), size = 0.8 * nrow(df))\n\n# Create training and testing sets\n#df <- df[split, ]  # 80% of the data\n#df_test_for_choice <- df[-split, ]  # other 20%\n\n#\ndf_test_preformence=as.data.frame(read.csv(\"test.csv\"))\n```\n:::\n\n\n\n## Recoding NA\nIn the description, I noticed that some of the variables use NA as a category.\n\nVaribels where NA is a catgori is listed below.\n\n-   Alley\n-   BsmtQual\n-   BsmtCond\n-   BsmtExposure\n-   BsmtFinType1\n-   BsmtFinType2\n-   FireplaceQu\n-   GarageType\n-   GarageFinish\n-   GarageQual\n-   GarageCond\n-   PoolQC\n-   Fence\n-   MiscFeature\n\nThese variables need to be recoded, as the values are not missing—they are simply in the wrong format.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#change \nchang_to_df<-function(DF){\n  DF$Alley<-DF$Alley[is.na(DF$Alley)] <- \"No alley\"\n  DF$BsmtQual[is.na(DF$BsmtQual)]<-\"No Basement\"\n  DF$BsmtCond[is.na(DF$BsmtCond)]<-\"No Basement\"\n  DF$BsmtExposure[is.na(DF$BsmtExposure)]<-\"No Basement\"\n  DF$BsmtFinType1[is.na(DF$BsmtFinType1)]<- \"No Basement\"\n  DF$BsmtFinType2[is.na(DF$BsmtFinType2)]<- \"No Basement\"\n  DF$FireplaceQu[is.na(DF$FireplaceQu)]<-\"No Fireplace\"\n  DF$GarageType[is.na(DF$GarageType)]<-\"No Garage\"\n  DF$GarageFinish[is.na(DF$GarageFinish)]<-\"No Garage\"\n  DF$GarageQual[is.na(DF$GarageQual)]<-\"No Garage\"\n  DF$GarageCond[is.na(DF$GarageCond)]<-\"No Garage\"\n  DF$GarageYrBlt[is.na(DF$GarageYrBlt)]<-0 # all values that is NA corespond wither other showing that their is no garage.\n  DF$PoolQC[is.na(DF$PoolQC)]<-\"No Pool\"\n  DF$Fence[is.na(DF$Fence)]<-\"No Fence\"\n  DF$MiscFeature[is.na(DF$MiscFeature)]<-\"None\"\n  return(DF)\n}\n\ndf<-chang_to_df(df)\n#df_test_for_choice<-chang_to_df(df_test_for_choice)\ndf_test_preformence<-chang_to_df(df_test_preformence)\n```\n:::\n\n\n\n\n## Examining Missing Values\nAfter recoding cases where NA does not represent missing values, we will now analyze the actual missing values.\n\nBelow is a plot showing the missing values. I had to split the plot into two parts; otherwise, the variable names would not be readable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(naniar)\nvis_miss(df[,1:40])\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nvis_miss(df[,40:81])\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n\n\n\nLotFrontage missing values look weird.\n\nThe definition is Linear feet of street connected to property.\nBelow i have printed uniqe entreances in LotFrontage.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable((df$LotFrontage))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n 21  24  30  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48 \n 23  19   6   5   1  10   9   6   5   1   1  12   6   4  12   9   3   1   5   6 \n 49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68 \n  4  57  15  14  10   6  17   5  12   7  13 143   8   9  17  19  44  15  12  19 \n 69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88 \n 11  70  12  17  18  15  53  11   9  25  17  69   6  12   5   9  40  10   5  10 \n 89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 \n  6  23   6  10   8   6   7   8   2   8   3  16   2   4   3   3   6   1   7   3 \n109 110 111 112 114 115 116 118 120 121 122 124 128 129 130 134 137 138 140 141 \n  2   6   1   1   2   2   2   2   7   2   2   2   1   2   3   2   1   1   1   1 \n144 149 150 152 153 160 168 174 182 313 \n  1   1   1   1   1   1   1   2   1   2 \n```\n\n\n:::\n:::\n\n\nFrom the definition, it could refer to farms, but there are no farms in the dataset.\n\nIf we plot the house types against the number of missing values, we see that most of them come from single-family detached houses.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n# Test farm theory\ntest_LotFrontage <- df$LotFrontage\ntest_LotFrontage[is.na(test_LotFrontage)] <- 0\ntest_BldgType <- factor(df$BldgType)\n\n\n\n# Create a data frame\ndata <- data.frame(test_BldgType, test_LotFrontage)\n\nzero_counts <- data %>%\n  filter(test_LotFrontage == 0) %>%\n  count(test_BldgType)\n\n# Create the bar plot with category labels at the bottom\nbarplot(zero_counts$n, \n        names.arg = zero_counts$test_BldgType,  # Use the correct column for labels\n        main = \"Number of Zeros by Category\",\n        xlab = \"Categories\",\n        ylab = \"Count of Zeros\",\n        col = \"lightblue\",\n        las = 2)  # Rotate labels to vertical\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nIt seems like most of the missing values are from single-family detached houses, so it’s unrealistic to assume there is no street connected to the property. This means setting the values to zero is likely a bad option.\n\nOptions for imputation:\n\nLinear regression based on other variables.\nk-nearest neighbors.\nI could also disregard this variable since I doubt it has high predictive power.\n\nIt seems like an interactive linear model, incorporating the interaction between LotArea and LotShape, would provide good imputation (this also makes sense conceptually).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nImputasion_model <- lm(LotFrontage ~ LotArea * LotShape, df)\n\n# Plot fitted values vs. residuals\nplot(Imputasion_model$fitted.values, Imputasion_model$residuals,\n     xlab = \"Fitted Values\",\n     ylab = \"Residuals\",\n     main = \"Residuals plot (LotFrontage ~ LotArea * LotShape)\")\n\n# Add a horizontal line at the mean of the residuals (which should be zero)\nabline(h = mean(Imputasion_model$residuals), col = \"red\", lwd = 2, lty = 2)\nmean_value <- mean(Imputasion_model$residuals)\ntext(x = max(Imputasion_model$fitted.values), \n     y = mean_value, \n     labels = \"Mean\", \n     col = \"red\")\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\nMissing values for LotFrontage will be imputed using the linear model described above.\n\nThis also means that if LotFrontage is used in the model, the imputation will need to be performed on the test set as well. I have internally debated whether I should include the test set when building the imputation model—essentially, creating a model based on both the training and test sets for this purpose.\n\nSince I’m a little new to Kaggle, if the test set contains covariates and I only need to upload the fitted values, I would proceed with this approach. Otherwise, I would not. For now, I will just use the model based only on the training set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# imputatsion\nImputasion_model <- lm(LotFrontage ~ LotArea * LotShape, df)\n\nInput_value<-function(DF){\n  for (i in 1:length(DF$LotFrontage)){\n    if(is.na(DF$LotFrontage[i])){\n      DF$LotFrontage[i]<-predicted_value <- predict(Imputasion_model, newdata = list(\n        LotArea = DF$LotArea[i],\n        LotShape = DF$LotShape[i] ))\n    }\n  }\n  return(DF)\n  }\n\n#input df for all input values\ndf<-Input_value(df)\n#df_test_for_choice<-Input_value(df_test_for_choice)\ndf_test_preformence<-Input_value(df_test_preformence)\n```\n:::\n\n\n\nThere are still 1% of missing values in MasVnrArea and MasVnrType. I will discard the last row containing these missing values.\n\n# Looking at the Data\nIn this section, I will examine the variable distributions and determine if any transformations or categorizations are necessary.\nTo explore the data, I have created a number of plots for the variables. To make this process more accessible to potential readers, I have developed a small Shiny live application. To run this application live, I need to read the data from an online source, which is hosted on my GitHub page.\n\nI have included the code for the plots, and a potential reader can see in the code where the data is written to a text file.\n\n## Recoding of Variables\nSome of these variables have a ranking, which makes them well-suited for recoding. By recoding them as numerical values, the ranking becomes more obvious.\n\n<details>\n<summary>Click to expand/collapse varibel recoeding</summary>\n\nTo give a exampel of the recoding.\nPoolQC: Pool quality\n\t\t\n       Ex\tExcellent         (4)\n       Gd\tGood              (3)\n       TA\tAverage/Typical   (2)   (TA is never obsevered in pool varibel for traning set)\n       Fa\tFair              (1)\n       NA\tNo Pool           (0)\ncan be codes as 0-4\n\nBelowe here i have made list of how the varibels is code.\nI have made is so no pressent cagories NA is code to 0. Meaning 0 is only a value if the item is not pressent. and otherwise it starts from 1.\n\nLotShape: General shape of property\n       Reg\tRegular\t               (4)\n       IR1\tSlightly irregular     (3)\n       IR2\tModerately Irregular   (2)\n       IR3\tIrregular              (1)\n       \nLandContour: Flatness of the property\n\n       Lvl\tNear Flat/Level\t                                                    (4)\n       Bnk\tBanked - Quick and significant rise from street grade to building   (3)\n       HLS\tHillside - Significant slope from side to side                      (2)\n       Low\tDepression                                                          (1)\n\nLandSlope: Slope of property\n\t\t\n       Gtl\tGentle slope    (3)\n       Mod\tModerate Slope\t(2)\n       Sev\tSevere Slope    (1)\n\n\nExterQual: Evaluates the quality of the material on the exterior \n\t\t\n       Ex\tExcellent       (5)\n       Gd\tGood            (4)\n       TA\tAverage/Typical (3)\n       Fa\tFair            (2)\n       Po\tPoor            (1)\n       \nExterCond: Evaluates the present condition of the material on the exterior\n\t\t\n       Ex\tExcellent       (5)\n       Gd\tGood            (4)\n       TA\tAverage/Typical (3)\n       Fa\tFair            (2)\n       Po\tPoor            (1)\n       \nBsmtQual: Evaluates the height of the basement\n\n       Ex\tExcellent (100+ inches)\t  (5)\n       Gd\tGood (90-99 inches)       (4)\n       TA\tTypical (80-89 inches)    (3)\n       Fa\tFair (70-79 inches)       (2)\n       Po\tPoor (<70 inches          (1)\n       NA\tNo Basement               (0)\n\nBsmtCond: Evaluates the general condition of the basement\n\n       Ex\tExcellent                                     (5)\n       Gd\tGood                                          (4)\n       TA\tTypical - slight dampness allowed             (3)\n       Fa\tFair - dampness or some cracking or settling  (2)\n       Po\tPoor - Severe cracking, settling, or wetness  (1)\n       NA\tNo Basement                                   (0)\n\nBsmtExposure: Refers to walkout or garden level walls\n\n       Gd\tGood Exposure                                                               (4)\n       Av\tAverage Exposure (split levels or foyers typically score average or above)  (3)\t\n       Mn\tMimimum Exposure                                                            (2)\n       No\tNo Exposure                                                                 (1)\n       NA\tNo Basement                                                                 (0)\n\nBsmtFinType1: Rating of basement finished area\n\n       GLQ\tGood Living Quarters            (6)\n       ALQ\tAverage Living Quarters         (5)\n       BLQ\tBelow Average Living Quarters\t  (4)\n       Rec\tAverage Rec Room                (3)\n       LwQ\tLow Quality                     (2)\n       Unf\tUnfinshed                       (1)\n       NA\tNo Basement                       (0)\n\nBsmtFinType2: Rating of basement finished area (if multiple types)\n\n       GLQ\tGood Living Quarters            (6)\n       ALQ\tAverage Living Quarters         (5)\n       BLQ\tBelow Average Living Quarters\t  (4)\n       Rec\tAverage Rec Room                (3)\n       LwQ\tLow Quality                     (2)\n       Unf\tUnfinshed                       (1)\n       NA\tNo Basement                       (0)\n\nHeatingQC: Heating quality and condition\n\n       Ex\tExcellent       (5)\n       Gd\tGood            (4)\n       TA\tAverage/Typical (3)\n       Fa\tFair            (2)\n       Po\tPoor            (1)\n\nKitchenQual: Kitchen quality\n\n       Ex\tExcellent       (5)\n       Gd\tGood            (4)\n       TA\tAverage/Typical (3)\n       Fa\tFair            (2)\n       Po\tPoor            (1)\n\nFunctional: Home functionality (Assume typical unless deductions are warranted)\n\n       Typ\tTypical Functionality   (7)\n       Min1\tMinor Deductions 1      (6)\n       Min2\tMinor Deductions 2      (5)\n       Mod\tModerate Deductions     (4)\n       Maj1\tMajor Deductions 1      (3)\n       Maj2\tMajor Deductions 2      (2)\n       Sev\tSeverely Damaged        (1)\n       Sal\tSalvage only            (0)\n       \nFireplaceQu: Fireplace quality\n\n       Ex\tExcellent - Exceptional Masonry Fireplace                                               (5)\n       Gd\tGood - Masonry Fireplace in main level                                                  (4)\n       TA\tAverage - Prefabricated Fireplace in main living area or Masonry Fireplace in basement  (3)\n       Fa\tFair - Prefabricated Fireplace in basement                                              (2)\n       Po\tPoor - Ben Franklin Stove                                                               (1)\n       NA\tNo Fireplace                                                                            (0)\n\n\nGarageFinish: Interior finish of the garage\n\n       Fin\tFinished        (3)\n       RFn\tRough Finished  (2)\t\n       Unf\tUnfinished      (1)\n       NA\tNo Garage         (0)\n       \nGarageQual: Garage quality\n\n       Ex\tExcellent         (5)\n       Gd\tGood              (4)\n       TA\tTypical/Average   (3)\n       Fa\tFair              (2)\n       Po\tPoor              (1)\n       NA\tNo Garage         (0)\n\n\nGarageCond: Garage condition\n\n       Ex\tExcellent         (5)\n       Gd\tGood              (4)\n       TA\tTypical/Average   (3)\n       Fa\tFair              (2)\n       Po\tPoor              (1)\n       NA\tNo Garage         (0)\n\nPoolQC: Pool quality\n\t\t\n       Ex\tExcellent         (4)\n       Gd\tGood              (3)\n       TA\tAverage/Typical   (2)   (TA is never obsevered in pool varibel for traning set)\n       Fa\tFair              (1)\n       NA\tNo Pool           (0)\n       \nFence: Fence quality\n\t\t\n       GdPrv\tGood Privacy    (4)\n       MnPrv\tMinimum Privacy (3)\n       GdWo\tGood Wood         (2)\n       MnWw\tMinimum Wood/Wire (1)\n       NA\tNo Fence            (0)\n\n\nTheir properly also some rank to other varibels, varibels like building matrials must have a ranking in terms of price. But i dont have any idear about whese.\n<details>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Reencoding can be don via match.\n\n# recoding\nrecoding<-function(DF){\n  DF$LotShape<-match(DF$LotShape,c(\"IR3\",\"IR2\",\"IR1\",\"Reg\"))\n  #DF$LandContour<-match(DF$LandContour,c(\"Low\",\"HLS\",\"Bnk\",\"Lvl\"))\n  #DF$LandSlope<-match(DF$LandSlope,c(\"Sev\",\"Mod\",\"Gtl\"))\n  DF$ExterQual<-match(DF$ExterQual,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$ExterCond<-match(DF$ExterCond,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$BsmtQual<-match(DF$BsmtQual,c(\"No Basement\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$BsmtCond<-match(DF$BsmtCond,c(\"No Basement\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$BsmtExposure<-match(DF$BsmtExposure,c(\"No Basement\",\"No\",\"Mn\",\"Av\",\"Gd\"))-1\n  DF$BsmtFinType1<-match(DF$BsmtFinType1,c(\"No Basement\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"))-1\n  DF$BsmtFinType2<-match(DF$BsmtFinType2,c(\"No Basement\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"))-1\n  DF$HeatingQC<-match(DF$HeatingQ,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$KitchenQual<-match(DF$KitchenQual,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$Functional<-match(DF$Functional,c(\"Sal\",\"Sev\",\"Maj2\",\"Maj1\",\"Mod\",\"Min2\",\"Min1\",\"Typ\"))-1\n  DF$FireplaceQu<-match(DF$FireplaceQu,c(\"No Fireplace\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$GarageFinish<-match(DF$GarageFinish,c(\"No Garage\",\"Unf\",\"RFn\",\"Fin\"))-1\n  DF$GarageQual<-match(DF$GarageQual,c(\"No Garage\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$GarageCond<-match(DF$GarageCond,c(\"No Garage\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$PoolQC<-match(DF$PoolQC, c(\"No Pool\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  #DF$Fence<-match(DF$Fence,c(\"No Fence\",\"MnWw\",\"GdWo\",\"MnPrv\",\"GdPrv\"))-1\n  return(DF)\n}\n\ndf<-recoding(df)\n#df_test_for_choice<-recoding(df_test_for_choice)\ndf_test_preformence<-recoding(df_test_preformence)\n```\n:::\n\n\n\n# View varibels\nBelow is a plot of all the variables.\nI prefer histograms over boxplots as they provide a better overview of the distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.csv(df, \"data_before_clean.csv\", row.names = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(purrr)\n\nfor (col in names(df)) {\n  if (is.numeric(df[[col]])) {\n    # Numeriske variabler - Lav histogram\n    n_unique <- length(unique(df[[col]]))\n    p <- ggplot(df, aes_string(x = col)) + \n         geom_histogram(bins = min(n_unique, 30), fill = 'skyblue', color = 'black', alpha = 0.7) + \n         labs(title = paste('Histogram of', col), x = col, y = 'Frequency') + \n         theme_minimal()\n  } else {\n    # Ikke-numeriske variabler - Lav bar plot\n    p <- ggplot(df, aes_string(x = col)) + \n         geom_bar(fill = 'orange', color = 'black', alpha = 0.7) + \n         labs(title = paste('Bar Plot of', col), x = col, y = 'Count') + \n         theme_minimal()\n  }\n  print(p)  # Vis plottet\n}\n```\n:::\n\n\n\n\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(ggplot2)\n\ndata_before <- read.csv(\"https://raw.githubusercontent.com/Missing-almost-everywhere/Missing-almost-everywhere.io/main/Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/data_before_clean.csv\")\n\nui <- fluidPage(\n  titlePanel(\"House Prices Data Visualization\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"variable\", \"Select Variable:\", choices = names(data_before))\n    ),\n    mainPanel(\n      plotOutput(\"dynamicPlot\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  output$dynamicPlot <- renderPlot({\n    col <- input$variable\n    \n    if (is.numeric(data_before[[col]])) {\n      ggplot(data_before, aes(x = .data[[col]])) + \n        geom_histogram(bins = 30, fill = 'skyblue', color = 'black', alpha = 0.7) +\n        labs(title = paste('Histogram of', col), x = col, y = 'Frequency') +\n        theme_minimal()\n    } else {\n      ggplot(data_before, aes(x = .data[[col]])) + \n        geom_bar(fill = 'orange', color = 'black', alpha = 0.7) +\n        labs(title = paste('Bar Plot of', col), x = col, y = 'Count') +\n        theme_minimal()\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\nNotes to historigrams\n\n- MSSubClass their is some cagories without alot of cases.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncounts_df_MSSubClass <- as.data.frame(table(df$MSSubClass))\n\n# Rename the columns for clarity\ncolnames(counts_df_MSSubClass) <- c(\"MSSubClass\", \"Count\")\nprint(counts_df_MSSubClass)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   MSSubClass Count\n1          20   536\n2          30    69\n3          40     4\n4          45    12\n5          50   144\n6          60   299\n7          70    60\n8          75    16\n9          80    58\n10         85    20\n11         90    52\n12        120    87\n13        160    63\n14        180    10\n15        190    30\n```\n\n\n:::\n:::\n\n\n\n- Id value are unique, and should not be incluede (it could have bin the case the same house hade bin sold multipuel times).\n\n- KitchenAbvGr is very skewed in distribution. \n\n## Feature transformations\nIn this section i will list the varibels i have change and what the changes was.\n\n\n<details>\n<summary>Click to expand/collapse varibel Changes</summary>\n\nLooking at the description of the data the varibesl with lowe count, is \n\n-  40:   1-STORY W/FINISHED ATTIC ALL AGES \n-  45:   1-1/2 STORY - UNFINISHED ALL AGES\n-  75:   2-1/2 STORY ALL AGES\n-  85:   SPLIT FOYER\n- 185:   PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n\n\nHere is what i will combine them to.\n\n40 will be combined with 50\n- 50:\t  1-1/2 STORY FINISHED ALL AGES\nMaking  1-1/2 STORY FINISHED ALL AGES\n\n85 will be combined with 80 \n- 80:   SPLIT OR MULTI-LEVEL\nMaking  Split\n\n185 will be combined with 160 \n- 160:\t2-STORY PUD - 1946 & NEWER\nMaking  PUD - MULTILEVEL\n\nThis only leaves 45: UNFINISHED ALL AGES\n\nwith a vary small number of obsevations, in this section we looking at the data but this kind of asumption will affect models choice, what is the value of unfinished house, alle ages.\nA unfinish house can either be a cheap way to get new house ore a extra expens since it proberly should be removed. Both of these would proberly have lower value than the 1-1/2 STORY FINISHED ALL AGES. So if i inclued them i can drag down the estimat for this groups, and this can be unproporsional if the model is fitted with least sqaur. \nIf i dont comined it with a varibel want to use a interaction effect in linear model I will get model wiht a lot NA, where this the combinations is not represented, i can get around this by tackling overide the model, so basically  get to make geuss in whose case, that could be the average house price, something more cleaver, so for now i will leave it in.\nIf i have to do the overird i will tell, this is better than a zero score. In the total linear model this is not a problem, so i may be the case this not problem.\n\n\n- YearBuilt Som of this varibel is allready pressent in MSSubClass\n\n- GarageCars I have think about, it would give nice distribution if it was seplified belove ore equalt 2 cars with true and false. but i think their is more infomation in o zeror\nso the the varibel vill be zero, one, two, above 2\n\n\nLooking at this it seam like it would be a god idear to cagaorise/colabse some of these varibel.\nThis is to try to simplify the information. The problem that can arise is that with some of these varibels, they have such ueven distribution that, one can end up fitting a combination to uniqe house. this could be good fine but, in some case by colsaping one can better overall predictiv results. Later I will proberly reduce the dimension based on either spearman ore kendall tau corelation so one would want to ensure that if their is rank in the data is preserved.\nTo give a exampel number of fireplace can be reduces to Yes or No. Yes would stille be higer than No.\n\n- fireplace siplified to binary yes and NO\n\n- PoolArea will be change to yes no their is not alot of information \n-pool yes No\n\n\nFor these i could it could potensiel be a good idear to eihter make fullbath total Halfbath total. \nore colabs them in to yes no\n\n-   Fullbath \n-   Halfbath  \n-   bsmtFullBath \n-   BsmtHalfBath \n-   Fenche will be change to True ore FAlSE\n-   SaleCondition will be normal True ore False\n-   LandContour will be change to level ore not. I will stil count this haveing a order, meaning level wich is equal to one is prefered\n-   LandSlope will be change to ground level ore not.\n\nFoundation will be combined by combin all other than \"PConc\" ore \"CBlock\"\nto \"other\"\n\nLotConfig all other than Corner Inside will be combined\n\nRoofStyle will be \"Gable\" or \"other\" \n\nThe one data point in MiscFeature that is tenis court will go under other\n\nIn Condition1 the following\nRRNe\tWithin 200' of East-West Railroad\nRRAe\tAdjacent to East-West Railroad\nRRNn\tWithin 200' of North-South Railroad\nRRAn\tAdjacent to North-South Railroad\n\nWill be combied to (Near Railroad) \n\nIn the Exterior1st and Exterior2nd. Their is some catagories with single obsevations in, I will put them in the closet cagori.\n\n\n<details>\n\nI have rapt all the change in a function so it esay to aplly to the test dataset aswell\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodify_df <- function(DF){\n  DF$MSSubClass[DF$MSSubClass == 40] <- 50\n  DF$MSSubClass[DF$MSSubClass == 85] <- 80\n  DF$MSSubClass[DF$MSSubClass == 185] <- 160\n  DF$have_Garage<-DF$GarageCars>0\n  DF$GarageCars<-NULL\n  # is Fireplace pressent\n  DF$Fireplaces_present<-DF$Fireplaces>0\n  DF$Fireplaces<-NULL\n  DF$FireplaceQu<-NULL\n  # is pool pressent\n  DF$pool_present<-DF$PoolArea>0\n  DF$PoolArea<-NULL\n  # Full bath pressent above ground\n  DF$FullBath_total<-DF$BsmtFullBath+DF$FullBath\n  DF$BsmtFullBath<-NULL\n  DF$FullBath<-NULL\n  DF$HalfBath_total<-DF$HalfBath+DF$BsmtHalfBath\n  DF$HalfBath<-NULL\n  DF$BsmtHalfBath<-NULL\n  DF$Alley<-NULL\n  DF$Fence<-DF$Fence!=\"No Fence\"\n  DF$SaleCondition<-DF$SaleCondition==\"Normal\"\n  DF$LandContour<-DF$LandContour==\"Lvl\"\n  DF$LandSlope<-DF$LandSlope==\"Gtl\"\n  DF$Foundation[!(DF$Foundation %in% c(\"PConc\", \"CBlock\"))] <- \"Other\"\n  DF$LotConfig[!(DF$LotConfig %in% c(\"Corner\",\"Inside\"))]<-\"Other\"\n  DF$RoofStyle<-DF$RoofStyle!=\"Gable\"\n  DF$MiscFeature[DF$MiscFeature==\"TenC\"]<-\"Othr\"\n  DF$Condition1[DF$Condition1 %in% c(\"RRNe\",\"RRAe\",\"RRNn\",\"RRAn\")]<-\"NR\"\n  DF$Exterior1st[DF$Exterior1st==\"AsphShn\"]<-\"AsbShng\"\n  DF$Exterior1st[DF$Exterior1st==\"CBlock\"]<-\"CemntBd\"\n  DF$Exterior1st[DF$Exterior1st==\"ImStucc\"]<-\"Stucco\"\n  DF$Exterior2nd[DF$Exterior2nd==\"CBlock\"]<-\"CemntBd\"\n  DF$Exterior2nd[DF$Exterior2nd==\"Other\"]<-\"plywood\"\n  return(DF)\n}\n\ndf=modify_df(df)\n#df_test_for_choice<-modify_df(df_test_for_choice)\ndf_test_preformence<-modify_df(df_test_preformence)\n```\n:::\n\n\n\n\n\n## Remove varibels\nIn this section, I will list the variables I plan to remove or disregard.\nThe reason for their removal is that their distributions are very skewed compared to other variables, which I deem similar in the information they provide.\n\nThe main reason for disregarding these variables is that I do not believe they have high predictive strength, and I need to move forward with the data cleaning process.\nAlso, I’m not getting paid for this.\n\n<details>\n<summary>Click to expand/collapse varibel removed</summary>\n-   X3SsnPorch no real information will be removed (removed)\n\n-   ScreenPorch no information aviable will be removed\n\n-   Alley contain no information and will be removed\n\n-   street contain not information and will be removed\n\n-   Utilities contain not information and will be removed\n\n-   Condition2 contain not information and will be removed\n\n-   RoofMatl contain not information and will be removed will be removed \n\n-   Heating contain not information and will be removed will be removed \n\n-   CentralAir contain not information and will be removed will be removed \n\n-   PoolQC contain not information and will be removed will be removed \n\n-   MiscFeature contain not information and will be removed will be removed \n\n-   GarageQual\n\n-   GarageCond\n\n-   GarageType\n\n-   Electrical\n\n<details>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nremove_varibels<-function(DF){\n  DF$X3SsnPorch<-NULL\n  DF$ScreenPorch<-NULL\n  DF$Alley<-NULL\n  DF$Street<-NULL\n  DF$Utilities<-NULL\n  DF$Condition2<-NULL\n  DF$RoofMatl<-NULL\n  DF$Heating<-NULL\n  DF$CentralAir<-NULL\n  DF$PoolQC<-NULL\n  DF$SaleType<-NULL\n  DF$GarageQual<-NULL\n  DF$GarageCond<-NULL\n  DF$GarageType<-NULL\n  DF$Electrical<-NULL\n  return(DF)\n}\n\ndf=remove_varibels(df)\n#df_test_for_choice<-remove_varibels(df_test_for_choice)\ndf_test_preformence<-remove_varibels(df_test_preformence)\n```\n:::\n\n\n\n## Variables After Cleanup\nBelow, I have plotted the variables remaining after the cleanup.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.csv(df, \"data_after_clean.csv\", row.names = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(purrr)\n\nfor (col in names(df)) {\n  if (is.numeric(df[[col]])) {\n    # Numeriske variabler - Lav histogram\n    n_unique <- length(unique(df[[col]]))\n    p <- ggplot(df, aes_string(x = col)) + \n         geom_histogram(bins = min(n_unique, 30), fill = 'skyblue', color = 'black', alpha = 0.7) + \n         labs(title = paste('Histogram of', col), x = col, y = 'Frequency') + \n         theme_minimal()\n  } else {\n    # Ikke-numeriske variabler - Lav bar plot\n    p <- ggplot(df, aes_string(x = col)) + \n         geom_bar(fill = 'orange', color = 'black', alpha = 0.7) + \n         labs(title = paste('Bar Plot of', col), x = col, y = 'Count') + \n         theme_minimal()\n  }\n  print(p)  # Vis plottet\n}\n```\n:::\n\n\n\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(ggplot2)\n\ndata_before <- read.csv(\"https://raw.githubusercontent.com/Missing-almost-everywhere/Missing-almost-everywhere.io/main/Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/data_after_clean.csv\")\n\nui <- fluidPage(\n  titlePanel(\"House Prices Data Visualization\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"variable\", \"Select Variable:\", choices = names(data_before))\n    ),\n    mainPanel(\n      plotOutput(\"dynamicPlot\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  output$dynamicPlot <- renderPlot({\n    col <- input$variable\n    \n    if (is.numeric(data_before[[col]])) {\n      ggplot(data_before, aes(x = .data[[col]])) + \n        geom_histogram(bins = 30, fill = 'skyblue', color = 'black', alpha = 0.7) +\n        labs(title = paste('Histogram of', col), x = col, y = 'Frequency') +\n        theme_minimal()\n    } else {\n      ggplot(data_before, aes(x = .data[[col]])) + \n        geom_bar(fill = 'orange', color = 'black', alpha = 0.7) +\n        labs(title = paste('Bar Plot of', col), x = col, y = 'Count') +\n        theme_minimal()\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n## Looking Into Cross-Correlation\nBelow, I have plotted the correlation matrix for the variables.\nSince many of these variables are categorical but have a ranking, I used Spearman correlation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf=na.omit(df)\nlibrary(corrplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ncorrplot 0.95 loaded\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(Hmisc)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Hmisc'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n```\n\n\n:::\n\n```{.r .cell-code}\nnumeric_df <- df[sapply(na.omit(df), is.numeric)]\n\n# Compute Spearman correlation matrix\nspearman_corr <- cor(numeric_df, method = \"spearman\")\n\n# Compute p-values (optional)\nlibrary(Hmisc)\nres <- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr <- res$r    # Correlation coefficients\n\n# Plot the correlation matrix\ncorrplot(spearman_corr, method = \"color\",tl.col = \"black\", tl.srt = 60, insig = \"blank\",tl.cex = 0.5,title = \"Spearman Correlation Plot\")\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nNotes on the Plot\nOverall, the internal correlation is a lot smaller than I thought, which is good. This should make it easier to find good predictors.\n\nThe third variable from the bottom is the sales price. From this, one can see which variables could be potential predictors. It seems like OverallQual could be a really good predictor.\n\nHowever, correlation only captures linear effects, so one should be cautious about relying solely on correlation for feature selection. Imagine there was a feature with a sinusoidal curve that perfectly described the data—it would have a correlation of zero.\n\n## Reducing Variables Based on Multicollinearity\n\nIn this section, I will reduce some variables, as multicollinearity can pose challenges for certain models. While some partitioning-based models are less sensitive to multicollinearity, linear models can be significantly impacted. In a simple linear model with two inputs, the variance of a parameter can be expressed as:  \n\n$$\\text{Var}(\\beta_1) = \\frac{\\sigma^2}{1 - \\text{cor}(x_1, x_2)}$$  \n\nThis means that as the correlation between two variables increases, the variance of the estimate explodes. Consequently, certain models are sensitive to multicollinearity.  \n\nOne approach to addressing this is to remove variables with a correlation above a certain threshold. However, determining the optimal threshold is not straightforward. For a specific model, simulations can be conducted to estimate a suitable value. In this case, I want to perform some reduction before applying any models. I have chosen a cutoff point of \\(|0.7|\\). If there is a high correlation between two variables, I will remove the one with the weaker correlation to the log of the sale price.  \n\nSince the variables are not numeric but do have a ranking, PCA is not an option. However, Spearman correlation can still be used.  \n\nFor variable reduction, hierarchical clustering can be applied using the absolute value of the Spearman correlations as a distance measure. Single-linkage clustering can then be employed, with a slight reformulation of the problem. Hierarchical clustering requires a distance metric, and for this purpose, the distance between two variables based on correlation can be defined as:  \n\n$d(x_1, x_2) = 1 - |\\text{cor}(x_1, x_2)|$  \n\nGiven the cutoff of \\(|0.7|\\) for correlation, the corresponding distance after reformulation would be:  \n\n$1 - 0.7 = 0.3$  \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The code here is not optimized but it was fast to write\nnumeric_df <- df[sapply(na.omit(df), is.numeric)]\n\nnumeric_saleprice<-numeric_df$SalePrice\nnumeric_df$SalePrice<-NULL # remove from dataframe\n\nlibrary(Hmisc)\nres <- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr <- res$r    # Correlation coefficients\n\n\ndist_matrix <- as.dist(1-abs(spearman_corr))\n\nhc <- hclust(dist_matrix, method = \"single\")\n\n\nclusters <- cutree(hc, h = 0.3) #\n\n# Visualize dendrogram\nplot(hc, main = \"Hierarchical Clustering Dendrogram\")\nabline(h = 0.3, col = \"red\") # Add cutoff line\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# reduce varibels\nnumeric_saleprice_corr <- rcorr(as.matrix(numeric_df), numeric_saleprice, type = \"spearman\")#$r[, \"numeric_saleprice\"]\n\n\n# Step 2: Identify clusters with multiple variables\nduplicates <- table(clusters)[table(clusters) > 1]\nclusters_with_duplicates <- names(duplicates)\n\n\nfor (cluster in clusters_with_duplicates) {\n  clusters_with_duplicates\n  n_cluster=length(names(clusters[clusters==cluster]))\n  spear_var=rep(NA,n_cluster)\n  names_cluster=names(clusters[clusters==cluster])\n  for (i in 1:n_cluster){\n    names(clusters[clusters==cluster])[i]\n    spear_var[i]=rcorr(as.matrix(numeric_df[names(clusters[clusters==cluster])[i]]),log(numeric_saleprice),type=\"spearman\")$r[1,2]\n  }\n  index_of_max <- which.max(spear_var)\n  names_cluster[-index_of_max]\n  #numeric_df[names_cluster[-index_of_max]]<-NULL\n  df[names_cluster[-index_of_max]]<-NULL\n  #df_test_for_choice[names_cluster[-index_of_max]]<-NULL\n  df_test_preformence[names_cluster[-index_of_max]]<-NULL\n}\n```\n:::\n\n\n\nWith the clusters found, for each cluster, the feature with the highest Spearman correlation to the log of the sale price will be chosen.  \nAs a result, there was a reduction from 61 to 41 features.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnumeric_df <- df[sapply(na.omit(df), is.numeric)]\n\nnumeric_saleprice<-numeric_df$SalePrice\nnumeric_df$SalePrice<-NULL # remove from dataframe\n\nres <- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr <- res$r    # Correlation coefficients\n\n\ndist_matrix <- as.dist(1-abs(spearman_corr))\n\nhc <- hclust(dist_matrix, method = \"single\")\n\n\nclusters <- cutree(hc, h = 0.3) #\n\n# Visualize dendrogram\nplot(hc, main = \"Hierarchical Clustering Dendrogram\")\nabline(h = 0.3, col = \"red\") # Add cutoff line\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\nAs can be seen in the new dendrogram, the dataframe has been reduced to meet the requirements.  \n\n## Overview of Variables with the Highest Correlation to Log of Sale Price\n\nBelow, I have ordered the variables based on the absolute value of their Spearman correlation.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Required libraries\nlibrary(corrplot)\nlibrary(Hmisc)\n\n# Subset the dataframe to numeric columns\nnumeric_df$log_saleprice<-log(df$SalePrice)\nnumeric_df$SalePrice<-NULL\n# Ensure the target variable is present\ntarget_variable <- \"log_saleprice\"\nif (!target_variable %in% colnames(numeric_df)) {\n  stop(\"Target variable not found in numeric dataframe.\")\n}\n\n# Compute Spearman correlation matrix and p-values\nres <- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr <- res$r    # Correlation coefficients\n\n# Extract correlation with the target variable\ntarget_corr <- spearman_corr[, target_variable, drop = FALSE]\ntarget_corr <- as.data.frame(target_corr)\ncolnames(target_corr) <- \"Spearman_Correlation\"\n\n# Add a column indicating if the correlation could be computed\ntarget_corr$Computed <- !is.na(target_corr$Spearman_Correlation)\n\n# Sort variables by Spearman correlation in descending order\nsorted_corr <- target_corr[order(abs(target_corr$Spearman_Correlation), decreasing = TRUE), ]\n\n# Display sorted variables with their correlation and computed status\nprint(sorted_corr[1:10,])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Spearman_Correlation Computed\nlog_saleprice             1.0000000     TRUE\nOverallQual               0.8087095     TRUE\nGrLivArea                 0.7304531     TRUE\nBsmtQual                  0.6760372     TRUE\nGarageArea                0.6477811     TRUE\nFullBath_total            0.6371933     TRUE\nGarageFinish              0.6323531     TRUE\nTotalBsmtSF               0.6023901     TRUE\nYearRemodAdd              0.5688537     TRUE\nHeatingQC                 0.4906598     TRUE\n```\n\n\n:::\n:::\n\n\n\nLook at the Year sold the collation is really low.\nthe spand of the years it relly lowe, this would idicate that the price do not raise a lot over time in this area. I would have expted some kind of increase to compensate for inflation. \n\nBelow i have recoded the Time as months since 2006, 1 being january. I also plottet logsale price as agianst the month\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\nnumeric_df$time<-NA\nfor (i in 1:length(df$MoSold)){\n  numeric_df$time[i]<-(numeric_df$YrSold[i]-2006)*12+numeric_df$MoSold[i]\n}\n\nyearly_means <- numeric_df %>%\n  group_by(time) %>%\n  summarise(mean_value = mean(log_saleprice))\n\n# Plot residuals against time\nggplot(data=numeric_df[!is.na(numeric_df$log_saleprice),], aes(x=numeric_df$time, y=log_saleprice)) +\n  geom_point(alpha = 0.6, color = \"black\") +  # Individual data points\n  geom_line(data = yearly_means, aes(x = time, y = mean_value, color = \"Mean Sold Price\"), \n            size = 1, show.legend = TRUE) +\n  geom_point(data = yearly_means, aes(x = (time), y = mean_value),\n             color = \"red\", size = 2) +\n  labs(\n    x = \"Month\",\n    y = \"Value\",\n    title = \"Monthly Data with Mean Overlay\"\n  ) +\n  theme_minimal()# caluculate mean \n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmonthly_means <- numeric_df %>%\n  group_by(MoSold) %>%\n  summarise(mean_value = mean(log_saleprice))\n\n\n\nggplot(data=numeric_df[!is.na(numeric_df$log_saleprice),], aes(x=numeric_df$MoSold, y=log_saleprice)) +\n  geom_point(alpha = 0.6, color = \"black\") +  # Individual data points\n  geom_line(data = monthly_means, aes(x = MoSold, y = mean_value, color = \"Mean Sold Price\"), \n            size = 1, show.legend = TRUE) +\n  geom_point(data = monthly_means, aes(x = (MoSold), y = mean_value),\n             color = \"red\", size = 2) +\n  labs(\n    x = \"Month\",\n    y = \"Value\",\n    title = \"Monthly Data with Mean Overlay\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-20-2.png){width=672}\n:::\n:::\n\n\n\nOverall their dont seam to be any big conettiction between pice and time. From the moth plot it clear if ther varinace of the price is dependen on the time ore if the salevolum changes letting to less spread.\n\n\n## OverallQual \nOverallQual has high corelation with saleprice.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data=df[!is.na(df$SalePrice),], aes(x=factor(OverallQual), y=log(SalePrice)))+\n        geom_point()+ labs(title=\"OverallQual\")\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\nlog price seam linear related to overall quality.\n\n## Ground lvving area\nGround living area.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data=df[!is.na(df$SalePrice),], aes(x=factor(GrLivArea), y=log(SalePrice)))+\n        geom_point()+ labs(title=\"GrLivArea\")\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\n## Plotting Variables Against logsaleprice\nSome of these variables do not have natural ordering, which means computing Spearman correlation is not possible. In that case, the best option (I know) is to plot the target variable against the categories in the variables. There are different forms, such as boxplots or violin plots. I have chosen violin plots since they have more resemblance to a histogram, which I prefer over boxplots.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.csv(df, \"data_for_final_model.csv\", row.names = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nnon_numeric_cols <- df %>%\n  select_if(~!is.numeric(.)) %>%\n  colnames()\n\nfor (col in non_numeric_cols) {\n  p <- ggplot(df, aes_string(x = col, y = \"log(SalePrice)\")) +  \n    geom_violin() +\n    stat_summary(fun = \"mean\", geom = \"point\", color = \"red\", size = 3) +  # Tilføj mean som rødt punkt\n    labs(title = paste(\"Violin plot for\", col, \"vs log(SalePrice)\")) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  \n  print(p)  # Udskriv violin plot for hver iteration\n}\n```\n:::\n\n\n\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata_final <- read.csv(\"https://raw.githubusercontent.com/Missing-almost-everywhere/Missing-almost-everywhere.io/main/Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/data_for_final_model.csv\")\n\nui <- fluidPage(\n  titlePanel(\"None Numerical varibels\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"variable\", \"Select Variable:\", \n                  choices = names(data_final))\n    ),\n    mainPanel(\n      plotOutput(\"dynamicPlot\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  output$dynamicPlot <- renderPlot({\n    col <- input$variable\n    \n    if (is.numeric(data_final[[col]])) {\n      if(col != \"SalePrice\") {\n        ggplot(data_final, aes(x = .data[[col]], y = log(SalePrice))) +\n          geom_point(alpha = 0.5) +\n          labs(title = paste(col, \"vs log(SalePrice)\")) +\n          theme_minimal()\n      } else {\n        ggplot(data_final, aes(x = .data[[col]])) +\n          geom_histogram(bins = 30, fill = 'skyblue', color = 'black', alpha = 0.7) +\n          labs(title = paste('Histogram of', col)) +\n          theme_minimal()\n      }\n    } else {\n      ggplot(data_final, aes(x = .data[[col]], y = log(SalePrice))) +\n        geom_violin(fill = 'skyblue', alpha = 0.7) +\n        stat_summary(fun = mean, geom = \"point\", color = \"red\", size = 3) +\n        labs(title = paste(col, \"vs log(SalePrice)\")) +\n        theme_minimal() +\n        theme(axis.text.x = element_text(angle = 45, hjust = 1))\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\n```\n\n\nSo what I am looking for is something their it seam like the means i sale price is different between the factors. Neighborhood, seam like a good varibel to include, in many intances on can see that the their is not even a overlab in the observed price.\n\nThe roofing seam like it could have potesial \n\n\n# Models\nI start by using some simple models, wheer I use some intuition and look at the residuals, then i move on to some more advance techies.\nFist i am gona plot some models and look at residuals, compare to some other varibels.\nI like to start this way since i give me a intuitions abote the data. The models made this way have the advantage their a esay to explain. if some one want to trade some preditiv stregnth for some explaniabillaty.\nPersonaly, I am still gona use of sample preditv strength to evaluat the models. preferabelly by corss validation\n\n## Base line mocel\nAfter Plotting different models and looking the residual i found the below model.\nWhere the saleprice pr sqft of living area is combination the Neighborhood the type of dwelling (MSSubClass)\n\nwich could be good candidate if one look the residuals being drawn from af t distribusion.\nSo more heavy tale distribution, than a normal distribution. In many ways this model seam resanbull from a intuition point of view.  \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot original data\n\nmodel_baseline <-lm((SalePrice/GrLivArea) ~ TotalBsmtSF*MSSubClass:Neighborhood:GrLivArea , data = df) # lm((SalePrice / GrLivArea) ~ TotalBsmtSF+MSSubClass+Neighborhood,data=df)\n\n\nplot(df$SalePrice,log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea))\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea),breaks=50)\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-25-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# The histogram almost look like a t distribution\nlibrary(MASS)\nfitdistr(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea), \"t\",start = list(m=mean(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea)),s=sd(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea)), df=2),lower=c(-1, 0.001,1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        m              s              df     \n  -0.009049068    0.156394619    4.526172860 \n ( 0.004804025) ( 0.005221797) ( 0.565497406)\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(Dowd)\nTQQPlot(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea),df=4.5 )\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-25-3.png){width=672}\n:::\n:::\n\n\n\n\n## A linear interaction model.\nBelove I have fitted a interaction model where log saleprice is a interactiv funtion between \nThe type of house, the neighborhood and the size of the dewelling+ sum effect from the size of the basement. It seam reasonbell and is somewat how i would estimate a type of house for the overall price.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel_second_baseline<- lm(log(SalePrice)~TotalBsmtSF+MSSubClass:GrLivArea:Neighborhood,data=df)\n\n\nlibrary(Dowd)\nTQQPlot(Model_second_baseline$residuals,df=4.5 )\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Define the custom metric for CV evaluation\nlog_diff_metric_model_baseline <- function(data, label) {\n  groups=unique(label)\n  groups_score=rep(NA,length(groups))\n  \n  for (i in 1:length(groups)){\n    groupe_for_test=groups[i]\n    partion_vector_train <- which(label!= groupe_for_test)\n    partion_vector_test <- which(label== groupe_for_test)\n    train_data <- data[partion_vector_train, ] \n    test_data <- data[partion_vector_test, ] \n    # fit model \n    model_baseline <-lm(log(SalePrice)~TotalBsmtSF+MSSubClass:GrLivArea:Neighborhood,data=train_data)\n    # get predition \n    predicted_SalePrice <- predict(model_baseline, newdata = test_data) \n    # get log diff\n    log_diff <- log(test_data$SalePrice) - (predicted_SalePrice)\n    # save results\n    groups_score[i]<-sum(log_diff**2)\n  }\n  \n  return(sum(groups_score)/length(data$SalePrice))\n}\n\n# make labels\n\nn_cv=50\n\nlabels=sample(rep(seq(1,n_cv),floor(length(df$Id)/n_cv)))\nif(length(df$Id)-floor(length(df$Id)/n_cv)*n_cv!=0){\n  labels<-c(labels,seq(1:(length(df$Id)-floor(length(df$Id)/n_cv)*n_cv)))\n}\nlabels=sample(labels)\n\n\nprint(\"RMSE of log prices\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"RMSE of log prices\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(log_diff_metric_model_baseline(df,labels))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0641033\n```\n\n\n:::\n:::\n\n\nOver all the predicative stregth look good the reisudall of the model looks more simullaro to t distribuin than a normal.\n\n\n# Advances model\n\n## Lasso\nIn this chunck of code i fit lasso model for the dataframe, by asuming a linear model over all the vearibels in the datafram penlized by the size of beta.\nThe Lasso solves the following\n\n$$\\underset{\\beta}{argmin} ||log(Y)-\\beta X||_{2}^2+\\lambda||\\beta||_1$$.\n\nIt penalizes the size of the coefficients (beta). Since the penalty is based on the $L_1$\n -norm, it tends to set some coefficients to zero. This encourages sparse solutions, where only the most relevant features are retained.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n\n# Prepare the data\ndf_lasso <- df\nlog_saleprice <- log(df_lasso$SalePrice)\ndf_lasso$SalePrice <- NULL\ndf_lasso$log_saleprice <- log_saleprice  # Add log_saleprice to df_lasso\ndf_lasso_id<-df_lasso$Id\ndf_lasso$Id<-NULL\n# Remove rows with NA values\ndf_lasso <- na.omit(df_lasso)\n\n# Split data into folds\nk <- 10\nfolds <- sample(1:k, size = nrow(df_lasso), replace = TRUE)\n\n# Initialize a vector to store MSEs for each fold\ncv_mse <- numeric(k)\nbest_lambda_vector <- rep(NA, k)\n\nfor (i in 1:k) {\n  # Split data into training and test sets\n  train_indices <- which(folds != i)\n  test_indices <- which(folds == i)\n  \n  train_data <- df_lasso[train_indices, ]\n  test_data <- df_lasso[test_indices, ]\n  \n  # Create design matrices\n  X_train <- model.matrix(log_saleprice ~ . , data = train_data)\n  X_test <- model.matrix(log_saleprice ~ . , data = test_data)\n  \n  # Fix column mismatch between X_train and X_test\n  common_columns <- intersect(colnames(X_train), colnames(X_test))\n  X_test <- X_test[, common_columns, drop = FALSE]\n  X_train <- X_train[, common_columns, drop = FALSE]\n  \n  # Fit LASSO model with cross-validation to choose lambda\n  lasso_model <- cv.glmnet(X_train, train_data$log_saleprice, alpha = 1, family = \"gaussian\")\n  best_lambda <- lasso_model$lambda.min\n  best_lambda_vector[i] <- best_lambda\n  \n  # Predict on test data\n  predictions <- predict(lasso_model, s = best_lambda, newx = X_test)\n  \n  # Compute MSE for this fold\n  cv_mse[i] <- mean((test_data$log_saleprice - predictions)^2)\n}\n\n# Average MSE across all folds\nmean_cv_mse <- mean(cv_mse)\ncat(\"Mean Cross-Validated MSE:\", mean_cv_mse, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean Cross-Validated MSE: 0.0229118 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Fit final LASSO model with the average best lambda\nmean_best_lambda <- mean(best_lambda_vector, na.rm = TRUE)  # Calculate mean lambda\nX_full <- model.matrix(log_saleprice ~ . , data = df_lasso)\nfinal_lasso_model <- glmnet(X_full, log_saleprice, alpha = 1, lambda = mean_best_lambda, family = \"gaussian\")\n\n# Print final model coefficients\nprint(mean_cv_mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0229118\n```\n\n\n:::\n:::\n\n\n\nAs observed, the predictive strength of the model is better than the baseline model, but not by a lot in terms of predictive performance.\n\n## XGBoost\nXGBoost is an ensemble learning method that uses repeated decision trees, where each tree is fitted on the residuals of the previous one. Essentially, it’s a sophisticated method of partitioning the dataset.\n\nThis approach has gained significant popularity, so I decided to give it a try.\nSince XGBoost relies on tree structures for partitioning, the data must have an inherent ordering. To include categorical variables, I applied one-hot encoding to transform them into a suitable format.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xgboost)\nlibrary(data.table)\n\n# Prepare the dataset\ndf_xgb <- df\ntarget <- log(df_xgb$SalePrice)  # Log-transform the target variable\ndf_xgb$SalePrice <- NULL         # Remove the target variable from features\ndf_xgb_Id <- df_xgb$Id\ndf_xgb$Id <- NULL\n\n# One-hot encode categorical variables\ndf_xgb <- as.data.table(df_xgb)\ndf_xgb <- model.matrix(~ . - 1, data = df_xgb)  # Perform one-hot encoding and remove intercept\n\n# Create DMatrix\nddata <- xgb.DMatrix(data = df_xgb, label = target)\n\n# Set parameters for the XGBoost model\nparams <- list(\n  objective = \"reg:squarederror\",  # Regression task\n  eta = 0.1,                       # Learning rate\n  max_depth = 6,                   # Maximum depth of trees\n  subsample = 0.8,                 # Subsampling ratio\n  colsample_bytree = 0.8           # Feature subsampling ratio\n)\n\n# Perform 10-fold cross-validation\nset.seed(123)  # For reproducibility\ncv_results <- xgb.cv(\n  params = params,\n  data = ddata,\n  nrounds = 100,                    # Number of boosting rounds\n  nfold = 10,                       # Number of folds for cross-validation\n  metrics = \"rmse\",                 # Evaluation metric\n  early_stopping_rounds = 10,       # Stop early if no improvement\n  print_every_n = 10,               # Print progress every 10 rounds\n  verbose = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]\ttrain-rmse:10.379960+0.002592\ttest-rmse:10.379961+0.026888 \nMultiple eval metrics are present. Will use test_rmse for early stopping.\nWill train until test_rmse hasn't improved in 10 rounds.\n\n[11]\ttrain-rmse:3.637180+0.001203\ttest-rmse:3.637569+0.017285 \n[21]\ttrain-rmse:1.286535+0.000669\ttest-rmse:1.287011+0.011197 \n[31]\ttrain-rmse:0.469571+0.000640\ttest-rmse:0.476945+0.007929 \n[41]\ttrain-rmse:0.190574+0.000775\ttest-rmse:0.215138+0.012590 \n[51]\ttrain-rmse:0.102444+0.000572\ttest-rmse:0.147398+0.018300 \n[61]\ttrain-rmse:0.075914+0.000609\ttest-rmse:0.132825+0.019453 \n[71]\ttrain-rmse:0.065099+0.000603\ttest-rmse:0.129387+0.019381 \n[81]\ttrain-rmse:0.057990+0.001078\ttest-rmse:0.128064+0.019680 \n[91]\ttrain-rmse:0.052257+0.001140\ttest-rmse:0.127166+0.019725 \n[100]\ttrain-rmse:0.047739+0.001064\ttest-rmse:0.126684+0.019586 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Extract the best number of boosting rounds\nbest_nrounds <- cv_results$best_iteration\ncat(\"Best number of rounds:\", best_nrounds, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBest number of rounds: 98 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Train final model using the best number of rounds\nfinal_model <- xgb.train(\n  params = params,\n  data = ddata,\n  nrounds = best_nrounds\n)\n\n# Evaluate final model on entire dataset (or split as desired)\npredictions <- predict(final_model, ddata)\noverall_rmse <- sqrt(mean((target - predictions)^2))\ncat(\"RMSE on entire dataset:\", overall_rmse, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRMSE on entire dataset: 0.05299007 \n```\n\n\n:::\n:::\n\n\n\n\n# Getting ready for predition\nOverall, it seems like the best option is the lasso model for prediction.  \n\n## Prepration of test set.\nThere are some missing values, which I address using the K-Nearest Neighbors (KNN) algorithm. This method is particularly effective for handling missing values in categorical variables.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(\"df_test_preformence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"df_test_preformence\"\n```\n\n\n:::\n\n```{.r .cell-code}\ntable(df_test_preformence$MSSubClass)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n 20  30  45  50  60  70  75  80  90 120 150 160 180 190 \n543  70   6 145 276  68   7  88  57  95   1  65   7  31 \n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"df\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"df\"\n```\n\n\n:::\n\n```{.r .cell-code}\ntable(df$MSSubClass)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n 20  30  45  50  60  70  75  80  90 120 160 180 190 \n532  67  12 148 296  60  16  78  52  86  63  10  30 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_test_preformence$MSSubClass[df_test_preformence$MSSubClass==150]=160\n```\n:::\n\n\n\nmissing values \n\n\n::: {.cell}\n\n```{.r .cell-code}\nvis_miss(df_test_preformence)\n```\n\n::: {.cell-output-display}\n![](Housing-pricing-project_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(VIM)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'VIM' was built under R version 4.4.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: colorspace\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: grid\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nVIM is ready to use.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSuggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'VIM'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:bootstrap':\n\n    diabetes\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:datasets':\n\n    sleep\n```\n\n\n:::\n\n```{.r .cell-code}\n# Impute missing values using KNN\nimputed_data <- kNN(df_test_preformence, k = 5)\n\n# the imputed_data ad a new colum to say what vaules is imputed i am remvoving them.\ndf_test_preformence<-imputed_data[,1:dim(df_test_preformence)[2]]\n\n# Check the updated dataframe\ndim(df_test_preformence)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1459   54\n```\n\n\n:::\n:::\n\n\nTheir is one Plywood in test set\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsetdiff(unique(df$Exterior2nd),unique(df_test_preformence$Exterior2nd))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"plywood\"\n```\n\n\n:::\n\n```{.r .cell-code}\ntable(df_test_preformence$Exterior2nd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAsbShng AsphShn Brk Cmn BrkFace CemntBd CmentBd HdBoard ImStucc MetalSd Plywood \n     18       1      15      22       2      66     199       5     233     128 \n  Stone  Stucco VinylSd Wd Sdng Wd Shng \n      1      21     511     194      43 \n```\n\n\n:::\n\n```{.r .cell-code}\ntable(df$Exterior2nd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAsbShng AsphShn Brk Cmn BrkFace CemntBd CmentBd HdBoard ImStucc MetalSd plywood \n     20       3       7      25       1      58     207      10     213       1 \nPlywood   Stone  Stucco VinylSd Wd Sdng Wd Shng \n    142       4      26     499     196      38 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nX_full <- model.matrix(log_saleprice ~ . , data = df_lasso)\nfinal_lasso_model <- glmnet(X_full, df_lasso$log_saleprice, alpha = 1, lambda = mean_best_lambda, family = \"gaussian\")\n\ndf_lasso_evalatio <- df_test_preformence\ndf_lasso_evalatio_id<-df_lasso_evalatio$Id\ndf_lasso_evalatio$Id<-NULL\n# Create design matrix for prediction from test data\nX_predict <- model.matrix(~., data = df_lasso_evalatio)\n\n# Identify any missing columns between X_full and the prediction data (X_predict)\nmissing_cols <- setdiff(colnames(X_full), colnames(X_predict))\n\n# Add missing columns to X_predict with zero values (align columns)\nX_predict <- cbind(X_predict, matrix(0, nrow = nrow(X_predict), ncol = length(missing_cols),\n                                     dimnames = list(NULL, missing_cols)))\n\n# Ensure column order matches X_full\nX_predict <- X_predict[, colnames(X_full)]\n\n# Predict log_saleprice for the test dataset\npredictions <- predict(final_lasso_model, newx = X_predict)\n\n# Convert log predictions to SalePrice scale (exp of log predictions)\nSalePrice_pred <- exp(c(predictions))\n\n# Create a data frame with Id and SalePrice\nprediction_df <- data.frame(Id = df_lasso_evalatio_id, SalePrice = SalePrice_pred)\n\n# Write predictions to a CSV file\n#write.csv(prediction_df, \"sale_prices.csv\", row.names = FALSE)\n\n# Check the length of SalePrice predictions\nlength(prediction_df$SalePrice)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1459\n```\n\n\n:::\n:::\n\n\n\n# Conclusion\nOverall, the score is good, and I’ve noticed that many people achieve a similar score.\nOn the leaderboards, a score of 0.00044 can be found, which I find oddly low. Apparently, this is because the same dataset is used for another project [some Boston dataset](https://github.com/JinalShah2002/House-Prices-Challenge-Solution), and people are finding the corresponding test values there.\n\nI achieved a score of 0.13755, which is good but might be improved by using a superlearner or performing regression on the predictive values from different models.\n\nI liked it was the lasso that is chosen since i vary esay to interipted, wich i nice.",
    "supporting": [
      "Housing-pricing-project_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}