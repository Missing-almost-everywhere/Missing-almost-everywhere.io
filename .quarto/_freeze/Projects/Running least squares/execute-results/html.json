{
  "hash": "3d5317f2780646e3724ca0d89e1c6b6a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Update rules for least squares\"\nformat:\n  html:\n    code-fold: true\n---\n\n# Introduction\nIn this project, I will explore fitting a running linear model as well as running predictions.\n\nDuring my bachelor's studies, I focused on the prediction of time series. The model I used was fitted using least squares.\nI worked with [FFP3](https://robjhyndman.com/publications/), which is implemented in R.\n\nAt that time, I used the Forecast package but ended up implementing the estimator from the ground up.\nAs I have improved my programming and mathematical skills, I thought it would be fun to revisit this problem and see if I could develop a better solution. \n\nRob how wrote FFP3, has a blog their i sais their is trick to speeding op the calculation of rolling predition for AR process, and  I think I figured it out.\n\n\nLest squres can be written as\n\n$$\\underset{\\beta}{argmin} ||Y-X\\beta||^2$$\n\nSince this is a linear function the problem above i convex with one uniq closed form solution \n\n$$\\hat{\\beta}=(X^TX)^{-1}X^TY$$\n\n# Running Prediction\nWhen using a running model, a simple solution is to add a new data point and fit the model again.\nThis approach works but is not very efficient.\n\nInstead, let’s consider whether we can update the $\\beta$ estimates directly.\n\n\n\nOne option is the [Recursive least squares filter](https://en.wikipedia.org/wiki/Recursive_least_squares_filter)\n\n\n\nLastly, let’s explore how this can be used.\n\nLet $x[i]$  represent the i-th index of the vector.\n\nFor $XX^T$ a rank-1 update can be expressed as:\n\n$$(X_{new}X_{new}^T)=(X_{old}^TX_{old})+x_{new}^Tx_{new}$$ \n\nThis becomes clear when the product is written out explicitly.\n\nLook at $(X^TX)[j,g]=x_j^Tx_g$ So $\\sum_{i=1}^{n}x_{j}[i]x_{g}[i]$\n\nNext, consider a given entry in the inner product.\n$(X_{old}X_{old}^T)_{[j,g]}=x_j^Tx_g$ So $\\sum_{i=1}^{n}x_{j}[i]x_{g}[i]$\n\nNow look at.\n\n$(X_{new}X_{new}^T)[j,g]=(\\sum_{i=1}^{n}x_{j}[i]x_{g}[i])+x_{new}[j]x_{new}[g]$\n\nuse [Sherman–Morrison formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula)\n\nFor the update of $(X^TX)$\n\n $$(X_{new}^TX_{new})^{-1}=(X_{old}^TX_{old}+x_{new}^Tx_{new})^{-1}=(X_{old}^TX_{old})^{-1}-\\frac{(X_{old}^TX_{old})^{-1}x_{new}^Tx_{new}(X_{old}^TX_{old})^{-1}}{1+x_{new}(X_{old}^TX_{old})^{-1}x_{new}^T}$$\n\nLet's look at the nex part of the expresion\n\nFor $B=X^TY$ the udpate rule is\n$$B=(X_{new}^TY_{new})=(X_{old}^TY_{old})+x_{new}*y_{new}$$ \n\nnote here that $y_{new}$ act as a scale\n\n\n# Use Cases\nRolling prediction can be used for the evaluation of time series.\n\nHowever, these techniques can also be used for the quick implementation of a sliding window estimator or to update a linear model as new data arrives. Additionally, this method provides a fast approach for leave-one-out evaluation of predictive strength.\n\nI will create one implementations, where I use this for rolling prediction.\n\nThe method could be used for making a implements of a sliding window.\n\nFor the sliding window, it is necessary not only to add the new row but also to remove the old one.\n\n::: {#40ce0835 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport unittest\n\ndef get_rank_updated_matrix(A: list, v: list, u: list):\n    return A + v.T @ u\ndef test_get_rank_updated_matrix():\n    A_test=np.array([[1,0],[0,1]])\n    v_test=np.array([[1,1]])\n    u_test=np.array([[1,1]])\n    assert (np.all(get_rank_updated_matrix(A_test,v_test,u_test)==np.array([[2, 1], [1, 2]])))\n    return\ntest_get_rank_updated_matrix()\n\n\ndef test_get_inverse_updated_matrix_remove_row():\n    X_T_inner_X_inverse_test=np.array([[1,1],[1,1]])\n    x_test=np.array([[1,1]])\n    assert np.all(get_inverse_updated_matrix(X_T_inner_X_inverse_test,x_test)==np.array([[1,1],[1,1]])+4/5)\n    return\n\ndef get_inverse_updated_matrix_remove_row(X_T_inner_X_inverse: list, x: list):\n    return X_T_inner_X_inverse + X_T_inner_X_inverse @ (x.T @ x) @ X_T_inner_X_inverse / (1 + x @ X_T_inner_X_inverse @ x.T)\n\n\ndef get_inverse_updated_matrix(X_T_inner_X_inverse: list, x: list):\n    return X_T_inner_X_inverse - X_T_inner_X_inverse @ (x.T @ x) @ X_T_inner_X_inverse / (1 + x @ X_T_inner_X_inverse @ x.T)\n\ndef test_get_inverse_updated_matrix():\n    X_T_inner_X_inverse_test=np.array([[1,1],[1,1]])+4/5#np.array([[1,1],[1,1]])\n    x_test=np.array([[1,1]])\n    assert np.all(get_inverse_updated_matrix_remove_row(X_T_inner_X_inverse_test,x_test)==np.array([[1,1],[1,1]]))\n    return\n\n\ndef get_least_sqrd_closed_form_parameter(X: list, Y: list):\n    return np.linalg.inv(X.T @ X) @ X.T @ Y\n\ndef test_get_least_sqrd_closed_form_parameter():\n    X_test=np.array([[1,0],[0,1]])\n    Y_test=np.array([[1],[1]])\n    assert np.all((get_least_sqrd_closed_form_parameter(X_test,Y_test))==[[1],[1]])\n    return\ntest_get_least_sqrd_closed_form_parameter()\n\ndef get_least_sqrd_A_B_parameter(A: list, B: list):\n    return A @ B\n\ndef test_get_least_sqrd_A_B_parameter():\n    A_test=np.array([[1,0],[0,1]])\n    B_test=np.array([[1,0],[0,1]])\n    assert np.all(get_least_sqrd_A_B_parameter(A_test,B_test)==[[1,0],[0,1]])\n    return\ntest_get_least_sqrd_A_B_parameter()\n\ndef get_matrix_update_B(B: list, x: list, y:float):\n    return B + x.T *y\n\ndef test_get_matrix_update_B():\n    B_test=np.array([[1,1]])\n    x_test=np.array([[1,1]])\n    y_test=2.0\n    assert np.all(get_matrix_update_B(B_test,x_test,y_test)==np.array([[3.0,3.0]]))\n    return\ntest_get_matrix_update_B()\n\ndef get_matrix_update_B_remove_row(B: list, x: list, y:float):\n    return B - x.T *y\n\ndef get_matrix_update_B_remove_row():\n    B_test=np.array([[1,1]])\n    x_test=np.array([[1,1]])\n    y_test=1.0\n    assert np.all(get_matrix_update_B_remove_row(B_test,x_test,y_test)==np.array([[0.0,0.0]]))\n    return\ntest_get_matrix_update_B()\n\ndef get_prediction(A, B, X):\n    return X@get_least_sqrd_A_B_parameter(A, B)\n\ndef test_get_prediction():\n    x_test = np.array([[1, 0], [1, 1]])\n    y_test = np.array([[1], [2]])\n    A_test = np.linalg.inv(x_test.T @ x_test)\n    B_test = x_test.T @ y_test\n    assert np.all((get_prediction(A_test, B_test, np.array([[1, 2]])))==[[3]])\ntest_get_prediction()\n```\n:::\n\n\n::: {#44b8e550 .cell execution_count=2}\n``` {.python .cell-code}\ndef rolling_prediction(x: list, y: list, starting_length: int):\n    # Ensure x is 2D array with shape (n_samples, n_features)\n    x_array = np.array(x).reshape(-1, 1) if len(np.array(x).shape) == 1 else np.array(x)\n    y_array = np.array(y).reshape(-1, 1) if len(np.array(y).shape) == 1 else np.array(x)\n    \n    length = len(x_array)\n    prediction = np.zeros(length - starting_length)\n    los_score = np.zeros(length - starting_length)\n\n    # Initialize A and B matrices\n    initial_x = x_array[:starting_length]\n    initial_y = y_array[:starting_length]\n    \n    A = np.linalg.inv(initial_x.T @ initial_x)\n    B = initial_x.T @ initial_y\n\n    # Initial prediction\n    next_x = x_array[starting_length].reshape(1, -1)  \n    prediction[0] = get_prediction(A, B, next_x)\n    los_score[0] = y_array[starting_length] - prediction[0]\n    \n    # Rolling update for predictions and loss scores\n    \n    ind = starting_length + 1\n    current_x = x_array[ind].reshape(1, -1)  \n    for i in range(length - starting_length - 1): \n        ind = starting_length + i\n        current_x = x_array[ind].reshape(1, -1) \n        # Update matrices\n        A = get_inverse_updated_matrix(A, current_x)\n        B = get_matrix_update_B(B, current_x, float(y_array[ind]))\n        # Make prediction for next point\n        next_x = x_array[ind + 1].reshape(1, -1)  \n        prediction[i + 1] = get_prediction(A, B, next_x)\n        los_score[i + 1] = y_array[ind + 1] - prediction[i + 1]\n    return {\"prediction\": prediction, \"los_score\": los_score}\n\n```\n:::\n\n\n# Testing the Application\nSince the underlying model is not changing, this provides a straightforward way to test the model.\nThe residuals should closely resemble the errors.\n\nHowever, they will not be identical, as this would require the coefficients to be perfectly accurate.\n\n::: {#aa55c565 .cell execution_count=3}\n``` {.python .cell-code}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#Dianostic plot \nimport matplotlib.pyplot as plt\n\nstart_val=500\nx = np.random.rand(1000)  \nerror=np.random.normal(size=1000)  \ny = 2*x + error\n\n# Run the prediction\nresult = rolling_prediction(x, y, start_val)\n\n\nplt.hist(result['los_score'],alpha=0.5)\n\nplt.hist(error[(start_val):],alpha=0.5)\nplt.legend([\"Residuals\", \"True error\"])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Running least squares_files/figure-html/cell-4-output-1.png){width=575 height=411}\n:::\n:::\n\n\n# When the Application Fails\nThe reader should note that I have not included an intercept in the data-generating process.\nBelow, I have conducted the same test, but this time I have added a constant to the expression.\n\n::: {#cb4a9041 .cell execution_count=4}\n``` {.python .cell-code}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#Dianostic plot \nimport matplotlib.pyplot as plt\n\nstart_val=500\nx = np.random.rand(1000)  \nerror=np.random.normal(size=1000)  \ny =10+ 2*x + error\n\n# Run the prediction\nresult = rolling_prediction(x, y, start_val)\n\n\nplt.hist(result['los_score'],alpha=0.5)\n\nplt.hist(error[(start_val):],alpha=0.5)\nplt.legend([\"Residuals\", \"True error\"])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Running least squares_files/figure-html/cell-5-output-1.png){width=575 height=411}\n:::\n:::\n\n\nAs can be see now the aplication fails.\n\nThis can be fixed by adding a column of ones to the model matrix. I would also need to update all the functions to accommodate this change. Without adjusting for this, the dimensions will not match when adding the point to the inverse.\n\nI will stop the project here.\nThis was intended to be a small NumPy project. If I were to create functions that allow specifying a model, I might develop it more properly and let the user specify the model through an expression, similar to how linear models are specified in R. Since all model specifications in linear models are based on modifying the model matrix, this could provide flexibility.\n\n",
    "supporting": [
      "Running least squares_files"
    ],
    "filters": [],
    "includes": {}
  }
}