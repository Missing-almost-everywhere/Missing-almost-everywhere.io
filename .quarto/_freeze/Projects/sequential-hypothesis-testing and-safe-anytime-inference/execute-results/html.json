{
  "hash": "eca0e77cb4fb295ffbd90e965d7fd065",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Sequential Hypothesis Testing and Safe Anytime Inference\"\nformat:\n  html:\n    code-fold: true\n---\n\n# Preface to This Project\n\nThis project was one of those rabbit-hole endeavors. I wanted to learn about A/B testing, which turned out to be a straightforward hypothesis test with some power calculations.\n\nInterestingly, the calculations for power analysis were concepts I had already learned during my bachelor's degree. So, I decided to explore some newer methods instead. That led me to sequential testing—a fascinating field. It's this intriguing mix of betting strategies, martingales, and a lot of other fun concepts.\n\nI'm mostly writing this project for myself, so I have something to work on and a way to document my learning process. But if someone happens to stumble across this, they should be aware that it can get a bit technical. I've included references to the math so I can easily find it again, and I’ve conducted a few small simulation studies. These simulations help me get a sense of the performance of the methods while also allowing me to bend the assumptions a little to see what happens.\n\n# Introduction\nSo in this project i am looking in to Wald’s Sequential Probability Ratio Test also called SPRT and some variants like mSPRT.\nThe reason i want to learn this, is sequsial testing method.\nThis is used to make live monotering of of Hyposis. It eliminates the need for power calucluastions, insted the parameters $\\alpha$ and $\\beta$ is chosen/specified befor the start of the experiment and the sampling proces gones on, until the hyposis can be confirmed or rejected. \nThis is more inline with how i would want to conduct experimetns, their is some simularty to stocastic bandit, where agent tryes to optimize reward will they learn, for SPRT, the goal is to stop the experiment when a hypsis can be confirmed ore rejected.\n\nThe use cases for this live experiment such as A/B testing, but some mention trials aswell, I would think it would be hard to get aproved.\n\nThese test is based on significance and p-values, with the subject matter focusing on hypothesis testing.\nI assume the reader is already familiar with this concept, as I typically write these short notes for my personal reference.\nThe concept of significance can be explained by the user selecting an $\\alpha$ level, which defines the desired confidence level or coverage. This can be thought of as representing the user's tolerance for making an error. In many research fields, a common choice is $\\alpha = 0.05$, which corresponds to tolerating a 5% error rate.\nFor this experimental design, the goal is that if 100 experiments are conducted, we would expect the correct hypothesis to be confirmed approximately $(1 - \\alpha) \\times 100$ times.\n\nSo why use these Sequential test rather than normal likelihod ratio test og confiens intervalls and the reason is peeking, wish we will get into later.\n\n\n# Power calulation\nThis project started with me looking into power calculations. In this section i go throug how to make the power caluculations and look into what happens if on peeks.\n\nSo the power calulation works by the user setting the minimum effect they would want the calculate, and tolarance for type 1 error $\\alpha$ and tolerance for type 2 $\\beta$.\n\nLet $X=x_1,\\dots,x_n$ be the data depend if $x_i\\sim \\mathbb{N}(\\mu,\\sigma^2)$ ore if mean $\\bar{X}\\sim \\mathbb{N}(\\mu,\\sigma^2)$, the last come often from the law of large number.\nOne can use this method.\nI am not goin in to much in detail here, relly like this [explation](https://rugg2.github.io/AB%20testing%20-%20a%20simple%20explanation%20of%20what%20power%20analysis%20does.html).\nThe pivot is showen in the calulaten's.\n\n## Formally\nLet $H_0$ be the null and let $H_1$ be other hyposis \n\n$\\alpha$ is the proberbillaty of making a type one error meaning \n\n$P(acepting\\: H_1\\: when\\: H_0\\: is\\: true)$\n\n$\\beta$ is the probillaty of type 2 error\n\n$P(acepting\\: H_0\\: when\\: H_1\\: is\\: true)$\n\nIf the distribtion is nice meaning law of large number insure it conveges to normal distribuion.\n\n$$n\\geq 2(\\frac{\\phi^{-1}(1-\\alpha-\\phi^{-1}(1-\\beta)}{\\Delta/\\sigma})^2$$\n\n$\\phi^{-1}$ is the inverse cumulative standard normal distribution, \n\n$\\Delta$ is the abeslut differnece in effect betwwen the null $H_0$ and $H_1$.\n\n$\\sigma$ is the variance.\n\n\n## Peeking \nPeeking is when a one looks at the data as they come in, using standart hyposis test method, and make a conclusion if they dont see a effect.\nThe problem is that it destrou the coverage gaurenty. So if one look at the data as they flow in and end the experiment and before if their is no significans.\n\nThis idear of if we have hundret experiement, in $(1-\\alpha)*100$ the right hypothesis identified is not valid.\n\nBelow i have made some small simulations to ilustrate the problem \n\n$A=a_1,\\dots,a_n$ and $B=b_1,\\dots,b_n$ let $a_i$ and $b_i$ iid and let $a_i\\sim\\mathbb{N}(0,1)$ and $b_i\\sim\\mathbb{N}(0,1)$.\n\nSince $A$ and $B$ has the same distribuion their is no effect. This is sometime calle a A/A test. Sometimes this is used to test method in a live environment. If a useres test method, they should get the type 1 error cover from this.\n\nAt each point the simulation will draw with no replacment from A ore B calulat the p values and see if the true mean is in the confiden intervall.\n\n::: {#61138e09 .cell execution_count=1}\n``` {.python .cell-code}\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nrandom.seed(42)\n# Parameters\nn = 100  # Number of samples in A and B\nnum_simulations = 1  # Number of simulations\nalpha = 0.05  # Significance level\ntrue_mean = 0  # True mean to check against\nvariance=1 # varince know\na=np.random.normal(true_mean,variance,n) # draw\nb=np.random.normal(true_mean,variance,n) # draw\n\ndef get_sigficanlevet(A:list,B:list,A_variance:float=1,B_variance:float=1):\n    mean_A=np.mean(A)\n    mean_B=np.mean(B)\n    Z = (mean_A - mean_B) / np.sqrt(A_variance / len(A) + B_variance/ len(B))\n    p_value = 2 * (1 - norm.cdf(abs(Z)))\n    return p_value\n\n\ndef get_significant_seq(A: list, B: list, A_variance: float = 1, B_variance: float = 1):\n    # Initialize an empty list to store the significant values\n    sig_seq = []\n\n    # Create copies of A and B for sampling without replacement\n    A_copy = np.copy(A)\n    B_copy = np.copy(B)\n\n    # Initialize vectors for the selected samples\n    A_vector = []\n    B_vector = []\n\n    # Sample the first element from A and B\n    A_vector.append(A_copy[0])\n    A_copy = np.delete(A_copy, 0)\n\n    B_vector.append(B_copy[0])\n    B_copy = np.delete(B_copy, 0)\n\n    # Initial significance level calculation\n    sig_seq.append(get_sigficanlevet(A_vector, B_vector))  # Assuming this function exists\n\n    # Run the loop until both lists are empty\n    while len(A_copy) > 0 or len(B_copy) > 0:\n        # Randomly choose whether to sample from A or B\n        if np.random.rand() > 0.5 and len(A_copy) > 0:\n            A_vector.append(A_copy[0])  # Append the first element from A\n            A_copy = np.delete(A_copy, 0)  # Remove the sampled element from A\n        elif len(B_copy) > 0:\n            B_vector.append(B_copy[0])  # Append the first element from B\n            B_copy = np.delete(B_copy, 0)  # Remove the sampled element from B\n        \n        # Calculate and store the significance level at this step\n        sig_seq.append(get_sigficanlevet(A_vector, B_vector))  # Assuming this function exists\n\n    return sig_seq\n\n\ndef plot_significance_seq(sig_seq):\n    # Plot the significance sequence\n    plt.figure(figsize=(10, 6))\n    plt.plot(sig_seq, marker='o', linestyle='-', color='b', label='Significance Level')\n    \n    # Add horizontal line at y=0.05 for the threshold\n    plt.axhline(y=0.05, color='r', linestyle='-', label='Significance Threshold (0.05)')\n    \n    # Add small green line segments for significance values below 0.05\n    for i, value in enumerate(sig_seq):\n        if value < 0.05:\n            plt.plot([i, i], [0, value], color='green', lw=2)  # Line from 0 to the value\n\n    # Set limits for the y-axis between 0 and 1\n    plt.ylim(0, 1)\n    \n    # Adding labels and title\n    plt.xlabel('Sample Number')\n    plt.ylabel('Significance Level')\n    plt.title('Significance Level Sequence with Threshold at 0.05')\n    \n    # Adding a legend\n    plt.legend()\n\nresult=get_significant_seq(a,b)\nplot_significance_seq(result)\n```\n\n::: {.cell-output .cell-output-display}\n![](sequential-hypothesis-testing and-safe-anytime-inference_files/figure-html/cell-2-output-1.png){width=812 height=523}\n:::\n:::\n\n\nAs can be see from the plot the p values goes up and down doing the experiment, So if one was to stop the experiement when the p value cross a threshold. The cover is not correct, this means in this case that tyoe 1 error alot higer than one exptes.\n\nSo let's look at how the coverage is affected by the peaking. Belove i have made small simulation that corespond to making the plot above a 1000 times and with a p value $0.05$.\n\nI simulate two draws from the same distribution and check if there is a significant difference between them. If a significant difference is found, I stop the experiment and count it as a success. If no significant difference is found, the experiment continues with a new draw, and the same test is conducted. I repeat this process for 100 draws and 1000 experiments. Finally, I sum the number of experiments that show a significant result and divide this sum by the total number of experiments (1000) to calculate the proportion of significant results.\n\n::: {#d74cb522 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n\n\ndef get_sigficanlevet(A:list,B:list,A_variance:float=1,B_variance:float=1):\n    mean_A=np.mean(A)\n    mean_B=np.mean(B)\n    Z = (mean_A - mean_B) / np.sqrt(A_variance / len(A) + B_variance/ len(B))\n    p_value = 2 * (1 - norm.cdf(abs(Z)))\n    return p_value\n\n\ndef run_significant_seq(A: list, B: list, A_variance: float = 1, B_variance: float = 1):\n    # Initialize an empty list to store the significant values\n\n    # Create copies of A and B for sampling without replacement\n    A_copy = np.copy(A)\n    B_copy = np.copy(B)\n\n    # Initialize vectors for the selected samples\n    A_vector = []\n    B_vector = []\n\n    # Sample the first element from A and B\n    A_vector.append(A_copy[0])\n    A_copy = np.delete(A_copy, 0)\n\n    B_vector.append(B_copy[0])\n    B_copy = np.delete(B_copy, 0)\n    \n    sig_erro=0\n    # Run the loop until both lists are empty\n    while len(A_copy) > 0 or len(B_copy) > 0:\n        # Randomly choose whether to sample from A or B\n        if np.random.rand() > 0.5 and len(A_copy) > 0:\n            A_vector.append(A_copy[0])  # Append the first element from A\n            A_copy = np.delete(A_copy, 0)  # Remove the sampled element from A\n        elif len(B_copy) > 0:\n            B_vector.append(B_copy[0])  # Append the first element from B\n            B_copy = np.delete(B_copy, 0)  # Remove the sampled element from B\n        \n        # Calculate and store the significance level at this step\n        if(get_sigficanlevet(A_vector, B_vector)<0.05):\n            sig_erro=1\n            break \n    return sig_erro\n\n\n\nn = 100  # Number of samples in A and B\nnum_simulations = 1000  # Number of simulations\nalpha = 0.05  # Significance level\ntrue_mean = 0  # True mean to check against\nvariance=1 # varince know\n\nestimated=[]\nfor i in range(1,num_simulations):\n    a=np.random.normal(true_mean,variance,n) # draw\n    b=np.random.normal(true_mean,variance,n) # draw\n    estimated.append(run_significant_seq(a,b))\n\nprint(\"cover\")\nprint(np.mean(estimated))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncover\n0.38738738738738737\n```\n:::\n:::\n\n\nFor these settings, I usually obtain an estimate of around 0.4, which, to me, seems high.\nRemember, this should be the Type 1 coverage rate, so around 0.05.\nIn practice, a person would likely not peek at the data every time a new point arrives. This is merely intended to illustrate the problem of peeking.\n\nIf peeking causes problems, the solution seems obvious: avoid peeking. However, in practice, people often do.\n\nThe literature on the method introduced below discusses designing systems to account for user behavior. This includes cases where users receive credit for findings, even when peeking leads to results that may not be valid.\n\nThe goal is to develop mathematical models that mitigate the problem of peeking. Another approach is preregistration.\nThat said, there are many valid reasons to evaluate results as data comes in and to stop when a clear conclusion can be drawn. \n\n# Betting for Hypothesis Testing.\n\nThe main idea behind betting in hypothesis testing is to reframe the problem of hypothesis testing into a game.  \nIn this game, if the null hypothesis is true, there should be no strategy to consistently earn money. However, if one starts making money by betting against the null hypothesis, at some point it becomes highly unlikely that the null is true, allowing one to reject it.  \n\nLet us think about this.  \nTo give an example, imagine you are betting on the outcomes of two coin flips. You can bet on whether the coins match (both heads or both tails) or not. The coins may either be fair or have a matching bias, meaning both coins have a higher probability of landing on heads or tails.  \n\nIf you guess correctly, you double your money; if you guess incorrectly, you lose your money. Note that there are only two hypotheses here. A rational player would not bet all their money but rather a fraction of it. A small note: The optimal fraction can be determined by optimizing log wealth. I won't go into the details here, but this approach is widely used in the e-variable literature, which I refer to at the end.\n\nLet $H_0$ represent the null hypothesis that there is no difference between the coins, meaning they are fair. For a fair game, $P(x = \\text{match}) = 0.5$, and there is no strategy to earn money. You could always bet \"match,\" always bet \"no match,\" or condition your bets on the previous outcome—it doesn’t matter. This also means there are no consequences to betting on $H_1$.  \n\nNow, if the coins are biased, say $P(x = \\text{match}) = 0.6$, the optimal strategy is to always bet on \"match\".  \n\n\nThe above fair game is a sup [martingale meaning](https://en.wikipedia.org/wiki/Martingale_(probability_theory)) meaning that the expected winnings at any point in time are equal to or less than what one has currently, and the value of the wealth process is always positive. \n\nLet $M_t$ represent the wealth process in the game, i.e., the amount of money one has at time $t$.  \n\n\n[Ville's inequality](https://thestatsmap.com/Ville's-inequality)\nstates that\n\n$$\\mathbb{P}\\left( \\exists t \\geq 1 : M_t \\geq x \\right) \\leq \\frac{\\mathbb{E}[M_0]}{x}$$\n\nThis means that there is a set probability that $M_t$ will cross $x$ over the entire duration of the process, assuming the game is fair.  \n\nNow, if the game is not fair, at some point we will win enough money to cross the threshold defined by Ville's inequality. On the other hand, if the game is fair, with some probability (determined by $\\alpha$), $M_t$ will still cross the inequality.  \n\nThis provides the correct control for the null hypothesis, but it holds over the entire process.\n\n## sequential probability ratio test (SPRT)\nA little more formal explanation of the SPRT:  \nHere, we are only looking at two hypotheses and their corresponding points:\n\n- $H_0: \\theta = \\theta_0$, where $p(x_i)$ is the distribution under the null hypothesis.  \n- $H_1: \\theta = \\theta_1$, where $q(x_i)$ is the distribution under the alternative hypothesis.  \n\nWe begin with a wealth of one.  \n\nUnder the null hypothesis, the expectation of the likelihood ratio is given by:  \n\n$$  \n\\mathbb{E}_{q(x)}( \\frac{q(X_i)}{p(X_i)}) = \\int \\frac{p(x)}{q(x)} q(x) \\, dx = \\int p(x) \\, dx = 1  \n$$  \n\nThe wealth can be written as:  \n\n$$  \nM_t = \\prod_{i=1}^{T} \\frac{q(X_i)}{p(X_i)}  \n$$  \n\nThis is a submartingale, and by using Ville's inequality, we obtain a probability guarantee for the entire process.  \n\nUnder the alternative hypothesis, the inverse of the likelihood ratio will be a submartingale, leading to two thresholds: one for rejecting the null hypothesis $H_0$ and one for rejecting $H_1$.  \n\n$a\\approx \\frac{\\beta}{1-\\alpha}$ and $b \\approx \\frac{1-\\beta}{\\alpha}$\n\nif $M_t \\geq b$ accept $H_1$\nif $M_t \\leq a$ accept $H_0$\n\nOtherwise sample a new point.\n\nNotice how simple the theory is here.\n\nWiki is good [reference](https://en.wikipedia.org/wiki/Sequential_probability_ratio_test) for this.\n\n## Simulation\nBelow i have made small simulation of the the fair game desciped above and the SPRT.\nboth coins af fair meaning their is $P(X=head)=0.5$ for both $\\alpha = 0.05$\n\n::: {#562f1d10 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\n\n# Variables for the experiment\np_coin_1 = 0.5\np_coin_2 = 0.5\n\n# Probability of match and no match\np_match = (1 - p_coin_1) * (1 - p_coin_2) + p_coin_1 * p_coin_2\np_no_match = 1 - p_match\n\n# Hypotheses\nmatch = 0.5  # Null hypothesis\nalternative_hypothesis = 0.7  # Alternative hypothesis\n\n# Tolerances\nalpha = 0.05  # Type I error\nbeta = 0.20  # Type II error\n\n# Thresholds\na = np.log(beta / (1 - alpha))\nb = np.log((1 - beta) / alpha)\n\n# Make button\n\n\ndef ber_llr(h_0, h_1, outcome):\n    \"\"\"\n    Calculate the log-likelihood ratio for a Bernoulli outcome.\n    \n    Args:\n        h_0: Null hypothesis probability\n        h_1: Alternative hypothesis probability\n        outcome: Observed outcome (0 or 1)\n        \n    Returns:\n        Log-likelihood ratio for the given outcome.\n    \"\"\"\n    return np.log((h_1**outcome * (1 - h_1)**(1 - outcome)) /\n                  (h_0**outcome * (1 - h_0)**(1 - outcome)))\n\ndef sprt(p_match, h_0, h_1, a, b):\n    \"\"\"\n    Perform Sequential Probability Ratio Test (SPRT).\n    \n    Args:\n        p_match: Probability of a match\n        h_0: Null hypothesis probability\n        h_1: Alternative hypothesis probability\n        a: Lower threshold (log-scale)\n        b: Upper threshold (log-scale)\n        \n    Returns:\n        Dictionary with test results.\n    \"\"\"\n    Lambda = 0  # Cumulative log-likelihood ratio\n    draws = []  # Store outcomes\n    log_likelihood_sum = [0]  # Track likelihood values\n    confirmed_hypothesis = None  # Store confirmed hypothesis\n\n    while Lambda > a and Lambda < b:\n        outcome = np.random.binomial(1, p_match)\n        draws.append(outcome)\n        Lambda += ber_llr(h_0, h_1, outcome)\n        log_likelihood_sum.append(Lambda)\n    \n    # Determine the final hypothesis\n    if Lambda >= b:\n        confirmed_hypothesis = \"Alternative\"\n    elif Lambda <= a:\n        confirmed_hypothesis = \"Null\"\n\n    return {\n        \"confirmed_hypothesis\": confirmed_hypothesis,\n        \"log_likelihood_sum\": log_likelihood_sum,\n        \"draws\": draws,\n        \"length\": len(draws)\n    }\n\n# Run the SPRT\nresult = sprt(p_match, match, alternative_hypothesis, a, b)\n\n\ndef simulate_cover_sprt(p_match, h_0, h_1, a, b,N=1000):\n    cover=[]\n    length=[]\n    for i in range(N):\n        sim_ob=sprt(p_match, h_0, h_1, a, b)\n        cover.append(sim_ob[\"confirmed_hypothesis\"]==\"Null\")\n        length.append(sim_ob[\"length\"])\n    return {\"cover\":np.mean(cover),\"mean length\":np.mean(length)}\n\nsim_result = simulate_cover_sprt(p_match, match, alternative_hypothesis, a, b,10000)\nprint(\"cover\")\nprint(sim_result[\"cover\"])\nprint(\"mean run length\")\nprint(sim_result[\"mean length\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncover\n0.9548\nmean run length\n18.283\n```\n:::\n:::\n\n\nAs can be see the cover i correct of the hyposis.\n\n## mSPRT\n\nIn practice, one rarely has a point hypothesis but rather a null and composite hypothesis.  \nThe question is often whether there is an effect or not.  \nThe composite hypothesis is a combination of multiple hypotheses.\n\nmSPRT uses a mixture distribution for this:  \n$$\n\\Lambda(X_n) = \\int_{\\theta}\\prod_{i=1}^{n}\\frac{f_{\\theta}(x_i)}{f_{\\theta_0}(x_i)}h(\\theta)d\\theta\n$$\n\nThis will be denoted $\\Lambda_t$\n\nThe same stopping threshold is used for $b$, meaning type 1 error control is the same since, under the null, the log-likelihood is a submartingale:  \n$$\n\\mathbb{E_{\\theta_0}}\\left(\\frac{f_{\\theta}(x)}{f_{\\theta_0}(x)}h(\\theta)\\right) = \\int f_{\\theta}(x)d\\theta = 1\n$$\n\nThis is described as \"hedging your bets.\" The idea here is that you are betting on multiple hypotheses.  \n\nIt becomes a little more complicated when considering the composite hypothesis, as there are multiple hypotheses, and only one can be true. It is not a submartingale unless one finds a way to adjust the expectation as the process progresses. I have not found such a method. This means the lower threshold for the sequence is not valid.  \n\nWhat one can do instead is to set a limit on the amount of data one wants to gather and set $\\beta$ to zero. This means losing type 2 error control.  \n\nA second option is two experiment with adjusted $\\beta$, using distributions on both sides of the null.  \n- If both are stopped by the $b$ criterion, which is related to $\\alpha$, this would suggest the null is true.  \n- If one confirms the null and the other rejects it, this would suggest evidence against the null.\n\n\nThus, it is easier to simply negate the type 2 error.\nIn practice, the user chooses an $\\alpha$, the tolerance for type 1 error they are willing to accept, and a maximum number of experiments they want to perform or can afford. Then the experiment runs, and if the process is not stopped, the conclusion is that there was no significance.\n\n\nBelow, I made a small simulation of how one can do this. In the simulation, I treat the distribution as the null hypothesis and ran 1,000 experiments with 1,000 samples.\n\n::: {#981fe8ee .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.stats import norm\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\", \n    category=RuntimeWarning, \n    message=\"invalid value encountered in scalar divide\"\n)\n\n# SPRT implementation\ndef msprt(l_null, l_composite, N_stopping, alpha, draw_function):\n    draws = []\n    log_likelihood_ratios = []\n    b = np.log(1 / alpha)\n    log_likelihood_ratio = 0\n    decision = \"Null\"\n    for i in range(N_stopping):\n        x = draw_function()\n        draws.append(x)\n        log_likelihood_ratio += np.log(l_composite(x) / l_null(x))\n        log_likelihood_ratios.append(log_likelihood_ratio)\n        if log_likelihood_ratios[i] > b:\n            decision = \"Alternative\"\n            break\n    return {\n        \"confirmed_hypothesis\": decision,\n        \"log_likelihood_sum\": log_likelihood_ratio,\n        \"draws\": draws,\n        \"length\": len(draws),\n    }\n\n# Simulation of coverage\ndef simulate_cover_msprt(l_null, l_composite, N_stopping, alpha, draw_function, N_experiments):\n    cover = []\n    length = []\n    for i in range(N_experiments):\n        sim_ob = msprt(l_null, l_composite, N_stopping, alpha, draw_function)\n        cover.append(sim_ob[\"confirmed_hypothesis\"] == \"Null\")\n        length.append(sim_ob[\"length\"])\n    return {\"cover\": np.mean(cover), \"mean length\": np.mean(length)}\n\n# Likelihood functions\ndef get_likelihood_null(x, mean_null, std_dev_null):\n    return norm.pdf(x, loc=mean_null, scale=std_dev_null)\n\ndef get_likelihood_composit(x, mean_1, std_dev_1, phi_1, mean_2, std_dev_2):\n    return phi_1 * norm.pdf(x, loc=mean_1, scale=std_dev_1) + (1 - phi_1) * norm.pdf(x, loc=mean_2, scale=std_dev_2)\n\n# Parameters\nmean_true = 0\nvariance_true = 1\nstd_dev_true = variance_true ** 0.5\n\n# Simulation\nsim_result = simulate_cover_msprt(\n    l_null=lambda x: get_likelihood_null(x, mean_null=0, std_dev_null=1),\n    l_composite=lambda x: get_likelihood_composit(x, mean_1=-2, std_dev_1=1, phi_1=0.5, mean_2=2, std_dev_2=1),\n    N_stopping=1000,\n    alpha=0.05,\n    draw_function=lambda: np.random.normal(mean_true, std_dev_true),\n    N_experiments=1000\n)\n\n# Results\nprint(\"cover:\", sim_result[\"cover\"])\nprint(\"mean length:\", sim_result[\"mean length\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncover: 0.986\nmean length: 986.037\n```\n:::\n:::\n\n\nas can be see the cover is good.\n\nVille's inequality can also be used for anytime-valid p-values:  \n$$\np_n = \\min \\left[ 1, \\min \\left( \\tilde{\\Lambda}_t^{-1} : t \\leq n \\right) \\right]\n$$\n\nIf one thinks about the equation, it is quite simple. This involves moving a horizontal line down until it hits the highest point of the likelihood ratio trajectory and finding the corresponding  $\\alpha$.  \n\nThere are important things to notice here. This way of looking at p-values is very consistent, meaning that if we gather evidence against the null, no matter what data is later gathered, the p-values do not rise again.  \n\nThis stands in contrast to \"normal\" p-values, which can be inconsistent over time periods.\n\nA very nice propperty.\n\n## A Small Point\nThe submartingale property comes under a very specific distribution under the null.  \nSo, one is not only testing if $\\theta = \\theta_0$ but also making assumptions about the distribution.  \nThe Central Limit Theorem (CLT) is very powerful in the sense that, by taking the mean, many distributions will converge, providing some robustness to the CLT.  \n\nHowever, when looking at every single point and making assumptions about the distribution, that robustness is not present in the same way.  \nOne way to combat this is by making batches of data, but this takes away some of the idea of anytime-valid confidence intervals.  \n\nA second approach would be to determine the number of times one is allowed to peek and adjust the $\\alpha$ based on this.  \nThese methods suffer from some of the same incentive structure errors as normal p-values. Over-peeking becomes a problem, and people not involved in the analysis cannot see if this has occurred.\n\n\n## Looking into if the distribion mispecified\nBelow here i have made simulation the true distribuion is a t distribuion mean zero with variance 3 scale 1. The null is normal distribuion with variance 3 and mean zero. \nThe only difference is the and the sape of the distribuionsion. The hyposis test is if the mean is the same.\n\n::: {#6b07972f .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.stats import norm, t\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\", \n    category=RuntimeWarning, \n    #message=\"invalid value encountered in scalar divide\"\n)\n\n\n# SPRT implementation\ndef msprt(l_null, l_composite, N_stopping, alpha, draw_function):\n    draws = []\n    log_likelihood_ratios = []\n    b = np.log(1 / alpha)\n    log_likelihood_ratio = 0\n    decision = \"Null\"\n    for i in range(N_stopping):\n        x = draw_function()\n        draws.append(x)\n        log_likelihood_ratio += np.log(l_composite(x) / l_null(x))\n        log_likelihood_ratios.append(log_likelihood_ratio)\n        if log_likelihood_ratios[i] > b:\n            decision = \"Alternative\"\n            break\n    return {\n        \"confirmed_hypothesis\": decision,\n        \"log_likelihood_sum\": log_likelihood_ratio,\n        \"draws\": draws,\n        \"length\": len(draws),\n    }\n\n# Simulation of coverage\ndef simulate_cover_msprt(l_null, l_composite, N_stopping, alpha, draw_function, N_experiments):\n    cover = []\n    length = []\n    for i in range(N_experiments):\n        sim_ob = msprt(l_null, l_composite, N_stopping, alpha, draw_function)\n        cover.append(sim_ob[\"confirmed_hypothesis\"] == \"Null\")\n        length.append(sim_ob[\"length\"])\n    return {\"cover\": np.mean(cover), \"mean length\": np.mean(length)}\n\n# Likelihood functions\ndef get_likelihood_null(x, mean_null, std_dev_null):\n    return norm.pdf(x, loc=mean_null, scale=std_dev_null)\n\ndef get_likelihood_composit(x, mean_1, std_dev_1, phi_1, mean_2, std_dev_2):\n    return phi_1 * norm.pdf(x, loc=mean_1, scale=std_dev_1) + (1 - phi_1) * norm.pdf(x, loc=mean_2, scale=std_dev_2)\n\n# Parameters\nmean_true = 0\nvariance_true = 1\nstd_dev_true = variance_true ** 0.5\n\n# Simulation\nsim_result = simulate_cover_msprt(\n    l_null=lambda x: get_likelihood_null(x, mean_null=0, std_dev_null=np.sqrt(3)),\n    l_composite=lambda x: get_likelihood_composit(x, mean_1=-5, std_dev_1=np.sqrt(3), phi_1=0.5, mean_2=5, std_dev_2=np.sqrt(3)),\n    N_stopping=1000,\n    alpha=0.05,\n    draw_function = lambda: t.rvs(df=3, loc=mean_true, scale=1),\n    N_experiments=1000\n)\n\n# Results\nprint(\"cover:\", sim_result[\"cover\"])\nprint(\"mean length:\", sim_result[\"mean length\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncover: 0.959\nmean length: 960.938\n```\n:::\n:::\n\n\nAs can be see in this case one actullay get good cover in this case. The simulated cover is close to the teoretical but if I make the null distribuion wide the cover is much lees acurate.\n\nbelow I have made a small change to the true distribusion so now the scale parater is 2 meaning a vider shape of the t distribusion.\nSo the mean is the same but variance is diffrent.\n\n::: {#bc6648ec .cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.stats import norm, t\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\", \n    category=RuntimeWarning, \n    message=\"divide by zero encountered in scalar divide\"\n)\n\n\n# SPRT implementation\ndef msprt(l_null, l_composite, N_stopping, alpha, draw_function):\n    draws = []\n    log_likelihood_ratios = []\n    b = np.log(1 / alpha)\n    log_likelihood_ratio = 0\n    decision = \"Null\"\n    for i in range(N_stopping):\n        x = draw_function()\n        draws.append(x)\n        log_likelihood_ratio += np.log(l_composite(x) / l_null(x))\n        log_likelihood_ratios.append(log_likelihood_ratio)\n        if log_likelihood_ratios[i] > b:\n            decision = \"Alternative\"\n            break\n    return {\n        \"confirmed_hypothesis\": decision,\n        \"log_likelihood_sum\": log_likelihood_ratio,\n        \"draws\": draws,\n        \"length\": len(draws),\n    }\n\n# Simulation of coverage\ndef simulate_cover_msprt(l_null, l_composite, N_stopping, alpha, draw_function, N_experiments):\n    cover = []\n    length = []\n    for i in range(N_experiments):\n        sim_ob = msprt(l_null, l_composite, N_stopping, alpha, draw_function)\n        cover.append(sim_ob[\"confirmed_hypothesis\"] == \"Null\")\n        length.append(sim_ob[\"length\"])\n    return {\"cover\": np.mean(cover), \"mean length\": np.mean(length)}\n\n# Likelihood functions\ndef get_likelihood_null(x, mean_null, std_dev_null):\n    return norm.pdf(x, loc=mean_null, scale=std_dev_null)\n\ndef get_likelihood_composit(x, mean_1, std_dev_1, phi_1, mean_2, std_dev_2):\n    return phi_1 * norm.pdf(x, loc=mean_1, scale=std_dev_1) + (1 - phi_1) * norm.pdf(x, loc=mean_2, scale=std_dev_2)\n\n# Parameters\nmean_true = 0\nvariance_true = 1\nstd_dev_true = variance_true ** 0.5\n\n# Simulation\nsim_result = simulate_cover_msprt(\n    l_null=lambda x: get_likelihood_null(x, mean_null=0, std_dev_null=np.sqrt(3)),\n    l_composite=lambda x: get_likelihood_composit(x, mean_1=-5, std_dev_1=np.sqrt(3), phi_1=0.5, mean_2=5, std_dev_2=np.sqrt(3)),\n    N_stopping=1000,\n    alpha=0.05,\n    draw_function = lambda: t.rvs(df=3, loc=mean_true, scale=2),\n    N_experiments=1000\n)\n\n# Results\nprint(\"cover:\", sim_result[\"cover\"])\nprint(\"mean length:\", sim_result[\"mean length\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncover: 0.588\nmean length: 594.124\n```\n:::\n:::\n\n\nThe coverage is now changed, as it should be. This is the point: the problem is that one is not only making assumptions about the values of the parameter but also about the distribution itself. \n\nTo sum up, in both these experiments, the model is mis-specified. However, if one is only interested in the mean, the coverage is quite good in one case and quite bad in the other.\n\nRemember, these assumptions about the distribution need to be made before the experiment starts.\nThus, one likely needs to gather some data to estimate the null distribution.\n\nThe same can be said about the distributions in the composite hypothesis. If I had to work with this in practice, I would conduct some simulation studies.\n\nA good use case is convergence rates, which can be described by a Bernoulli distribution. This is particularly useful since the Bernoulli distribution only has one parameter, making it behave nicely in terms of distribution.\nBy this, I mean that it should be less sensitive to being misspecified.\n\n## mSPRT aplications\nMost of the implented aplications of mSPRT i found is based on \n[The article](https://arxiv.org/pdf/1512.04922)\n\nFor the type II error control. It seam like that by running two test with leve $\\frac{\\alpha}{2}$\nthis come from [A modified sequential probability ratio test](https://www.sciencedirect.com/science/article/pii/S0022249621000109). I have decided with my self i will look into this, if am gona use this in practis. So for now this is out of the scope of this asigment, but the idear is pretty brilliant.\n\n\n\n# Conclusion\nOverall that a lot i like about the anytime valid p-values.\nI like the consitensy, i like dont have to make power calulation, i dont like that in mSPRT i am so depend on distribusion. I have to get the right distribtuion of null for it to be supmartingale.\nAnd for what the small example this the t distribuion, showed this can be problem. \n\nAlot time CLT mean at for a lot of distribuions will convege to normal distribuion insuring some robustness. If one make batches of the data on could achieve some conveges to normal distribuion, but then one is actually not using the anytime propperty. Their is also other method one can look into their is overview over the different method [here](https://engineering.atspotify.com/2023/03/). \n\nI would use it for convergens rates wich is bernuli distribusion, so the posbillatys to misspecifi the liklihood is limited, since it only dependt on one parameter.\n\n## A Final Remark  \nIf I want type 2 control, I would run two SPRT tests with $\\alpha$ and $\\beta$, and I would plot the distributions.  \nBefore starting, I would check if there is a region where the fraction $\\frac{p(x)}{q(x)}$ varies significantly. I would also need to adjust $\\beta$ accordingly.  \n\n\nThere is also an extension of the method called e-variables or an e-process.\nIt seems to rely on the fact that under the null, one can change the $H_1$ distribution and still have a supermartingale.\nOne can also scale the likelihood, which they use to make a betting strategy. The idea is really interesting.\nThe idea of being able to change the hypothesis midway seems a bit suspicious. I have yet to figure out if the new hypothesis will be penalized by the earlier outcome, which would be problematic.\n\nThere is an article with an overview here.\n\n[A tiny review on e-values and e-processes](https://sas.uwaterloo.ca/~wang/files/e-review.pdf) by Ruodu Wang.\n\n",
    "supporting": [
      "sequential-hypothesis-testing and-safe-anytime-inference_files"
    ],
    "filters": [],
    "includes": {}
  }
}