{"title":"Mixture of K scaled and shifted t-distributions","markdown":{"yaml":{"title":"Mixture of K scaled and shifted t-distributions","format":{"html":{"code-fold":true}}},"headingText":"Introduktion","containsRefs":false,"markdown":"\n\nIn this project, I will examine a mixture of K scaled and shifted t-distributions.\n\nThe scaled and shifted t-distributions have the following PDF.\n$$\nf(x|\\mu,\\sigma,\\nu)=\\frac{\\Gamma((\\nu+1)/2)}{\\sqrt{\\pi\\nu\\sigma^2}\\Gamma(\\nu/2)}(1+\\frac{(x-\\mu)^2}{\\nu\\sigma^2})^{-(\\nu+1)/2}\n$$\n\n$\\mu\\in \t\\mathbb{R}$, $\\sigma>0$ and $\\nu>0$\n\nWith the mixed distribution.\n\n$$\\sum_{k=1}^{K} \\pi_k f(x|\\mu_k,\\sigma_k,\\nu_k)$$\n\n# A littel theory\nThe EM algorithm consists of a two-step optimization process to maximize the likelihood. In the Expectation (E) step, the posterior probabilities are calculated. In the Maximization (M) step, the likelihood is maximized to find the parameters, given the posterior probabilities.\n\n\n## Posterior probabilities (Expectation step)\nThe posterior probabilities correspond to the probability that an outcome is drawn from a given distribution.\n$$\nP(Z=\\pi_k|x_i)=\\frac{P(x_i|Z=\\pi_k)P(Z_i=\\pi_k)}{P(x_i)}=\\frac{\\pi_kf(x_i|\\mu_k,\\sigma_k,\\nu_k)}{\\sum_{k=1}^{K}{\\pi_k}f(x_i|\\mu_k,\\sigma_k,\\nu_k)}=w(x_i)\n$$\n\n\n\n## Maximization step M step:\nIn the M step, the coefficients $\\{\\pi_k,\\theta_k\\}$  are estimated, where $\\theta_k=\\{\\mu_k,\\sigma_k,\\nu_k\\}$\n\n\n# Log Likelyhood\nThe simplification of writing makes it possible to express the density in the form $f(x|\\pi,\\theta)\\sum_{k=1}^K \\pi_kf(x|\\theta_k)$\nwhere $\\theta_i=\\{\\mu_i,\\sigma_i^2,\\nu_i\\}$. \n\n\nMaking the the likelihood\n$$\nL(\\pi_k,\\theta_k)=\\sum_{i=1}^{N}\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k)\n$$\n\nand the negative log likelihood.\n$$\nl(\\pi_k,\\theta_k)=-\\sum_{i=1}^{N} log(\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k))\n$$\n\nFor the estimation of $\\pi_k$ Lagrange optimization can be used on the negative log-likelihood. It should be noted that  $\\sum_{k=1}^{K}\\pi_k =1$. corresponds to the linear constraint.\n\nThe Lagrange function becomes\n$$\n\\mathcal{L}(\\pi_k)=-\\sum_{i=1}^{N} log(\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k))-(\\lambda\\sum_{k=1}^{K}\\pi_k-1)\n$$\n\nLagrange optimization requires the objective function to be convex. Convexity can be checked using the second derivative.\n\nFist derivative.\n$$\n\\frac{\\partial l(\\pi_k,\\theta_k)}{\\partial \\pi_k}=-\\sum_{i=1}^{n}\\frac{f(x_i|\\theta_k)}{\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k)}\n$$\n\nSecond derivative.\n$$\n\\frac{\\partial^2 l(\\pi_k,\\theta_k)}{\\partial^2 \\pi_k}=\\sum_{i=1}^{n}\\frac{f(x_i|\\theta_k)^2}{(\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k))^2}>0\n$$\n\nSince the second derivative of the negative log-likelihood is positive for all values of $X$, the function is convex. This follows from the fact that $f(x_i,\\theta_k),\\pi_k>0$ by the definition of probabilities.\n\n### Solving the lagrancian.\n\n$$\n\\mathcal{L}(\\pi_k)=-\\sum_{i=1}^{N} log(\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k))-(\\lambda\\sum_{k=1}^{K}\\pi_k-1)\n$$\n\nFist derivative.\n$$\n\\frac{\\partial\\mathcal{L}(\\pi_k)}{\\partial \\pi_k}=\\sum_{i=1}^{N}\\frac{f(x_i|\\theta_k)}{\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k)} +\\lambda =\\sum_{i=1}^{N}w(x_i)\\frac{1}{\\pi_k} +\\lambda =0\n$$\n\nThe left side is the same as the weight, which is the posterior probabilities multiplied by $\\pi_k$. \n\nThis gives a condition for the optimal value.\n$$\n\\sum_{i=1}^{N}w(x_i)+\\pi_k\\lambda = 0 => -\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i) =\\lambda\n$$\n\nBut since $\\sum_{i=1}^{N}w(x_i)+\\pi_k\\lambda = 0$  must hold for all $k$\nand that $\\sum_{k=1}^{K}w(x_i)=1$, since these are the posterior probabilities.\nOne can derive.\n$$\n\\begin{aligned}\n0=\\sum_{k=1}^{K}(\\sum_{i=1}^{N}w(x_i)+\\pi_k\\lambda )=\\sum_{k=1}^{K}\\sum_{i=1}^{N}w(x_i)+\\sum_{k=1}^{K}\\pi_k\\lambda =\\sum_{k=1}^{K}\\sum_{i=1}^{N}w(x_i)+\\lambda \\\\ = \\sum_{k=1}^{K}\\sum_{i=1}^{N}w(x_i) -\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i)=\\sum_{i=1}^{N}\\sum_{k=1}^{K}w(x_i)\n-\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i) \\\\\n\\sum_{i=1}^{N}\\sum_{k=1}^{K}w(x_i)\n-\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i)=N-\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i)\\\\ => \\hat{\\pi}_k= \\frac{1}{N}\\sum_{i=1}^{N}w(x_i)\n\\end{aligned}\n$$\n\nThe estimate for the probability of drawing from a given distribution is the average of the weights.\n\n## Estiamtion of $\\theta_k=\\{\\mu_k,\\sigma_k,\\nu_k\\}$\nThe estimation of the parameters $\\theta_k={\\mu_k,\\sigma_k,\\nu_k}$\nis done by minimizing the negative log-likelihood.\n\nAs demonstrated below, the problem is that there is no closed-form solution. This arises from the fact that there is a sum in the denominator of the gradient.\n\n\n# Gradient of $\\theta_k$\n$$\n\\frac{\\partial l(\\theta_k)  }{\\partial \\theta_k} =- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\theta_k)}\\frac{\\partial f(x_i|\\theta_k)  }{\\partial \\theta_k}\n$$\n\nNote that in the above, it is almost the sum of the weights, so it can be useful to express$\\frac{\\partial f(x_i|\\theta_k)}{\\partial\\theta_k}$ in terms of $f(x_i|\\theta_k)$,  if possible.\n\nSince $\\theta_k$ is a simplified notation for the parameters $\\{\\mu_k,\\sigma_k,\\nu_k\\}$, which are the parameters of interest to estimate, the derivatives with respect to these parameters will be calculated and substituted into the formula above.\n\nIn the next section, \n$\\frac{\\partial f(x_i|\\theta_k)  }{\\partial \\theta_k}$ will be found with respect to \n$\\{\\mu_k,\\sigma_k,\\nu_k\\}$  and substituted into $\\frac{\\partial l(\\theta_k)  }{\\partial \\theta_k}$ to give a formula for $\\frac{\\partial l(\\theta_k)  }{\\partial \\theta_k}$\nto provide a formula for $\\frac{\\partial l(\\theta_k)  }{\\partial \\theta_k}$.\n\n\n# Finding the gradient of negativ log likelyhood for $\\nu_k$\n\n$$\n\\begin{aligned}\n\\frac{\\partial f(x_i|\\mu_k,\\sigma_k,\\nu_k)}{\\partial \\mu_k}=(\\nu_k+1)(\\frac{x_i-\\mu_k}{\\nu_k\\sigma_k^2})(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{-1}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\ =(\\nu_k+1)\\frac{x_i-\\mu_k}{\\nu_k\\sigma_k^2+(x_i-\\mu_k)^2}f(x_i|\\mu_k,\\sigma_k,\\nu_k)\n\\end{aligned}\n$$\n\nLeading to.\n\n$$\n\\begin{aligned}\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\mu_k} =- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)}\\frac{\\partial f(x_i|\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\mu_k}\\\\ =-(\\nu_k+1)\\sum_{i=1}^{N}w(x_i)\\frac{x_i-\\mu_k}{\\nu_k\\sigma_k^2+(x_i-\\mu_k)^2}\n\\end{aligned}\n$$\n\nNote that a closed form does not seem possible since $x_i$\nappears in the denominator, and the sum cannot be split. However, the gradient can still be used for faster implementation.\n\n## Finding the gradient of negativ log likelyhood for $\\sigma_k$\n\n\n$$\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k} =- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_k,\\sigma_k\\nu_k)}\\frac{\\partial f(x_i|\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k}\n$$\n\n$$\n\\begin{aligned}\n\\frac{\\partial f(x_i|\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k}=\\frac{-1}{\\sigma}f(x_i|\\mu_k,\\sigma_k,\\nu_k)+\\frac{(\\nu_k+1)(x-\\mu_k)^2}{\\nu_k\\sigma_k^3}\\frac{\\nu_k\\sigma_k^2}{\\nu_k\\sigma_k^2+(x-\\mu_k)^2}f(x_i|\\mu_k,\\sigma_k,\\nu_k)=\\\\\n\\frac{1}{\\sigma_k}f(x_i|\\mu_k,\\sigma_k,\\nu_k)(-1+\\frac{(v_k+1)(x_i-\\mu_k)^2}{\\nu_k\\sigma_k +(x_i-\\mu_k)^2})\n\\end{aligned}\n$$\n\nThis leads to the gradient\n\n$$\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k} = -\\sum_{i=1}^{N}w(x_i)(-1+\\frac{(\\nu_k+1)(x_i-\\mu_k)^2}{\\nu_k\\sigma_k +(x_i-\\mu_k)^2})\\frac{1}{\\sigma_k}\n$$\n\nSince the denominator cannot be split, the expression cannot be simplified.\n\n$$\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)}{\\partial \\nu_k}=- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_i,\\sigma_i\\nu_i)} \\frac{\\partial f(x|\\mu_k,\\sigma_k,\\nu_k) }{\\partial \\nu_k}\n$$\n\nIt can be useful to know that\n$$\n\\begin{aligned}\n\\frac{\\partial \\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}}{\\partial \\nu} = \\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma(\\nu_k/2)}-\\frac{\\Gamma((\\nu_k+1)/2)}{2\\Gamma(\\nu_k/2)^2}\\psi(\\nu_k/2)\\\\ =\\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma((\\nu_k+1)/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}-\\frac{\\psi(\\nu_k/2)}{2\\Gamma(\\nu_k/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}\n\\end{aligned}\n$$\n\nGiving the derivative.\n\n$$\n\\begin{align*}\n\\frac{\\partial f(x_i|\\mu_k,\\sigma_k^2,\\nu_k) }{\\partial \\nu_k}=-\\frac{1}{2\\nu_k}\\frac{\\Gamma((\\nu_k+1)/2)}{\\sqrt{\\pi\\nu_k\\sigma_k^2}\\Gamma(\\nu_k/2)}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{\\frac{-(\\nu_k+1)}{2}}\\\\\n+\\frac{(x_i-\\mu_k)^2(\\nu_k+1)}{\\nu_k\\sigma^2+(x_i-\\mu_k)^2}\\frac{1}{2\\nu_k}\\frac{\\Gamma((\\nu_k+1)/2)}{\\sqrt{\\pi\\nu_k\\sigma_k^2}\\Gamma(\\nu_k/2)}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{\\frac{-(\\nu_k+1)}{2}-1}\n\\\\\n+\\frac{\\partial \\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}}{\\partial \\nu_k}\\frac{1}{\\sqrt{\\pi\\nu_k\\sigma_k^2}}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k \\sigma_k^2})^{\\frac{-(\\nu_k+1)}{2}}\n\\\\ = \\\\\n-\\frac{1}{2\\nu_k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\\n+\\frac{(x_i-\\mu_k)^2(\\nu_k+1)}{\\nu_k\\sigma_k^2+(x_i-\\mu_k)^2}\\frac{1}{2\\nu_k}f(x|\\mu_k,\\sigma_k^2,\\nu_k)\n\\\\\n\\\\ +\\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma((\\nu_k+1)/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}\\frac{1}{\\sqrt{\\pi\\nu_k\\sigma_k^2}}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{-\\frac{\\nu_k+1}{2}}\\\\\n-\\frac{\\psi(\\nu_k/2)}{2\\Gamma(\\nu_k/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}\\frac{1}{\\sqrt{\\pi\\nu_k\\sigma_k^2}}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{-\\frac{\\nu_k+1}{2}}\\\\\n=\\\\\n-\\frac{1}{2\\nu_k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\\n+\\frac{(x_i-\\mu_k)^2(\\nu_k+1)}{\\nu_k\\sigma^2+(x_i-\\mu_k)^2}\\frac{1}{2\\nu_k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\\n+\\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma((\\nu_k+1)/2)}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)-\\frac{\\psi(\\nu_k/2)}{2\\Gamma(\\nu_k/2)}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\n\\end{align*}\n$$\n\nGiving the gradient of the likelihood.\n\n$$\n\\begin{aligned}\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)}{\\partial \\nu_k}=- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_i,\\sigma_i\\nu_i)} \\frac{\\partial f(x_i|\\mu_k,\\sigma_k^2,\\nu_k) }{\\partial \\nu_k}\\\\ = \n-\\sum_{i=1}^{N}\\frac{-1}{2}w(x_i)+\\frac{(\\nu_k+1)}{2}\\frac{(x_i-\\mu_k)^2}{ \\nu_k^2\\sigma_k^2}w(x_i)+\\frac{\\psi((\\nu+1)/2)}{2\\Gamma((\\nu+1)/2)}w(x_i)-\\frac{\\psi(\\nu/2)}{2\\Gamma(\\nu/2)}w(x_i)\\\\\n\\end{aligned}\n$$\n\nAll the gradients are computed. The goal is to obtain the maximum likelihood, but there is no closed-form solution. For some special cases, the functions might be approximated nicely. However, the gradients can still be used for gradient descent, which is the method I have chosen.\n\n# Gradient decent.\nGradient descent is used in the M step to optimize the parameters given the weights.\n\nTwo problems arise when using gradient descent. These are described below.\n\n## Undershooting the gradient out of domain.\nIf gradient descent undershoots and sets $\\nu_i<0$ ore $\\sigma_i<0$, the density is not defined. This is handled by setting a minimum value for the parameters $\\nu_i$ and $\\sigma_i$. This means that the minimum value for these parameters is not zero but a value close to zero.\n\nUndershooting can also cause problems when using built-in optimization tools. A pro tip is that if you encounter NaN values as output during optimization, it’s a good idea to start debugging with this issue in mind. In my experience, there is not always a built-in check for these cases.\n\n## Gradient explotion\nThis does not fix the problem of gradient explosion. What happens is that the gradient decent may take too large a step in either direction. To make matters worse, the sum in the expression for the gradients can cause the gradients to grow as the sample size increases, which can lead to gradient explosion.\n\nTo combat this, I have tried different strategies.\nBelow are the two method i tried described.\n\n\n## Normalising the gradient\n\nWhen analyzing the gradient, it's important to note that the sum is not divided by the size of $X$(denoted as $N$). This means that the absolute value of the gradient grows with $N$. If gradient descent is used with a fixed learning rate $\\alpha$, problems may arise where the gradient either undershoots or overshoots. In an implementation setting, this issue is very apparent. However, in real-world problems where the true parameters are unknown, it is impossible to know if the estimated values are accurate.\n\nThere are two methods to address this issue: one is to check the norm of the gradient to see if you are near a local optimum, and the other is to increase the number of iterations. I have chosen to normalize the gradient by dividing by $N$. This corresponds to choosing $\\alpha$ based on the size of $X$.\n\n\n## Gradient clipping\nGradient clipping works by setting a maximum value for the norm of the gradient. Specifically, if $|\\nabla l(X,\\pi,\\theta)|>c$ then the gradient is scaled to $\\nabla l=c*\\frac{\\nabla l}{|\\nabla l|}$. This allows you to set a maximum value for the norm of the gradient step.\n\nSince this method is based on the norm of the entire gradient, it would be beneficial to vary the clipping based on the type of parameter $\\{\\mu,\\sigma,\\nu\\}$., especially for $\\mu$. I have implemented a version where the clipping is based on different parameters.\n\n\nIn testing, it seems that gradient clipping is easy to use and works better. Therefore, in the implementation, gradient clipping is used.\n\nBelow is a summary of the gradient descent approach\n#Gradient decent.\nGradient descent is used in the M step to optimize the parameters given the weights.\n\nThere is no closed-form solution for many of the gradients. To make matters worse, the sum in the expression for the gradients can cause the gradients to grow as the sample size increases, potentially leading to exploding gradients. If gradient descent undershoots and sets \n$\\nu_i<0$ or $\\sigma_i<0$, the density is not defined. I have addressed the issue of undershooting by setting a minimum value for the parameters \n$\\nu_i$ and $\\sigma_i$. This means that the minimum is not zero but a value close to zero.\n\nHowever, this does not solve the problem of overshooting gradients. The solution to this issue is normalization. One strategy is to normalize the gradient by the sample size \nN. Another solution is to use gradient clipping. Gradient clipping works by setting a maximum value for the norm of the gradient. Specifically, if $|\\nabla l|>c$, then the gradient is set to \n$\\nabla l=c*\\frac{\\nabla l}{|\\nabla l|}$. This allows you to set a maximum value for the norm of the gradient step. Since this method is based on the norm of the entire gradient, it would be beneficial to vary the clipping based on the type of parameter $\\{\\mu,\\sigma,\\nu\\}$, especially for $\\mu$.\n\nI have implemented a version where the clipping is based on different parameter types. Specifically, if the gradient matrix column corresponding to a given parameter is normalized by its corresponding vector and scaled by a given constant for that parameter. If this explanation is confusing, refer to the function Gradient_clipping_vec in the Rcpp file, which is fairly self-explanatory.\n\nAdditionally, I have included a check to see if a steep step leads to smaller values in the objective function. If not, the gradient is scaled again by $\\alpha$.\n\n### Implentatsion notes for gradient decent\nSince the EM algorithm is an iterative process and R handles loops poorly, Rcpp is used to run C++ code for faster implementation. This is a common approach in many libraries.\n\n## Combining it all to one EM function.\nSince the EM algorithm is a two-step optimization process, it is not strictly necessary to find the smallest value in the M step. The critical aspect is that both steps lead to a decrease in the negative likelihood.\n\nIn terms of implementation, this affects the settings used for running gradient descent. For example, setting a maximum number of iterations too high can result in very long runtimes.\n\nI have not found any definitive guidelines for tuning these parameters in general settings, so I have made it possible for the end user to adjust them as needed.\n\n## Initialization\nThe EM algorithm requires starting parameters, and if they are too far off, it can affect the runtime. I have chosen to use the hard clustering technique, k-means++, which provides a set of partitions. In each of these partitions, the scaled and shifted t-distributions are fitted and used as the starting parameters.\n\nK-means++ works similarly to k-means, with the primary difference being in the initialization process. The goal of k-means is to minimize the objective function:\n\n$\\underset{\\mu}{\\arg\\min}\\sum_{k=1}^{K}\\sum_{i=1}^{N}|x_i-\\mu_k|^2$\nK-means++ improves on this by initially selecting one center randomly and then iteratively assigning points to the nearest center and updating the centers. It can be shown that this process will decrease the objective function, making it a greedy algorithm.\n\n\n# Potential problems\nK-means has the potential problem of local minima. This can be illustrated in two dimensions by assigning points to the four corners of a square and using three of them as starting positions, which can lead to poor initializations.\n\nThe issue of non-uniqueness for optima should not be problematic in our case, as we are dealing with continuous distributions (or at least it seems unlikely).\n\nK-means++ is also sensitive to outliers. This problem is present in the structure of the likelihood for mixture models as well. If there is a small cluster of outliers, the fitted t-distribution can become very sharp, giving a high likelihood to those points. This should be mitigated by ensuring that the probability of drawing from this distribution is very small. However, there is no guarantee that this is always the case. Essentially, this means that outliers may not be treated as such but rather as draws from a separate distribution.\n\nDespite these issues, the method based on k-means++ still seems like the best option. Otherwise, one can set the starting parameters manually.\n\nFor estimating the individual t-distributions, we use the condition \n$\\nu >2$. Given this, we can use the following results:\n\n$\\mathbb{E}[X]=\\mu$ and $Var(X)=\\sigma^2\\frac{\\nu}{\\nu-2}\\rightarrow 2\\frac{ Var(X)}{Var(X)-\\sigma^2}=\\nu$\nore \n$Var(X)=\\sigma^2\\frac{\\nu}{\\nu-2}\\rightarrow \\sqrt{Var(X)\\frac{\\nu-2}{\\nu}}=\\sigma$\n\nSince this is only for initialization, if the parameters for the t-distributions are overshot, the EM algorithm should correct for it. The advantage is that with a good estimate of the parameters \n$\\mu$ and \n$\\nu$, and $\\sigma$, standard optimization can be used to estimate the last parameter based on the log likelihood.\n\nThis method often fails, so it may be better to use grid search or manual tuning.\n\n## GridSearch Initialization\nIn most cases, the method of using K-means partitions to estimate $\\mu$ and using the proportions to estimate \n$\\pi$ seems reasonable. So, I have implemented a version where GridSearch is used for the parameters $\\sigma$ and $\\nu$, which is what caused problems before. By doing this, one gets the advantage of the simplicity and relative speed of using K-means while obtaining usable starting parameters for $\\sigma$ and $\\nu$.\n\n# Measurs\nIf one wants to compare models, the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) are available.\n\n\n# Code \nLibrays used.\n```{r,warning=FALSE,message=FALSE}\n# Load required libraries\nlibrary(this.path) # find where you are\n\n# Note current script location\ncurrent_script_directory <- this.dir()\n# library's to Rcpp\nlibrary(\"Rcpp\")\nlibrary(\"RcppArmadillo\")\nlibrary(\"RcppGSL\")\nsourceCpp(\"compstat.cpp\")\n\nlibrary(ClusterR)\n#KMeans_rcpp #kmeans++ works well and is in Rcpp\n\n# for fiting scaled and shifted t-distribution\nlibrary(fitdistrplus)\nlibrary(MASS)\n\n\n# for paraleel gridseach in the intilasation\nlibrary(doParallel)\nlibrary(foreach)\n\n```\n\nMy own implented code i R.\n```{r,warning=FALSE,message=FALSE}\n# build function in R\nlibrary(\"metRology\")\nlibrary(\"extraDistr\")\n\n#function for generating a vector of draws from a mixture T distribuations\nDraw_mixture_componet <- function(Pi=c(0.2, 0.5, 0.3),\n                                  Mu=c(0,25,50),\n                                  Sigma=c(1,2,1),\n                                  Nu=c(3,4,4),\n                                  N_samples=1000){\n  #sample mixture componets\n  mixture_componoets_sample <- sample(x = 1:length(Pi), size = N_samples, replace = TRUE, prob = Pi)\n  \n  draws=c(rep(0,N_samples))\n  for (i in 1:N_samples){\n    d_i=sample(x = 1:length(Pi), size = 1, replace = TRUE, prob = Pi) #draw_index = d_i\n    draws[i]=rt.scaled(n = 1, df = Nu[d_i], mean = Mu[d_i], sd = Sigma[d_i])\n  }\n  return(draws)\n}\n\n# itnernall function of initialation\ninitialize_if_na <- function(var,# variabels\n                             K# numer of partions\n                             ) {\n  if (any(is.na(var))) {\n    return(rep(NA, K))\n  }\n  return(var)\n}\n# estimation of PI by taking proportion of partions\nEstiame_Pi_from_partions= function(Partions_vec,K){\n  Pi=c(rep(0,K))\n  for (i in 1:K){\n    Mid_vec=Partions_vec==i\n    Pi[i]=sum(Mid_vec)/length(Partions_vec)\n    }\n  return(Pi)\n}\n\n# This is not the best parameter, but for now its better than guissing.\n# it seam to work well for well seperated distribuations\nIntiall_parameter_optimasation<-function(X#Data from parametasion\n    ){\n  Mu<-mean(X)\n  variance=var(X)\n  fn1<-function(s){sum(-(dlst(X, df=abs(s), mu = Mu, sigma = sqrt(variance*(abs(s)-2)/abs(s)) , log = TRUE)))}\n  result <- optim(par = 5, fn = fn1, method = \"L-BFGS-B\",\n  lower = c(2+0.05))\n \n  Nu_val=(abs(result$par))\n  Sigma_val=sqrt(variance*(Nu_val-2)/Nu_val) \n  \n  return(c(Mu,Sigma_val,Nu_val))\n}\n\n\n\n\nIntiall_parameter<-function(X,#Data in vector form\n                        Pi=NA,#Problillaty vector\n                        Mu=NA,# mean vector\n                        Sigma=NA,#Sigma vector \n                        Nu =NA, # NU vector\n                        K # Number of distribuations\n                  ){\n  if(any(c(is.na(Pi),is.na(Mu),is.na(Sigma),is.na(Nu)))){\n    # get partions\n    KMeans_objet=KMeans_rcpp(as.matrix(X), clusters = K, num_init = 30, initializer = 'kmeans++')\n  }\n  if(any(is.na(Pi))){\n    #Estimate Pi\n    Pi=Estiame_Pi_from_partions(KMeans_objet$clusters,K)\n  }\n  \n  if(any(c(is.na(Mu),is.na(Sigma),is.na(Nu)))){\n    #Make sure their is vector \n    Mu=initialize_if_na(Mu,K)\n    Sigma=initialize_if_na(Sigma,K)\n    Nu=initialize_if_na(Nu,K)\n    for (i in 1:K){\n      partin_data=X[KMeans_objet$clusters==i]\n      partin_parameter=Intiall_parameter_optimasation(partin_data)\n      if(is.na(Mu[i])){\n        Mu[i]=partin_parameter[1]\n      }\n      if(is.na(Sigma[i])){\n        Sigma[i]=partin_parameter[2]\n      }\n      if(is.na(Nu[i])){\n        Nu[i]=partin_parameter[3]\n      }\n    }\n  }\n  ret_obj=list(Mu = Mu,\n               Sigma = Sigma,\n               Nu=Nu,\n               Pi=Pi)\n  return(ret_obj)\n}\n\n\n\n\n\n\n# use kmeans to Pi estimate pi and mu\nIntiall_parameter_grid=function(X,Pi,Mu,Sigma_grid,Nu_grid,K){\n  if(any(c(is.na(Pi),is.na(Mu)))){\n    # get partions\n    KMeans_objet=KMeans_rcpp(as.matrix(X), clusters = K, num_init = 30, initializer = 'kmeans++')\n  }\n  if(any(is.na(Pi))){\n    #Estimate Pi\n    Pi=Estiame_Pi_from_partions(KMeans_objet$clusters,K)\n  }\n  \n  if(any(is.na(Mu))){\n    #Make sure their is vector \n    Mu=initialize_if_na(Mu,K)\n    for (i in 1:K){\n      partin_data=X[KMeans_objet$clusters==i]\n      \n      if(is.na(Mu[i])){\n        Mu[i]=mean(partin_data)\n      }\n    }\n  }\n  #preform gridseach but only over paramters Sigma and nu\n  \n  #Finding combinations\n  #Sigma_grid=as.numeric(seq(2,5))\n  #make list with comnations \n  Sigma_grid_combinations <- expand.grid(rep(list(Sigma_grid), K))\n  #makes List of list whith every combantion in the seq Sigma_grid\n  list_of_combinations_Sigma <- split(as.matrix(Sigma_grid_combinations), seq(nrow(Sigma_grid_combinations)))\n  \n  #Nu_grid=as.numeric(seq(2,10))\n  Nu_grid_combinations<-expand.grid(rep(list(Nu_grid), K))\n  list_of_combinations_Nu <- split(as.matrix(Nu_grid_combinations), seq(nrow(Nu_grid_combinations)))\n  \n  \n  grid_for_seach=expand.grid(list_of_combinations_Sigma,list_of_combinations_Nu)\n  \n  \n  # This can be paralised but, it mean define all the functions from Rcpp in R so they can be importet into the clusters\n  #souch rcpp can export function to paralles so redefine functions\n  \n  # # Number of cores to use\n  # num_cores <- detectCores() - 1 # supose to be nice to let one stand for other stuff\n  # # Create a cluster\n  # cl <- makeCluster(num_cores)\n  # registerDoParallel(cl)\n  # clusterExport(cl, c(\"loglikelyhood_t_mix\", \"X\", \"Pi\", \"Mu\", \"grid_for_seach\"))\n  # \n  # results <- foreach(i = 1:nrow(grid_for_seach), .combine = rbind) %dopar% {\n  # row <- grid_for_seach[i, ]\n  # var1 <- as.vector(unlist((row[1]))) \n  # var2 <-  as.vector(unlist((row[2])))#\n  # \n  # log_likelihood_val <- den_test(X, Pi, Mu, c(5,5,5), c(4,4,4)) # function can be importet to clusters so the method wont work\n  # c(var1 = var1, var2 = var2, log_likelihood_val = log_likelihood_val) \n  # }\n  # stopCluster(cl)\n  \n  compute_log_likelihood <- function(row) {\n  var1 <- as.vector(unlist(row[1]))\n  var2 <- as.vector(unlist(row[2]))\n  loglikelyhood_t_mix(X, Pi, Mu, var1, var2)\n  }\n  \n  log_likelihood_values <- sapply(1:nrow(grid_for_seach), function(i) {\n    compute_log_likelihood(grid_for_seach[i, ])\n    })\n  max_log_likelihood <- max(log_likelihood_values)\n  max_index <- which.max(log_likelihood_values)\n  Sigma=as.vector(unlist(grid_for_seach[max_index,]$Var1))\n  Nu=as.vector(unlist(grid_for_seach[max_index,]$Var2))\n  ret_obj=list(Mu = Mu,\n               Sigma = Sigma,\n               Nu=Nu,\n               Pi=Pi,\n               likelyhood=max_log_likelihood,\n               Numer_of_combination=nrow(grid_for_seach))\n  return(ret_obj)\n}\n\n\n\n\n#Kmeans is used to run make intiall partions, when Scaled shifted t distribuation is fitted on each partions.\n# This method works well when the distribuations is well seperated, but not if mixture distributuin is to close.\n#basically what happen if the partions is to close what happens is that partions do not look like t distribution, and can somtime even look like uniform distribution (basically far of), this make the estimation of the ustabel and sometimes lead to error, the solution is to come with some manuall inputs \n\nEM_Mix_T_Dist<-function(X,#Data in vector form\n                        Pi=NA,#Problillaty vector\n                        Mu=NA,# mean vector\n                        Sigma=NA,#Sigma vector \n                        Nu =NA, # NU vector\n                        K, # Number of distribuations\n                        Clipping_vector=c(10,5,2),# Max value for clipping  for parameters (Mu,Sigma,Ni)\n                        Max_iter=100,# maxium number of interations for the EM algorithm\n                        alpha=0.02,# scaling for gradient decent\n                        max_iter_gradient=3,# maxium number of times in graident for each iteratin\n                        norm_gradient = 0.1, # stop criteria for gradient decent\n                        Start_method_optim=T){\n  #Checking for intiations process\n  #if no argument is given for Pi , Mu , Sigma and NU is given then a gues is made\n  if(Start_method_optim==T){\n    start_para=Intiall_parameter(X,Pi,Mu,Sigma,Nu,K)\n  }\n  else{\n    start_para=Intiall_parameter_grid(X,Pi,Mu,Sigma_grid=as.numeric(seq(1,15,2)),as.numeric(seq(2,10,2)),K)\n  }\n  Pi=start_para$Pi\n  Mu=start_para$Mu\n  Sigma=start_para$Sigma\n  Nu=start_para$Nu\n  \n  if(any(c(is.na(Pi),is.na(Mu),is.na(Sigma),is.na(Nu)))){\n    print(\"The intilasaions failed\")\n    return(NA)\n  } \n  #Run EM\n  EM_partion=EM_t_distribution(X,Pi,Mu,Sigma,Nu,Clipping_vector = Clipping_vector,Max_iter=Max_iter,alpha=alpha,max_iter_gradient=max_iter_gradient,norm_gradient = norm_gradient)\n  \n  #Make return object\n  likelyhood=likelyhood_t_mix(X,EM_partion[,4],EM_partion[,1],EM_partion[,2],EM_partion[,3])\n  loglikelyhood=loglikelyhood_t_mix(X,EM_partion[,4],EM_partion[,1],EM_partion[,2],EM_partion[,3])\n  AIC=2*K-2*loglikelyhood\n  BIC=K*log(length(X))-2*loglikelyhood\n  ret_obj=list(Mu = EM_partion[,1],\n               Sigma = EM_partion[,2],\n               Nu=EM_partion[,3],\n               Pi=EM_partion[,4],\n               AIC=AIC,\n               BIC=BIC,\n               likelyhood=likelyhood,\n               loglikelyhood=loglikelyhood,\n               weights=Weights_of_X(X,EM_partion[,4],EM_partion[,1],EM_partion[,2],EM_partion[,3]))\n  return(ret_obj)\n}\n\n```\n\nRcpp library\n\n<details>\n<summary>Click to expand/collapse the C++ code</summary>\n```{r,results='asis',echo=FALSE}\n# Path to your C++ file\ncpp_file_path <- \"compstat.cpp\"\n\n# Read the C++ file into an R character vector\ncpp_code <- readLines(cpp_file_path, warn = FALSE)\n\n# Print the content of the C++ file inside a code block with C++ syntax highlighting\nknitr::asis_output(paste0(\"```cpp\\n\", paste(cpp_code, collapse = \"\\n\"), \"\\n```\"))\n```\n\n<details>\n\nCode for visulsations.\n```{r}\n#code for visualasation\nlibrary(ggplot2)\n\nPlot_mix_t_distribution <- function(X,Pi,Mu,Sigma,Nu,bins=30){\n  plot_fun<-function(x){sapply(x,function(x){Mix_T_density_x(x,Pi,Mu,Sigma,Nu)})}\n  plot <- ggplot(data.frame(X), aes(x = X)) +\n    geom_histogram(aes(y = ..density..), bins = bins, color = \"black\", alpha = 0.7) +\n    stat_function(fun = plot_fun, geom = \"line\",color=\"red\")+\n    labs(title = \"Histogram with Mixture of t-Distributions\", x = \"X\", y = \"Density\") +\n    theme_minimal()  \n  return(plot)\n}\n\n```\n\n\n\n# Test\nBelow, I have created a small test of the application with a mixture of three well-separated distributions.\n\n```{r,warning=FALSE}\n# test\n\ndraws=Draw_mixture_componet()\n\nmodel<-EM_Mix_T_Dist(draws,K=3)\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins=50)\n\n\n\n```\nAs can be seen, the application performs very well.\n\n# A Look into When Initialization Fails\nThe initialization of the distribusion is too close together, as illustrated below.\n```{r}\nset.seed(3123)\ndraws=Draw_mixture_componet(Mu=c(-1,4,10))\n\nmodel<-EM_Mix_T_Dist(draws,K=3)\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins=50)\n```\n\nAbove, I have drawn from a mixture model with three components and fitted a three-component mixture model using automated initialization, even though there are three distinct peaks. The initialization fails.\n\nBelow, there is a second example, again with three components, but in this case, it is not easy to tell if there are two or three components.\n\n```{r}\n#Make \ndraws=Draw_mixture_componet(Mu=c(0,2,10))\n\nhist(draws,breaks = 100)\n\nKMeans_objet=KMeans_rcpp(as.matrix(draws), clusters = 3, num_init = 90, initializer = 'kmeans++')\npar(mfrow = c(1, 3))\nhist(draws[KMeans_objet$clusters==1],breaks = 30)\nhist(draws[KMeans_objet$clusters==2],breaks = 30)\nhist(draws[KMeans_objet$clusters==3],breaks = 30)\n\npar(mfrow = c(1, 1))\n\n#if one runs the stadart implentasion here the starting guase would fail.\n#model<-EM_Mix_T_Dist(draws,K=3)\n\n\n```\n\nIn the above, the histograms of the fractions generated from the K-means++ are shown. Cluster 1 is the well-separated cluster. The rest of the partitions do not resemble a T-distribution, which seems to lead to the high values of $\\nu$\n\nThe above is generated from mixing three t-distributions but could look like two distributions. My implementation for standardization will, in this case, fail. It will return since the value of gradient becomes to high do to large value of $\\nu$.\n\nBelow, I showcase how one can manually tune the input and run the method.\n```{r}\nPi_test=Estiame_Pi_from_partions(KMeans_objet$clusters,K=3)\nmu_test=c(mean(draws[KMeans_objet$clusters==1]),mean(draws[KMeans_objet$clusters==2]),mean(draws[KMeans_objet$clusters==3]))\n\n\n# here is to example on how to hand fit the values \n\npar(mfrow = c(1, 2))\nPlot_mix_t_distribution(draws,Pi_test,mu_test,c(1,2,3),c(2.5,2.5,4),bins=100)\n\nPlot_mix_t_distribution(draws,c(0.2,0.4,0.4),mu_test,c(1,1.7,3),c(2.5,2.5,4),bins=100)\npar(mfrow = c(1, 1))\n\n\n\n```\n\nLastly, we examine what the implementation yields after manual tuning is used as the starting values.\n\n```{r}\n\nPi_test=Estiame_Pi_from_partions(KMeans_objet$clusters,K=3)\nmu_test=c(mean(draws[KMeans_objet$clusters==1]),mean(draws[KMeans_objet$clusters==2]),mean(draws[KMeans_objet$clusters==3]))\n\nmodel<-EM_Mix_T_Dist(draws,K=3,Pi=c(0.2,0.4,0.4),Mu=mu_test,Sigma=c(1,1.7,3),Nu=c(2.5,2.5,4),,Max_iter = 100, max_iter_gradient=3)\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins = 100)\n\n```\n\nAs can be seen the Em algorithm still strugels here.\n\nIf one runs the implication with $K=2$ their is no problems.\n\n```{r}\n\nmodel<-EM_Mix_T_Dist(draws,K=2)\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins = 100)\n\n```\n\n## Test with second initialization\n```{r,cache=TRUE}\nset.seed(3123)\ndraws=Draw_mixture_componet(Mu=c(-1,4,10))\n\nmodel<-EM_Mix_T_Dist(draws,K=3,Start_method_optim = F)\n\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins=50)\n```\n\nThe method based on the grid search is computational expensiv but can give good result.\n\n\n\n## Why the initialization fails.\nIf the starting values are too extreme, such as $\\nu_i=50000$, the gradient cannot be computed properly due to issues with the gamma function or its derivative. As it stands, the initialization can produce such problematic values if the data from k-means is not well-behaved. If the distribution is not well-separated, the hard clustering technique will not capture enough of the distribution in the decreasing part as the function moves out. This can lead to excessively high values of $\\nu$.\n\n# Aplicantion on real data\n\n```{r,cache=TRUE,message=FALSE,warning=FALSE}\nlibrary(mixsmsn)\n\n# Load the dataset\n\n\ndata_real_test<-as.vector( unlist(faithful$eruptions))\n\n\nparamenter=Intiall_parameter_grid(data_real_test,Pi=NA,Mu=NA,Sigma=seq(0.1,4,0.5),Nu=seq(1,3,0.5),K=2)\n\n\n\nmodel<-EM_Mix_T_Dist(data_real_test,Pi=paramenter$Pi,Mu=paramenter$Mu,Sigma = paramenter$Sigma,Nu=paramenter$Nu,K=2,max_iter_gradient = 40,Max_iter =200 )\nPlot_mix_t_distribution(data_real_test,model$Pi,model$Mu,model$Sigma,model$Nu,bins=50)\n\n\n```\n\nAbove, I have applied the method to real-world data from the Old Faithful geyser in Yellowstone National Park. The y-axis represents the eruption height. I had to manually feed the grid optimization into the function to get some decent starting values, and I had to run the application more times than the standard, but overall, the method works.\n\nPro-tip: Potentially, one can speed up the process by hand-tuning the grid search.\n\n\n# Further improvements\n\n## Model selection\nFurther improvements should be made to the initialization process, as this would enable automatic model selection based on AIC or BIC. \n\n## confidence bands\nImplementing confidence bands using a parametric bootstrap approach would be beneficial. Their is small complication since $\\pi_i$ label ambiguity. For faster implementation, the bootstrap process could utilize a \"hot start,\" meaning setting the starting values for the parameters to the estimated values.\n\n### Initialization\nThe initialization could run in parallel for both the grid search and the optimization method. It seems like R won’t import functions defined in a separate Rcpp file to the cluster. There are two options: define the necessary functions and dependent functions directly in R via the RcppFunction, or do the parallelization in Rcpp. Doing the parallelization directly in Rcpp would be better.\n\n### Gradient decent.\nIt would be nice if the gradient descent could use more iterations in the later steps. This is fairly easy to implement, but for an end-user and for me, it's hard to set a good standard setting. Potentially, some of the same effect can be achieved by increasing the number of iterations since if the weights don't change, it would almost be the same.\n\n\n\nFor now the project is closed.\n\n\n\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n# Introduktion\nIn this project, I will examine a mixture of K scaled and shifted t-distributions.\n\nThe scaled and shifted t-distributions have the following PDF.\n$$\nf(x|\\mu,\\sigma,\\nu)=\\frac{\\Gamma((\\nu+1)/2)}{\\sqrt{\\pi\\nu\\sigma^2}\\Gamma(\\nu/2)}(1+\\frac{(x-\\mu)^2}{\\nu\\sigma^2})^{-(\\nu+1)/2}\n$$\n\n$\\mu\\in \t\\mathbb{R}$, $\\sigma>0$ and $\\nu>0$\n\nWith the mixed distribution.\n\n$$\\sum_{k=1}^{K} \\pi_k f(x|\\mu_k,\\sigma_k,\\nu_k)$$\n\n# A littel theory\nThe EM algorithm consists of a two-step optimization process to maximize the likelihood. In the Expectation (E) step, the posterior probabilities are calculated. In the Maximization (M) step, the likelihood is maximized to find the parameters, given the posterior probabilities.\n\n\n## Posterior probabilities (Expectation step)\nThe posterior probabilities correspond to the probability that an outcome is drawn from a given distribution.\n$$\nP(Z=\\pi_k|x_i)=\\frac{P(x_i|Z=\\pi_k)P(Z_i=\\pi_k)}{P(x_i)}=\\frac{\\pi_kf(x_i|\\mu_k,\\sigma_k,\\nu_k)}{\\sum_{k=1}^{K}{\\pi_k}f(x_i|\\mu_k,\\sigma_k,\\nu_k)}=w(x_i)\n$$\n\n\n\n## Maximization step M step:\nIn the M step, the coefficients $\\{\\pi_k,\\theta_k\\}$  are estimated, where $\\theta_k=\\{\\mu_k,\\sigma_k,\\nu_k\\}$\n\n\n# Log Likelyhood\nThe simplification of writing makes it possible to express the density in the form $f(x|\\pi,\\theta)\\sum_{k=1}^K \\pi_kf(x|\\theta_k)$\nwhere $\\theta_i=\\{\\mu_i,\\sigma_i^2,\\nu_i\\}$. \n\n\nMaking the the likelihood\n$$\nL(\\pi_k,\\theta_k)=\\sum_{i=1}^{N}\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k)\n$$\n\nand the negative log likelihood.\n$$\nl(\\pi_k,\\theta_k)=-\\sum_{i=1}^{N} log(\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k))\n$$\n\nFor the estimation of $\\pi_k$ Lagrange optimization can be used on the negative log-likelihood. It should be noted that  $\\sum_{k=1}^{K}\\pi_k =1$. corresponds to the linear constraint.\n\nThe Lagrange function becomes\n$$\n\\mathcal{L}(\\pi_k)=-\\sum_{i=1}^{N} log(\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k))-(\\lambda\\sum_{k=1}^{K}\\pi_k-1)\n$$\n\nLagrange optimization requires the objective function to be convex. Convexity can be checked using the second derivative.\n\nFist derivative.\n$$\n\\frac{\\partial l(\\pi_k,\\theta_k)}{\\partial \\pi_k}=-\\sum_{i=1}^{n}\\frac{f(x_i|\\theta_k)}{\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k)}\n$$\n\nSecond derivative.\n$$\n\\frac{\\partial^2 l(\\pi_k,\\theta_k)}{\\partial^2 \\pi_k}=\\sum_{i=1}^{n}\\frac{f(x_i|\\theta_k)^2}{(\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k))^2}>0\n$$\n\nSince the second derivative of the negative log-likelihood is positive for all values of $X$, the function is convex. This follows from the fact that $f(x_i,\\theta_k),\\pi_k>0$ by the definition of probabilities.\n\n### Solving the lagrancian.\n\n$$\n\\mathcal{L}(\\pi_k)=-\\sum_{i=1}^{N} log(\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k))-(\\lambda\\sum_{k=1}^{K}\\pi_k-1)\n$$\n\nFist derivative.\n$$\n\\frac{\\partial\\mathcal{L}(\\pi_k)}{\\partial \\pi_k}=\\sum_{i=1}^{N}\\frac{f(x_i|\\theta_k)}{\\sum_{k=1}^{K}\\pi_kf(x_i|\\theta_k)} +\\lambda =\\sum_{i=1}^{N}w(x_i)\\frac{1}{\\pi_k} +\\lambda =0\n$$\n\nThe left side is the same as the weight, which is the posterior probabilities multiplied by $\\pi_k$. \n\nThis gives a condition for the optimal value.\n$$\n\\sum_{i=1}^{N}w(x_i)+\\pi_k\\lambda = 0 => -\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i) =\\lambda\n$$\n\nBut since $\\sum_{i=1}^{N}w(x_i)+\\pi_k\\lambda = 0$  must hold for all $k$\nand that $\\sum_{k=1}^{K}w(x_i)=1$, since these are the posterior probabilities.\nOne can derive.\n$$\n\\begin{aligned}\n0=\\sum_{k=1}^{K}(\\sum_{i=1}^{N}w(x_i)+\\pi_k\\lambda )=\\sum_{k=1}^{K}\\sum_{i=1}^{N}w(x_i)+\\sum_{k=1}^{K}\\pi_k\\lambda =\\sum_{k=1}^{K}\\sum_{i=1}^{N}w(x_i)+\\lambda \\\\ = \\sum_{k=1}^{K}\\sum_{i=1}^{N}w(x_i) -\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i)=\\sum_{i=1}^{N}\\sum_{k=1}^{K}w(x_i)\n-\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i) \\\\\n\\sum_{i=1}^{N}\\sum_{k=1}^{K}w(x_i)\n-\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i)=N-\\frac{1}{\\pi_k}\\sum_{i=1}^{N}w(x_i)\\\\ => \\hat{\\pi}_k= \\frac{1}{N}\\sum_{i=1}^{N}w(x_i)\n\\end{aligned}\n$$\n\nThe estimate for the probability of drawing from a given distribution is the average of the weights.\n\n## Estiamtion of $\\theta_k=\\{\\mu_k,\\sigma_k,\\nu_k\\}$\nThe estimation of the parameters $\\theta_k={\\mu_k,\\sigma_k,\\nu_k}$\nis done by minimizing the negative log-likelihood.\n\nAs demonstrated below, the problem is that there is no closed-form solution. This arises from the fact that there is a sum in the denominator of the gradient.\n\n\n# Gradient of $\\theta_k$\n$$\n\\frac{\\partial l(\\theta_k)  }{\\partial \\theta_k} =- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\theta_k)}\\frac{\\partial f(x_i|\\theta_k)  }{\\partial \\theta_k}\n$$\n\nNote that in the above, it is almost the sum of the weights, so it can be useful to express$\\frac{\\partial f(x_i|\\theta_k)}{\\partial\\theta_k}$ in terms of $f(x_i|\\theta_k)$,  if possible.\n\nSince $\\theta_k$ is a simplified notation for the parameters $\\{\\mu_k,\\sigma_k,\\nu_k\\}$, which are the parameters of interest to estimate, the derivatives with respect to these parameters will be calculated and substituted into the formula above.\n\nIn the next section, \n$\\frac{\\partial f(x_i|\\theta_k)  }{\\partial \\theta_k}$ will be found with respect to \n$\\{\\mu_k,\\sigma_k,\\nu_k\\}$  and substituted into $\\frac{\\partial l(\\theta_k)  }{\\partial \\theta_k}$ to give a formula for $\\frac{\\partial l(\\theta_k)  }{\\partial \\theta_k}$\nto provide a formula for $\\frac{\\partial l(\\theta_k)  }{\\partial \\theta_k}$.\n\n\n# Finding the gradient of negativ log likelyhood for $\\nu_k$\n\n$$\n\\begin{aligned}\n\\frac{\\partial f(x_i|\\mu_k,\\sigma_k,\\nu_k)}{\\partial \\mu_k}=(\\nu_k+1)(\\frac{x_i-\\mu_k}{\\nu_k\\sigma_k^2})(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{-1}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\ =(\\nu_k+1)\\frac{x_i-\\mu_k}{\\nu_k\\sigma_k^2+(x_i-\\mu_k)^2}f(x_i|\\mu_k,\\sigma_k,\\nu_k)\n\\end{aligned}\n$$\n\nLeading to.\n\n$$\n\\begin{aligned}\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\mu_k} =- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)}\\frac{\\partial f(x_i|\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\mu_k}\\\\ =-(\\nu_k+1)\\sum_{i=1}^{N}w(x_i)\\frac{x_i-\\mu_k}{\\nu_k\\sigma_k^2+(x_i-\\mu_k)^2}\n\\end{aligned}\n$$\n\nNote that a closed form does not seem possible since $x_i$\nappears in the denominator, and the sum cannot be split. However, the gradient can still be used for faster implementation.\n\n## Finding the gradient of negativ log likelyhood for $\\sigma_k$\n\n\n$$\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k} =- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_k,\\sigma_k\\nu_k)}\\frac{\\partial f(x_i|\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k}\n$$\n\n$$\n\\begin{aligned}\n\\frac{\\partial f(x_i|\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k}=\\frac{-1}{\\sigma}f(x_i|\\mu_k,\\sigma_k,\\nu_k)+\\frac{(\\nu_k+1)(x-\\mu_k)^2}{\\nu_k\\sigma_k^3}\\frac{\\nu_k\\sigma_k^2}{\\nu_k\\sigma_k^2+(x-\\mu_k)^2}f(x_i|\\mu_k,\\sigma_k,\\nu_k)=\\\\\n\\frac{1}{\\sigma_k}f(x_i|\\mu_k,\\sigma_k,\\nu_k)(-1+\\frac{(v_k+1)(x_i-\\mu_k)^2}{\\nu_k\\sigma_k +(x_i-\\mu_k)^2})\n\\end{aligned}\n$$\n\nThis leads to the gradient\n\n$$\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k} = -\\sum_{i=1}^{N}w(x_i)(-1+\\frac{(\\nu_k+1)(x_i-\\mu_k)^2}{\\nu_k\\sigma_k +(x_i-\\mu_k)^2})\\frac{1}{\\sigma_k}\n$$\n\nSince the denominator cannot be split, the expression cannot be simplified.\n\n$$\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)}{\\partial \\nu_k}=- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_i,\\sigma_i\\nu_i)} \\frac{\\partial f(x|\\mu_k,\\sigma_k,\\nu_k) }{\\partial \\nu_k}\n$$\n\nIt can be useful to know that\n$$\n\\begin{aligned}\n\\frac{\\partial \\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}}{\\partial \\nu} = \\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma(\\nu_k/2)}-\\frac{\\Gamma((\\nu_k+1)/2)}{2\\Gamma(\\nu_k/2)^2}\\psi(\\nu_k/2)\\\\ =\\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma((\\nu_k+1)/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}-\\frac{\\psi(\\nu_k/2)}{2\\Gamma(\\nu_k/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}\n\\end{aligned}\n$$\n\nGiving the derivative.\n\n$$\n\\begin{align*}\n\\frac{\\partial f(x_i|\\mu_k,\\sigma_k^2,\\nu_k) }{\\partial \\nu_k}=-\\frac{1}{2\\nu_k}\\frac{\\Gamma((\\nu_k+1)/2)}{\\sqrt{\\pi\\nu_k\\sigma_k^2}\\Gamma(\\nu_k/2)}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{\\frac{-(\\nu_k+1)}{2}}\\\\\n+\\frac{(x_i-\\mu_k)^2(\\nu_k+1)}{\\nu_k\\sigma^2+(x_i-\\mu_k)^2}\\frac{1}{2\\nu_k}\\frac{\\Gamma((\\nu_k+1)/2)}{\\sqrt{\\pi\\nu_k\\sigma_k^2}\\Gamma(\\nu_k/2)}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{\\frac{-(\\nu_k+1)}{2}-1}\n\\\\\n+\\frac{\\partial \\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}}{\\partial \\nu_k}\\frac{1}{\\sqrt{\\pi\\nu_k\\sigma_k^2}}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k \\sigma_k^2})^{\\frac{-(\\nu_k+1)}{2}}\n\\\\ = \\\\\n-\\frac{1}{2\\nu_k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\\n+\\frac{(x_i-\\mu_k)^2(\\nu_k+1)}{\\nu_k\\sigma_k^2+(x_i-\\mu_k)^2}\\frac{1}{2\\nu_k}f(x|\\mu_k,\\sigma_k^2,\\nu_k)\n\\\\\n\\\\ +\\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma((\\nu_k+1)/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}\\frac{1}{\\sqrt{\\pi\\nu_k\\sigma_k^2}}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{-\\frac{\\nu_k+1}{2}}\\\\\n-\\frac{\\psi(\\nu_k/2)}{2\\Gamma(\\nu_k/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}\\frac{1}{\\sqrt{\\pi\\nu_k\\sigma_k^2}}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{-\\frac{\\nu_k+1}{2}}\\\\\n=\\\\\n-\\frac{1}{2\\nu_k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\\n+\\frac{(x_i-\\mu_k)^2(\\nu_k+1)}{\\nu_k\\sigma^2+(x_i-\\mu_k)^2}\\frac{1}{2\\nu_k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\\n+\\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma((\\nu_k+1)/2)}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)-\\frac{\\psi(\\nu_k/2)}{2\\Gamma(\\nu_k/2)}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\n\\end{align*}\n$$\n\nGiving the gradient of the likelihood.\n\n$$\n\\begin{aligned}\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)}{\\partial \\nu_k}=- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_i,\\sigma_i\\nu_i)} \\frac{\\partial f(x_i|\\mu_k,\\sigma_k^2,\\nu_k) }{\\partial \\nu_k}\\\\ = \n-\\sum_{i=1}^{N}\\frac{-1}{2}w(x_i)+\\frac{(\\nu_k+1)}{2}\\frac{(x_i-\\mu_k)^2}{ \\nu_k^2\\sigma_k^2}w(x_i)+\\frac{\\psi((\\nu+1)/2)}{2\\Gamma((\\nu+1)/2)}w(x_i)-\\frac{\\psi(\\nu/2)}{2\\Gamma(\\nu/2)}w(x_i)\\\\\n\\end{aligned}\n$$\n\nAll the gradients are computed. The goal is to obtain the maximum likelihood, but there is no closed-form solution. For some special cases, the functions might be approximated nicely. However, the gradients can still be used for gradient descent, which is the method I have chosen.\n\n# Gradient decent.\nGradient descent is used in the M step to optimize the parameters given the weights.\n\nTwo problems arise when using gradient descent. These are described below.\n\n## Undershooting the gradient out of domain.\nIf gradient descent undershoots and sets $\\nu_i<0$ ore $\\sigma_i<0$, the density is not defined. This is handled by setting a minimum value for the parameters $\\nu_i$ and $\\sigma_i$. This means that the minimum value for these parameters is not zero but a value close to zero.\n\nUndershooting can also cause problems when using built-in optimization tools. A pro tip is that if you encounter NaN values as output during optimization, it’s a good idea to start debugging with this issue in mind. In my experience, there is not always a built-in check for these cases.\n\n## Gradient explotion\nThis does not fix the problem of gradient explosion. What happens is that the gradient decent may take too large a step in either direction. To make matters worse, the sum in the expression for the gradients can cause the gradients to grow as the sample size increases, which can lead to gradient explosion.\n\nTo combat this, I have tried different strategies.\nBelow are the two method i tried described.\n\n\n## Normalising the gradient\n\nWhen analyzing the gradient, it's important to note that the sum is not divided by the size of $X$(denoted as $N$). This means that the absolute value of the gradient grows with $N$. If gradient descent is used with a fixed learning rate $\\alpha$, problems may arise where the gradient either undershoots or overshoots. In an implementation setting, this issue is very apparent. However, in real-world problems where the true parameters are unknown, it is impossible to know if the estimated values are accurate.\n\nThere are two methods to address this issue: one is to check the norm of the gradient to see if you are near a local optimum, and the other is to increase the number of iterations. I have chosen to normalize the gradient by dividing by $N$. This corresponds to choosing $\\alpha$ based on the size of $X$.\n\n\n## Gradient clipping\nGradient clipping works by setting a maximum value for the norm of the gradient. Specifically, if $|\\nabla l(X,\\pi,\\theta)|>c$ then the gradient is scaled to $\\nabla l=c*\\frac{\\nabla l}{|\\nabla l|}$. This allows you to set a maximum value for the norm of the gradient step.\n\nSince this method is based on the norm of the entire gradient, it would be beneficial to vary the clipping based on the type of parameter $\\{\\mu,\\sigma,\\nu\\}$., especially for $\\mu$. I have implemented a version where the clipping is based on different parameters.\n\n\nIn testing, it seems that gradient clipping is easy to use and works better. Therefore, in the implementation, gradient clipping is used.\n\nBelow is a summary of the gradient descent approach\n#Gradient decent.\nGradient descent is used in the M step to optimize the parameters given the weights.\n\nThere is no closed-form solution for many of the gradients. To make matters worse, the sum in the expression for the gradients can cause the gradients to grow as the sample size increases, potentially leading to exploding gradients. If gradient descent undershoots and sets \n$\\nu_i<0$ or $\\sigma_i<0$, the density is not defined. I have addressed the issue of undershooting by setting a minimum value for the parameters \n$\\nu_i$ and $\\sigma_i$. This means that the minimum is not zero but a value close to zero.\n\nHowever, this does not solve the problem of overshooting gradients. The solution to this issue is normalization. One strategy is to normalize the gradient by the sample size \nN. Another solution is to use gradient clipping. Gradient clipping works by setting a maximum value for the norm of the gradient. Specifically, if $|\\nabla l|>c$, then the gradient is set to \n$\\nabla l=c*\\frac{\\nabla l}{|\\nabla l|}$. This allows you to set a maximum value for the norm of the gradient step. Since this method is based on the norm of the entire gradient, it would be beneficial to vary the clipping based on the type of parameter $\\{\\mu,\\sigma,\\nu\\}$, especially for $\\mu$.\n\nI have implemented a version where the clipping is based on different parameter types. Specifically, if the gradient matrix column corresponding to a given parameter is normalized by its corresponding vector and scaled by a given constant for that parameter. If this explanation is confusing, refer to the function Gradient_clipping_vec in the Rcpp file, which is fairly self-explanatory.\n\nAdditionally, I have included a check to see if a steep step leads to smaller values in the objective function. If not, the gradient is scaled again by $\\alpha$.\n\n### Implentatsion notes for gradient decent\nSince the EM algorithm is an iterative process and R handles loops poorly, Rcpp is used to run C++ code for faster implementation. This is a common approach in many libraries.\n\n## Combining it all to one EM function.\nSince the EM algorithm is a two-step optimization process, it is not strictly necessary to find the smallest value in the M step. The critical aspect is that both steps lead to a decrease in the negative likelihood.\n\nIn terms of implementation, this affects the settings used for running gradient descent. For example, setting a maximum number of iterations too high can result in very long runtimes.\n\nI have not found any definitive guidelines for tuning these parameters in general settings, so I have made it possible for the end user to adjust them as needed.\n\n## Initialization\nThe EM algorithm requires starting parameters, and if they are too far off, it can affect the runtime. I have chosen to use the hard clustering technique, k-means++, which provides a set of partitions. In each of these partitions, the scaled and shifted t-distributions are fitted and used as the starting parameters.\n\nK-means++ works similarly to k-means, with the primary difference being in the initialization process. The goal of k-means is to minimize the objective function:\n\n$\\underset{\\mu}{\\arg\\min}\\sum_{k=1}^{K}\\sum_{i=1}^{N}|x_i-\\mu_k|^2$\nK-means++ improves on this by initially selecting one center randomly and then iteratively assigning points to the nearest center and updating the centers. It can be shown that this process will decrease the objective function, making it a greedy algorithm.\n\n\n# Potential problems\nK-means has the potential problem of local minima. This can be illustrated in two dimensions by assigning points to the four corners of a square and using three of them as starting positions, which can lead to poor initializations.\n\nThe issue of non-uniqueness for optima should not be problematic in our case, as we are dealing with continuous distributions (or at least it seems unlikely).\n\nK-means++ is also sensitive to outliers. This problem is present in the structure of the likelihood for mixture models as well. If there is a small cluster of outliers, the fitted t-distribution can become very sharp, giving a high likelihood to those points. This should be mitigated by ensuring that the probability of drawing from this distribution is very small. However, there is no guarantee that this is always the case. Essentially, this means that outliers may not be treated as such but rather as draws from a separate distribution.\n\nDespite these issues, the method based on k-means++ still seems like the best option. Otherwise, one can set the starting parameters manually.\n\nFor estimating the individual t-distributions, we use the condition \n$\\nu >2$. Given this, we can use the following results:\n\n$\\mathbb{E}[X]=\\mu$ and $Var(X)=\\sigma^2\\frac{\\nu}{\\nu-2}\\rightarrow 2\\frac{ Var(X)}{Var(X)-\\sigma^2}=\\nu$\nore \n$Var(X)=\\sigma^2\\frac{\\nu}{\\nu-2}\\rightarrow \\sqrt{Var(X)\\frac{\\nu-2}{\\nu}}=\\sigma$\n\nSince this is only for initialization, if the parameters for the t-distributions are overshot, the EM algorithm should correct for it. The advantage is that with a good estimate of the parameters \n$\\mu$ and \n$\\nu$, and $\\sigma$, standard optimization can be used to estimate the last parameter based on the log likelihood.\n\nThis method often fails, so it may be better to use grid search or manual tuning.\n\n## GridSearch Initialization\nIn most cases, the method of using K-means partitions to estimate $\\mu$ and using the proportions to estimate \n$\\pi$ seems reasonable. So, I have implemented a version where GridSearch is used for the parameters $\\sigma$ and $\\nu$, which is what caused problems before. By doing this, one gets the advantage of the simplicity and relative speed of using K-means while obtaining usable starting parameters for $\\sigma$ and $\\nu$.\n\n# Measurs\nIf one wants to compare models, the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) are available.\n\n\n# Code \nLibrays used.\n```{r,warning=FALSE,message=FALSE}\n# Load required libraries\nlibrary(this.path) # find where you are\n\n# Note current script location\ncurrent_script_directory <- this.dir()\n# library's to Rcpp\nlibrary(\"Rcpp\")\nlibrary(\"RcppArmadillo\")\nlibrary(\"RcppGSL\")\nsourceCpp(\"compstat.cpp\")\n\nlibrary(ClusterR)\n#KMeans_rcpp #kmeans++ works well and is in Rcpp\n\n# for fiting scaled and shifted t-distribution\nlibrary(fitdistrplus)\nlibrary(MASS)\n\n\n# for paraleel gridseach in the intilasation\nlibrary(doParallel)\nlibrary(foreach)\n\n```\n\nMy own implented code i R.\n```{r,warning=FALSE,message=FALSE}\n# build function in R\nlibrary(\"metRology\")\nlibrary(\"extraDistr\")\n\n#function for generating a vector of draws from a mixture T distribuations\nDraw_mixture_componet <- function(Pi=c(0.2, 0.5, 0.3),\n                                  Mu=c(0,25,50),\n                                  Sigma=c(1,2,1),\n                                  Nu=c(3,4,4),\n                                  N_samples=1000){\n  #sample mixture componets\n  mixture_componoets_sample <- sample(x = 1:length(Pi), size = N_samples, replace = TRUE, prob = Pi)\n  \n  draws=c(rep(0,N_samples))\n  for (i in 1:N_samples){\n    d_i=sample(x = 1:length(Pi), size = 1, replace = TRUE, prob = Pi) #draw_index = d_i\n    draws[i]=rt.scaled(n = 1, df = Nu[d_i], mean = Mu[d_i], sd = Sigma[d_i])\n  }\n  return(draws)\n}\n\n# itnernall function of initialation\ninitialize_if_na <- function(var,# variabels\n                             K# numer of partions\n                             ) {\n  if (any(is.na(var))) {\n    return(rep(NA, K))\n  }\n  return(var)\n}\n# estimation of PI by taking proportion of partions\nEstiame_Pi_from_partions= function(Partions_vec,K){\n  Pi=c(rep(0,K))\n  for (i in 1:K){\n    Mid_vec=Partions_vec==i\n    Pi[i]=sum(Mid_vec)/length(Partions_vec)\n    }\n  return(Pi)\n}\n\n# This is not the best parameter, but for now its better than guissing.\n# it seam to work well for well seperated distribuations\nIntiall_parameter_optimasation<-function(X#Data from parametasion\n    ){\n  Mu<-mean(X)\n  variance=var(X)\n  fn1<-function(s){sum(-(dlst(X, df=abs(s), mu = Mu, sigma = sqrt(variance*(abs(s)-2)/abs(s)) , log = TRUE)))}\n  result <- optim(par = 5, fn = fn1, method = \"L-BFGS-B\",\n  lower = c(2+0.05))\n \n  Nu_val=(abs(result$par))\n  Sigma_val=sqrt(variance*(Nu_val-2)/Nu_val) \n  \n  return(c(Mu,Sigma_val,Nu_val))\n}\n\n\n\n\nIntiall_parameter<-function(X,#Data in vector form\n                        Pi=NA,#Problillaty vector\n                        Mu=NA,# mean vector\n                        Sigma=NA,#Sigma vector \n                        Nu =NA, # NU vector\n                        K # Number of distribuations\n                  ){\n  if(any(c(is.na(Pi),is.na(Mu),is.na(Sigma),is.na(Nu)))){\n    # get partions\n    KMeans_objet=KMeans_rcpp(as.matrix(X), clusters = K, num_init = 30, initializer = 'kmeans++')\n  }\n  if(any(is.na(Pi))){\n    #Estimate Pi\n    Pi=Estiame_Pi_from_partions(KMeans_objet$clusters,K)\n  }\n  \n  if(any(c(is.na(Mu),is.na(Sigma),is.na(Nu)))){\n    #Make sure their is vector \n    Mu=initialize_if_na(Mu,K)\n    Sigma=initialize_if_na(Sigma,K)\n    Nu=initialize_if_na(Nu,K)\n    for (i in 1:K){\n      partin_data=X[KMeans_objet$clusters==i]\n      partin_parameter=Intiall_parameter_optimasation(partin_data)\n      if(is.na(Mu[i])){\n        Mu[i]=partin_parameter[1]\n      }\n      if(is.na(Sigma[i])){\n        Sigma[i]=partin_parameter[2]\n      }\n      if(is.na(Nu[i])){\n        Nu[i]=partin_parameter[3]\n      }\n    }\n  }\n  ret_obj=list(Mu = Mu,\n               Sigma = Sigma,\n               Nu=Nu,\n               Pi=Pi)\n  return(ret_obj)\n}\n\n\n\n\n\n\n# use kmeans to Pi estimate pi and mu\nIntiall_parameter_grid=function(X,Pi,Mu,Sigma_grid,Nu_grid,K){\n  if(any(c(is.na(Pi),is.na(Mu)))){\n    # get partions\n    KMeans_objet=KMeans_rcpp(as.matrix(X), clusters = K, num_init = 30, initializer = 'kmeans++')\n  }\n  if(any(is.na(Pi))){\n    #Estimate Pi\n    Pi=Estiame_Pi_from_partions(KMeans_objet$clusters,K)\n  }\n  \n  if(any(is.na(Mu))){\n    #Make sure their is vector \n    Mu=initialize_if_na(Mu,K)\n    for (i in 1:K){\n      partin_data=X[KMeans_objet$clusters==i]\n      \n      if(is.na(Mu[i])){\n        Mu[i]=mean(partin_data)\n      }\n    }\n  }\n  #preform gridseach but only over paramters Sigma and nu\n  \n  #Finding combinations\n  #Sigma_grid=as.numeric(seq(2,5))\n  #make list with comnations \n  Sigma_grid_combinations <- expand.grid(rep(list(Sigma_grid), K))\n  #makes List of list whith every combantion in the seq Sigma_grid\n  list_of_combinations_Sigma <- split(as.matrix(Sigma_grid_combinations), seq(nrow(Sigma_grid_combinations)))\n  \n  #Nu_grid=as.numeric(seq(2,10))\n  Nu_grid_combinations<-expand.grid(rep(list(Nu_grid), K))\n  list_of_combinations_Nu <- split(as.matrix(Nu_grid_combinations), seq(nrow(Nu_grid_combinations)))\n  \n  \n  grid_for_seach=expand.grid(list_of_combinations_Sigma,list_of_combinations_Nu)\n  \n  \n  # This can be paralised but, it mean define all the functions from Rcpp in R so they can be importet into the clusters\n  #souch rcpp can export function to paralles so redefine functions\n  \n  # # Number of cores to use\n  # num_cores <- detectCores() - 1 # supose to be nice to let one stand for other stuff\n  # # Create a cluster\n  # cl <- makeCluster(num_cores)\n  # registerDoParallel(cl)\n  # clusterExport(cl, c(\"loglikelyhood_t_mix\", \"X\", \"Pi\", \"Mu\", \"grid_for_seach\"))\n  # \n  # results <- foreach(i = 1:nrow(grid_for_seach), .combine = rbind) %dopar% {\n  # row <- grid_for_seach[i, ]\n  # var1 <- as.vector(unlist((row[1]))) \n  # var2 <-  as.vector(unlist((row[2])))#\n  # \n  # log_likelihood_val <- den_test(X, Pi, Mu, c(5,5,5), c(4,4,4)) # function can be importet to clusters so the method wont work\n  # c(var1 = var1, var2 = var2, log_likelihood_val = log_likelihood_val) \n  # }\n  # stopCluster(cl)\n  \n  compute_log_likelihood <- function(row) {\n  var1 <- as.vector(unlist(row[1]))\n  var2 <- as.vector(unlist(row[2]))\n  loglikelyhood_t_mix(X, Pi, Mu, var1, var2)\n  }\n  \n  log_likelihood_values <- sapply(1:nrow(grid_for_seach), function(i) {\n    compute_log_likelihood(grid_for_seach[i, ])\n    })\n  max_log_likelihood <- max(log_likelihood_values)\n  max_index <- which.max(log_likelihood_values)\n  Sigma=as.vector(unlist(grid_for_seach[max_index,]$Var1))\n  Nu=as.vector(unlist(grid_for_seach[max_index,]$Var2))\n  ret_obj=list(Mu = Mu,\n               Sigma = Sigma,\n               Nu=Nu,\n               Pi=Pi,\n               likelyhood=max_log_likelihood,\n               Numer_of_combination=nrow(grid_for_seach))\n  return(ret_obj)\n}\n\n\n\n\n#Kmeans is used to run make intiall partions, when Scaled shifted t distribuation is fitted on each partions.\n# This method works well when the distribuations is well seperated, but not if mixture distributuin is to close.\n#basically what happen if the partions is to close what happens is that partions do not look like t distribution, and can somtime even look like uniform distribution (basically far of), this make the estimation of the ustabel and sometimes lead to error, the solution is to come with some manuall inputs \n\nEM_Mix_T_Dist<-function(X,#Data in vector form\n                        Pi=NA,#Problillaty vector\n                        Mu=NA,# mean vector\n                        Sigma=NA,#Sigma vector \n                        Nu =NA, # NU vector\n                        K, # Number of distribuations\n                        Clipping_vector=c(10,5,2),# Max value for clipping  for parameters (Mu,Sigma,Ni)\n                        Max_iter=100,# maxium number of interations for the EM algorithm\n                        alpha=0.02,# scaling for gradient decent\n                        max_iter_gradient=3,# maxium number of times in graident for each iteratin\n                        norm_gradient = 0.1, # stop criteria for gradient decent\n                        Start_method_optim=T){\n  #Checking for intiations process\n  #if no argument is given for Pi , Mu , Sigma and NU is given then a gues is made\n  if(Start_method_optim==T){\n    start_para=Intiall_parameter(X,Pi,Mu,Sigma,Nu,K)\n  }\n  else{\n    start_para=Intiall_parameter_grid(X,Pi,Mu,Sigma_grid=as.numeric(seq(1,15,2)),as.numeric(seq(2,10,2)),K)\n  }\n  Pi=start_para$Pi\n  Mu=start_para$Mu\n  Sigma=start_para$Sigma\n  Nu=start_para$Nu\n  \n  if(any(c(is.na(Pi),is.na(Mu),is.na(Sigma),is.na(Nu)))){\n    print(\"The intilasaions failed\")\n    return(NA)\n  } \n  #Run EM\n  EM_partion=EM_t_distribution(X,Pi,Mu,Sigma,Nu,Clipping_vector = Clipping_vector,Max_iter=Max_iter,alpha=alpha,max_iter_gradient=max_iter_gradient,norm_gradient = norm_gradient)\n  \n  #Make return object\n  likelyhood=likelyhood_t_mix(X,EM_partion[,4],EM_partion[,1],EM_partion[,2],EM_partion[,3])\n  loglikelyhood=loglikelyhood_t_mix(X,EM_partion[,4],EM_partion[,1],EM_partion[,2],EM_partion[,3])\n  AIC=2*K-2*loglikelyhood\n  BIC=K*log(length(X))-2*loglikelyhood\n  ret_obj=list(Mu = EM_partion[,1],\n               Sigma = EM_partion[,2],\n               Nu=EM_partion[,3],\n               Pi=EM_partion[,4],\n               AIC=AIC,\n               BIC=BIC,\n               likelyhood=likelyhood,\n               loglikelyhood=loglikelyhood,\n               weights=Weights_of_X(X,EM_partion[,4],EM_partion[,1],EM_partion[,2],EM_partion[,3]))\n  return(ret_obj)\n}\n\n```\n\nRcpp library\n\n<details>\n<summary>Click to expand/collapse the C++ code</summary>\n```{r,results='asis',echo=FALSE}\n# Path to your C++ file\ncpp_file_path <- \"compstat.cpp\"\n\n# Read the C++ file into an R character vector\ncpp_code <- readLines(cpp_file_path, warn = FALSE)\n\n# Print the content of the C++ file inside a code block with C++ syntax highlighting\nknitr::asis_output(paste0(\"```cpp\\n\", paste(cpp_code, collapse = \"\\n\"), \"\\n```\"))\n```\n\n<details>\n\nCode for visulsations.\n```{r}\n#code for visualasation\nlibrary(ggplot2)\n\nPlot_mix_t_distribution <- function(X,Pi,Mu,Sigma,Nu,bins=30){\n  plot_fun<-function(x){sapply(x,function(x){Mix_T_density_x(x,Pi,Mu,Sigma,Nu)})}\n  plot <- ggplot(data.frame(X), aes(x = X)) +\n    geom_histogram(aes(y = ..density..), bins = bins, color = \"black\", alpha = 0.7) +\n    stat_function(fun = plot_fun, geom = \"line\",color=\"red\")+\n    labs(title = \"Histogram with Mixture of t-Distributions\", x = \"X\", y = \"Density\") +\n    theme_minimal()  \n  return(plot)\n}\n\n```\n\n\n\n# Test\nBelow, I have created a small test of the application with a mixture of three well-separated distributions.\n\n```{r,warning=FALSE}\n# test\n\ndraws=Draw_mixture_componet()\n\nmodel<-EM_Mix_T_Dist(draws,K=3)\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins=50)\n\n\n\n```\nAs can be seen, the application performs very well.\n\n# A Look into When Initialization Fails\nThe initialization of the distribusion is too close together, as illustrated below.\n```{r}\nset.seed(3123)\ndraws=Draw_mixture_componet(Mu=c(-1,4,10))\n\nmodel<-EM_Mix_T_Dist(draws,K=3)\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins=50)\n```\n\nAbove, I have drawn from a mixture model with three components and fitted a three-component mixture model using automated initialization, even though there are three distinct peaks. The initialization fails.\n\nBelow, there is a second example, again with three components, but in this case, it is not easy to tell if there are two or three components.\n\n```{r}\n#Make \ndraws=Draw_mixture_componet(Mu=c(0,2,10))\n\nhist(draws,breaks = 100)\n\nKMeans_objet=KMeans_rcpp(as.matrix(draws), clusters = 3, num_init = 90, initializer = 'kmeans++')\npar(mfrow = c(1, 3))\nhist(draws[KMeans_objet$clusters==1],breaks = 30)\nhist(draws[KMeans_objet$clusters==2],breaks = 30)\nhist(draws[KMeans_objet$clusters==3],breaks = 30)\n\npar(mfrow = c(1, 1))\n\n#if one runs the stadart implentasion here the starting guase would fail.\n#model<-EM_Mix_T_Dist(draws,K=3)\n\n\n```\n\nIn the above, the histograms of the fractions generated from the K-means++ are shown. Cluster 1 is the well-separated cluster. The rest of the partitions do not resemble a T-distribution, which seems to lead to the high values of $\\nu$\n\nThe above is generated from mixing three t-distributions but could look like two distributions. My implementation for standardization will, in this case, fail. It will return since the value of gradient becomes to high do to large value of $\\nu$.\n\nBelow, I showcase how one can manually tune the input and run the method.\n```{r}\nPi_test=Estiame_Pi_from_partions(KMeans_objet$clusters,K=3)\nmu_test=c(mean(draws[KMeans_objet$clusters==1]),mean(draws[KMeans_objet$clusters==2]),mean(draws[KMeans_objet$clusters==3]))\n\n\n# here is to example on how to hand fit the values \n\npar(mfrow = c(1, 2))\nPlot_mix_t_distribution(draws,Pi_test,mu_test,c(1,2,3),c(2.5,2.5,4),bins=100)\n\nPlot_mix_t_distribution(draws,c(0.2,0.4,0.4),mu_test,c(1,1.7,3),c(2.5,2.5,4),bins=100)\npar(mfrow = c(1, 1))\n\n\n\n```\n\nLastly, we examine what the implementation yields after manual tuning is used as the starting values.\n\n```{r}\n\nPi_test=Estiame_Pi_from_partions(KMeans_objet$clusters,K=3)\nmu_test=c(mean(draws[KMeans_objet$clusters==1]),mean(draws[KMeans_objet$clusters==2]),mean(draws[KMeans_objet$clusters==3]))\n\nmodel<-EM_Mix_T_Dist(draws,K=3,Pi=c(0.2,0.4,0.4),Mu=mu_test,Sigma=c(1,1.7,3),Nu=c(2.5,2.5,4),,Max_iter = 100, max_iter_gradient=3)\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins = 100)\n\n```\n\nAs can be seen the Em algorithm still strugels here.\n\nIf one runs the implication with $K=2$ their is no problems.\n\n```{r}\n\nmodel<-EM_Mix_T_Dist(draws,K=2)\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins = 100)\n\n```\n\n## Test with second initialization\n```{r,cache=TRUE}\nset.seed(3123)\ndraws=Draw_mixture_componet(Mu=c(-1,4,10))\n\nmodel<-EM_Mix_T_Dist(draws,K=3,Start_method_optim = F)\n\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins=50)\n```\n\nThe method based on the grid search is computational expensiv but can give good result.\n\n\n\n## Why the initialization fails.\nIf the starting values are too extreme, such as $\\nu_i=50000$, the gradient cannot be computed properly due to issues with the gamma function or its derivative. As it stands, the initialization can produce such problematic values if the data from k-means is not well-behaved. If the distribution is not well-separated, the hard clustering technique will not capture enough of the distribution in the decreasing part as the function moves out. This can lead to excessively high values of $\\nu$.\n\n# Aplicantion on real data\n\n```{r,cache=TRUE,message=FALSE,warning=FALSE}\nlibrary(mixsmsn)\n\n# Load the dataset\n\n\ndata_real_test<-as.vector( unlist(faithful$eruptions))\n\n\nparamenter=Intiall_parameter_grid(data_real_test,Pi=NA,Mu=NA,Sigma=seq(0.1,4,0.5),Nu=seq(1,3,0.5),K=2)\n\n\n\nmodel<-EM_Mix_T_Dist(data_real_test,Pi=paramenter$Pi,Mu=paramenter$Mu,Sigma = paramenter$Sigma,Nu=paramenter$Nu,K=2,max_iter_gradient = 40,Max_iter =200 )\nPlot_mix_t_distribution(data_real_test,model$Pi,model$Mu,model$Sigma,model$Nu,bins=50)\n\n\n```\n\nAbove, I have applied the method to real-world data from the Old Faithful geyser in Yellowstone National Park. The y-axis represents the eruption height. I had to manually feed the grid optimization into the function to get some decent starting values, and I had to run the application more times than the standard, but overall, the method works.\n\nPro-tip: Potentially, one can speed up the process by hand-tuning the grid search.\n\n\n# Further improvements\n\n## Model selection\nFurther improvements should be made to the initialization process, as this would enable automatic model selection based on AIC or BIC. \n\n## confidence bands\nImplementing confidence bands using a parametric bootstrap approach would be beneficial. Their is small complication since $\\pi_i$ label ambiguity. For faster implementation, the bootstrap process could utilize a \"hot start,\" meaning setting the starting values for the parameters to the estimated values.\n\n### Initialization\nThe initialization could run in parallel for both the grid search and the optimization method. It seems like R won’t import functions defined in a separate Rcpp file to the cluster. There are two options: define the necessary functions and dependent functions directly in R via the RcppFunction, or do the parallelization in Rcpp. Doing the parallelization directly in Rcpp would be better.\n\n### Gradient decent.\nIt would be nice if the gradient descent could use more iterations in the later steps. This is fairly easy to implement, but for an end-user and for me, it's hard to set a good standard setting. Potentially, some of the same effect can be achieved by increasing the number of iterations since if the weights don't change, it would almost be the same.\n\n\n\nFor now the project is closed.\n\n\n\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"Mixture of K scaled and shifted t-distributions.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","title":"Mixture of K scaled and shifted t-distributions"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}