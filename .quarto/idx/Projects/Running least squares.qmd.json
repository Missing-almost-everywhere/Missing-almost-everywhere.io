{"title":"Update rules for least squares","markdown":{"yaml":{"title":"Update rules for least squares","format":{"html":{"code-fold":true}}},"headingText":"Introduction","containsRefs":false,"markdown":"\nIn this project, I will explore fitting a running linear model as well as running predictions.\n\nDuring my bachelor's studies, I focused on the prediction of time series. The model I used was fitted using least squares.\nI worked with [FFP3](https://robjhyndman.com/publications/), which is implemented in R.\n\nAt that time, I used the Forecast package but ended up implementing the estimator from the ground up.\nAs I have improved my programming and mathematical skills, I thought it would be fun to revisit this problem and see if I could develop a better solution. \n\nRob how wrote FFP3, has a blog their i sais their is trick to speeding op the calculation of rolling predition for AR process, and i think figured out.\n\n\nlest squres can be written as\n\n$$\\underset{\\beta}{argmin} ||Y-X\\beta||^2$$\n\nSince this is a linear function the problem above i convex with one uniq closed form solution \n\n$\\hat{\\beta}=(X^TX)^{-1}X^TY$\n\n# Running Prediction\nWhen using a running model, a simple solution is to add a new data point and fit the model again.\nThis approach works but is not very efficient.\n\nInstead, let’s consider whether we can update the $\\beta$ estimates directly.\n\n\n\nOne option is the [Recursive least squares filter](https://en.wikipedia.org/wiki/Recursive_least_squares_filter)\n\n\n\nLastly, let’s explore how this can be used.\n\nLet $x[i]$  represent the i-th index of the vector.\n\nFor $XX^T$ a rank-1 update can be expressed as:\n\n$$(X_{new}X_{new}^T)=(X_{old}^TX_{old})+x_{new}^Tx_{new}$$ \n\nThis becomes clear when the product is written out explicitly.\nLook at $(X^TX)[j,g]=x_j^Tx_g$ So $\\sum_{i=1}^{n}x_{j}[i]x_{g}[i]$\n\nNext, consider the first entry in the inner product.\n$(X_{old}X_{old}^T)_{[1,1]}=x_j^Tx_g$ So $\\sum_{i=1}^{n}x_{j}[i]x_{g}[i]$\n\nNow look at.\n\n$(X_{new}X_{new}^T)[j,g]=(\\sum_{i=1}^{n}x_{j}[i]x_{g}[i])+x_{new}[j]x_{new}[g]$\n\nuse [Sherman–Morrison formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula)\n\nFor the update of $(X^TX)$\n\n $$(X_{new}^TX_{new})^{-1}=(X_{old}^TX_{old}+x_{new}^Tx_{new})^{-1}=(X_{old}^TX_{old})^{-1}-\\frac{(X_{old}^TX_{old})^{-1}x_{new}^Tx_{new}(X_{old}^TX_{old})^{-1}}{1+x_{new}(X_{old}^TX_{old})^{-1}x_{new}^T}$$\n\nLet's look at the nex part of the expresion\n\nFor $B=X^TY$ the udpate rule is\n$$B=(X_{new}^TY_{new})=(X_{old}^TY_{old})+x_{new}*y_{new}$$ \n\nnote here that $y_{new}$ act as a scale\n\n\n# Use Cases\nRolling prediction can be used for the evaluation of time series.\n\nHowever, one can also use these techniques for the fast implementation of a sliding window estimator or simply to update a linear model as new data arrives.\n\nI will create two implementations: one where I use this for rolling prediction, and another that implements a sliding window.\n\nFor the sliding window, it is necessary not only to add the new row but also to remove the old one.\n\n\n```{python}\nimport numpy as np\nimport unittest\n\ndef get_rank_updated_matrix(A: list, v: list, u: list):\n    return A + v.T @ u\ndef test_get_rank_updated_matrix():\n    A_test=np.array([[1,0],[0,1]])\n    v_test=np.array([[1,1]])\n    u_test=np.array([[1,1]])\n    assert (np.all(get_rank_updated_matrix(A_test,v_test,u_test)==np.array([[2, 1], [1, 2]])))\n    return\ntest_get_rank_updated_matrix()\n\n\ndef test_get_inverse_updated_matrix_remove_row():\n    X_T_inner_X_inverse_test=np.array([[1,1],[1,1]])\n    x_test=np.array([[1,1]])\n    assert np.all(get_inverse_updated_matrix(X_T_inner_X_inverse_test,x_test)==np.array([[1,1],[1,1]])+4/5)\n    return\n\ndef get_inverse_updated_matrix_remove_row(X_T_inner_X_inverse: list, x: list):\n    return X_T_inner_X_inverse + X_T_inner_X_inverse @ (x.T @ x) @ X_T_inner_X_inverse / (1 + x @ X_T_inner_X_inverse @ x.T)\n\n\ndef get_inverse_updated_matrix(X_T_inner_X_inverse: list, x: list):\n    return X_T_inner_X_inverse - X_T_inner_X_inverse @ (x.T @ x) @ X_T_inner_X_inverse / (1 + x @ X_T_inner_X_inverse @ x.T)\n\ndef test_get_inverse_updated_matrix():\n    X_T_inner_X_inverse_test=np.array([[1,1],[1,1]])+4/5#np.array([[1,1],[1,1]])\n    x_test=np.array([[1,1]])\n    assert np.all(get_inverse_updated_matrix_remove_row(X_T_inner_X_inverse_test,x_test)==np.array([[1,1],[1,1]]))\n    return\n\n\ndef get_least_sqrd_closed_form_parameter(X: list, Y: list):\n    return np.linalg.inv(X.T @ X) @ X.T @ Y\n\ndef test_get_least_sqrd_closed_form_parameter():\n    X_test=np.array([[1,0],[0,1]])\n    Y_test=np.array([[1],[1]])\n    assert np.all((get_least_sqrd_closed_form_parameter(X_test,Y_test))==[[1],[1]])\n    return\ntest_get_least_sqrd_closed_form_parameter()\n\ndef get_least_sqrd_A_B_parameter(A: list, B: list):\n    return A @ B\n\ndef test_get_least_sqrd_A_B_parameter():\n    A_test=np.array([[1,0],[0,1]])\n    B_test=np.array([[1,0],[0,1]])\n    assert np.all(get_least_sqrd_A_B_parameter(A_test,B_test)==[[1,0],[0,1]])\n    return\ntest_get_least_sqrd_A_B_parameter()\n\ndef get_matrix_update_B(B: list, x: list, y:float):\n    return B + x.T *y\n\ndef test_get_matrix_update_B():\n    B_test=np.array([[1,1]])\n    x_test=np.array([[1,1]])\n    y_test=2.0\n    assert np.all(get_matrix_update_B(B_test,x_test,y_test)==np.array([[3.0,3.0]]))\n    return\ntest_get_matrix_update_B()\n\ndef get_matrix_update_B_remove_row(B: list, x: list, y:float):\n    return B - x.T *y\n\ndef get_matrix_update_B_remove_row():\n    B_test=np.array([[1,1]])\n    x_test=np.array([[1,1]])\n    y_test=1.0\n    assert np.all(get_matrix_update_B_remove_row(B_test,x_test,y_test)==np.array([[0.0,0.0]]))\n    return\ntest_get_matrix_update_B()\n\ndef get_prediction(A, B, X):\n    return X@get_least_sqrd_A_B_parameter(A, B)\n\ndef test_get_prediction():\n    x_test = np.array([[1, 0], [1, 1]])\n    y_test = np.array([[1], [2]])\n    A_test = np.linalg.inv(x_test.T @ x_test)\n    B_test = x_test.T @ y_test\n    assert np.all((get_prediction(A_test, B_test, np.array([[1, 2]])))==[[3]])\ntest_get_prediction()\n\n```\n\n\n\n```{python}\ndef rolling_prediction(x: list, y: list, starting_length: int):\n    # Ensure x is 2D array with shape (n_samples, n_features)\n    x_array = np.array(x).reshape(-1, 1) if len(np.array(x).shape) == 1 else np.array(x)\n    y_array = np.array(y).reshape(-1, 1) if len(np.array(y).shape) == 1 else np.array(x)\n    \n    length = len(x_array)\n    prediction = np.zeros(length - starting_length)\n    los_score = np.zeros(length - starting_length)\n\n    # Initialize A and B matrices\n    initial_x = x_array[:starting_length]\n    initial_y = y_array[:starting_length]\n    \n    A = np.linalg.inv(initial_x.T @ initial_x)\n    B = initial_x.T @ initial_y\n\n    # Initial prediction\n    next_x = x_array[starting_length].reshape(1, -1)  \n    prediction[0] = get_prediction(A, B, next_x)\n    los_score[0] = y_array[starting_length] - prediction[0]\n    \n    # Rolling update for predictions and loss scores\n    \n    ind = starting_length + 1\n    current_x = x_array[ind].reshape(1, -1)  \n    for i in range(length - starting_length - 1): \n        ind = starting_length + i\n        current_x = x_array[ind].reshape(1, -1) \n        # Update matrices\n        A = get_inverse_updated_matrix(A, current_x)\n        B = get_matrix_update_B(B, current_x, float(y_array[ind]))\n        # Make prediction for next point\n        next_x = x_array[ind + 1].reshape(1, -1)  \n        prediction[i + 1] = get_prediction(A, B, next_x)\n        los_score[i + 1] = y_array[ind + 1] - prediction[i + 1]\n    return {\"prediction\": prediction, \"los_score\": los_score}\n\n\n```\n\n# Testing the Application\nSince the underlying model is not changing, this provides a straightforward way to test the model.\nThe residuals should closely resemble the errors.\n\nHowever, they will not be identical, as this would require the coefficients to be perfectly accurate.\n\n\n```{python}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#Dianostic plot \nimport matplotlib.pyplot as plt\n\nstart_val=500\nx = np.random.rand(1000)  \nerror=np.random.normal(size=1000)  \ny = 2*x + error\n\n# Run the prediction\nresult = rolling_prediction(x, y, start_val)\n\n\nplt.hist(result['los_score'],alpha=0.5)\n\nplt.hist(error[(start_val):],alpha=0.5)\nplt.legend([\"Residuals\", \"True error\"])\nplt.show()\n\n```\n\n\n# When the Application Fails\nThe reader should note that I have not included an intercept in the data-generating process.\nBelow, I have conducted the same test, but this time I have added a constant to the expression.\n\n\n```{python}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#Dianostic plot \nimport matplotlib.pyplot as plt\n\nstart_val=500\nx = np.random.rand(1000)  \nerror=np.random.normal(size=1000)  \ny =10+ 2*x + error\n\n# Run the prediction\nresult = rolling_prediction(x, y, start_val)\n\n\nplt.hist(result['los_score'],alpha=0.5)\n\nplt.hist(error[(start_val):],alpha=0.5)\nplt.legend([\"Residuals\", \"True error\"])\nplt.show()\n\n```\n\nAs can be see now the aplication fails.\n\nThis can be fixed by adding a column of ones to the model matrix. I would also need to update all the functions to accommodate this change. Without adjusting for this, the dimensions will not match when adding the point to the inverse.\n\n\nI will stop the project here.\nThis was intended to be a small NumPy project. If I were to create functions that allow specifying a model, I might develop it more properly and let the user specify the model through an expression, similar to how linear models are specified in R. Since all model specifications in linear models are based on modifying the model matrix, this could provide flexibility.","srcMarkdownNoYaml":"\n# Introduction\nIn this project, I will explore fitting a running linear model as well as running predictions.\n\nDuring my bachelor's studies, I focused on the prediction of time series. The model I used was fitted using least squares.\nI worked with [FFP3](https://robjhyndman.com/publications/), which is implemented in R.\n\nAt that time, I used the Forecast package but ended up implementing the estimator from the ground up.\nAs I have improved my programming and mathematical skills, I thought it would be fun to revisit this problem and see if I could develop a better solution. \n\nRob how wrote FFP3, has a blog their i sais their is trick to speeding op the calculation of rolling predition for AR process, and i think figured out.\n\n\nlest squres can be written as\n\n$$\\underset{\\beta}{argmin} ||Y-X\\beta||^2$$\n\nSince this is a linear function the problem above i convex with one uniq closed form solution \n\n$\\hat{\\beta}=(X^TX)^{-1}X^TY$\n\n# Running Prediction\nWhen using a running model, a simple solution is to add a new data point and fit the model again.\nThis approach works but is not very efficient.\n\nInstead, let’s consider whether we can update the $\\beta$ estimates directly.\n\n\n\nOne option is the [Recursive least squares filter](https://en.wikipedia.org/wiki/Recursive_least_squares_filter)\n\n\n\nLastly, let’s explore how this can be used.\n\nLet $x[i]$  represent the i-th index of the vector.\n\nFor $XX^T$ a rank-1 update can be expressed as:\n\n$$(X_{new}X_{new}^T)=(X_{old}^TX_{old})+x_{new}^Tx_{new}$$ \n\nThis becomes clear when the product is written out explicitly.\nLook at $(X^TX)[j,g]=x_j^Tx_g$ So $\\sum_{i=1}^{n}x_{j}[i]x_{g}[i]$\n\nNext, consider the first entry in the inner product.\n$(X_{old}X_{old}^T)_{[1,1]}=x_j^Tx_g$ So $\\sum_{i=1}^{n}x_{j}[i]x_{g}[i]$\n\nNow look at.\n\n$(X_{new}X_{new}^T)[j,g]=(\\sum_{i=1}^{n}x_{j}[i]x_{g}[i])+x_{new}[j]x_{new}[g]$\n\nuse [Sherman–Morrison formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula)\n\nFor the update of $(X^TX)$\n\n $$(X_{new}^TX_{new})^{-1}=(X_{old}^TX_{old}+x_{new}^Tx_{new})^{-1}=(X_{old}^TX_{old})^{-1}-\\frac{(X_{old}^TX_{old})^{-1}x_{new}^Tx_{new}(X_{old}^TX_{old})^{-1}}{1+x_{new}(X_{old}^TX_{old})^{-1}x_{new}^T}$$\n\nLet's look at the nex part of the expresion\n\nFor $B=X^TY$ the udpate rule is\n$$B=(X_{new}^TY_{new})=(X_{old}^TY_{old})+x_{new}*y_{new}$$ \n\nnote here that $y_{new}$ act as a scale\n\n\n# Use Cases\nRolling prediction can be used for the evaluation of time series.\n\nHowever, one can also use these techniques for the fast implementation of a sliding window estimator or simply to update a linear model as new data arrives.\n\nI will create two implementations: one where I use this for rolling prediction, and another that implements a sliding window.\n\nFor the sliding window, it is necessary not only to add the new row but also to remove the old one.\n\n\n```{python}\nimport numpy as np\nimport unittest\n\ndef get_rank_updated_matrix(A: list, v: list, u: list):\n    return A + v.T @ u\ndef test_get_rank_updated_matrix():\n    A_test=np.array([[1,0],[0,1]])\n    v_test=np.array([[1,1]])\n    u_test=np.array([[1,1]])\n    assert (np.all(get_rank_updated_matrix(A_test,v_test,u_test)==np.array([[2, 1], [1, 2]])))\n    return\ntest_get_rank_updated_matrix()\n\n\ndef test_get_inverse_updated_matrix_remove_row():\n    X_T_inner_X_inverse_test=np.array([[1,1],[1,1]])\n    x_test=np.array([[1,1]])\n    assert np.all(get_inverse_updated_matrix(X_T_inner_X_inverse_test,x_test)==np.array([[1,1],[1,1]])+4/5)\n    return\n\ndef get_inverse_updated_matrix_remove_row(X_T_inner_X_inverse: list, x: list):\n    return X_T_inner_X_inverse + X_T_inner_X_inverse @ (x.T @ x) @ X_T_inner_X_inverse / (1 + x @ X_T_inner_X_inverse @ x.T)\n\n\ndef get_inverse_updated_matrix(X_T_inner_X_inverse: list, x: list):\n    return X_T_inner_X_inverse - X_T_inner_X_inverse @ (x.T @ x) @ X_T_inner_X_inverse / (1 + x @ X_T_inner_X_inverse @ x.T)\n\ndef test_get_inverse_updated_matrix():\n    X_T_inner_X_inverse_test=np.array([[1,1],[1,1]])+4/5#np.array([[1,1],[1,1]])\n    x_test=np.array([[1,1]])\n    assert np.all(get_inverse_updated_matrix_remove_row(X_T_inner_X_inverse_test,x_test)==np.array([[1,1],[1,1]]))\n    return\n\n\ndef get_least_sqrd_closed_form_parameter(X: list, Y: list):\n    return np.linalg.inv(X.T @ X) @ X.T @ Y\n\ndef test_get_least_sqrd_closed_form_parameter():\n    X_test=np.array([[1,0],[0,1]])\n    Y_test=np.array([[1],[1]])\n    assert np.all((get_least_sqrd_closed_form_parameter(X_test,Y_test))==[[1],[1]])\n    return\ntest_get_least_sqrd_closed_form_parameter()\n\ndef get_least_sqrd_A_B_parameter(A: list, B: list):\n    return A @ B\n\ndef test_get_least_sqrd_A_B_parameter():\n    A_test=np.array([[1,0],[0,1]])\n    B_test=np.array([[1,0],[0,1]])\n    assert np.all(get_least_sqrd_A_B_parameter(A_test,B_test)==[[1,0],[0,1]])\n    return\ntest_get_least_sqrd_A_B_parameter()\n\ndef get_matrix_update_B(B: list, x: list, y:float):\n    return B + x.T *y\n\ndef test_get_matrix_update_B():\n    B_test=np.array([[1,1]])\n    x_test=np.array([[1,1]])\n    y_test=2.0\n    assert np.all(get_matrix_update_B(B_test,x_test,y_test)==np.array([[3.0,3.0]]))\n    return\ntest_get_matrix_update_B()\n\ndef get_matrix_update_B_remove_row(B: list, x: list, y:float):\n    return B - x.T *y\n\ndef get_matrix_update_B_remove_row():\n    B_test=np.array([[1,1]])\n    x_test=np.array([[1,1]])\n    y_test=1.0\n    assert np.all(get_matrix_update_B_remove_row(B_test,x_test,y_test)==np.array([[0.0,0.0]]))\n    return\ntest_get_matrix_update_B()\n\ndef get_prediction(A, B, X):\n    return X@get_least_sqrd_A_B_parameter(A, B)\n\ndef test_get_prediction():\n    x_test = np.array([[1, 0], [1, 1]])\n    y_test = np.array([[1], [2]])\n    A_test = np.linalg.inv(x_test.T @ x_test)\n    B_test = x_test.T @ y_test\n    assert np.all((get_prediction(A_test, B_test, np.array([[1, 2]])))==[[3]])\ntest_get_prediction()\n\n```\n\n\n\n```{python}\ndef rolling_prediction(x: list, y: list, starting_length: int):\n    # Ensure x is 2D array with shape (n_samples, n_features)\n    x_array = np.array(x).reshape(-1, 1) if len(np.array(x).shape) == 1 else np.array(x)\n    y_array = np.array(y).reshape(-1, 1) if len(np.array(y).shape) == 1 else np.array(x)\n    \n    length = len(x_array)\n    prediction = np.zeros(length - starting_length)\n    los_score = np.zeros(length - starting_length)\n\n    # Initialize A and B matrices\n    initial_x = x_array[:starting_length]\n    initial_y = y_array[:starting_length]\n    \n    A = np.linalg.inv(initial_x.T @ initial_x)\n    B = initial_x.T @ initial_y\n\n    # Initial prediction\n    next_x = x_array[starting_length].reshape(1, -1)  \n    prediction[0] = get_prediction(A, B, next_x)\n    los_score[0] = y_array[starting_length] - prediction[0]\n    \n    # Rolling update for predictions and loss scores\n    \n    ind = starting_length + 1\n    current_x = x_array[ind].reshape(1, -1)  \n    for i in range(length - starting_length - 1): \n        ind = starting_length + i\n        current_x = x_array[ind].reshape(1, -1) \n        # Update matrices\n        A = get_inverse_updated_matrix(A, current_x)\n        B = get_matrix_update_B(B, current_x, float(y_array[ind]))\n        # Make prediction for next point\n        next_x = x_array[ind + 1].reshape(1, -1)  \n        prediction[i + 1] = get_prediction(A, B, next_x)\n        los_score[i + 1] = y_array[ind + 1] - prediction[i + 1]\n    return {\"prediction\": prediction, \"los_score\": los_score}\n\n\n```\n\n# Testing the Application\nSince the underlying model is not changing, this provides a straightforward way to test the model.\nThe residuals should closely resemble the errors.\n\nHowever, they will not be identical, as this would require the coefficients to be perfectly accurate.\n\n\n```{python}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#Dianostic plot \nimport matplotlib.pyplot as plt\n\nstart_val=500\nx = np.random.rand(1000)  \nerror=np.random.normal(size=1000)  \ny = 2*x + error\n\n# Run the prediction\nresult = rolling_prediction(x, y, start_val)\n\n\nplt.hist(result['los_score'],alpha=0.5)\n\nplt.hist(error[(start_val):],alpha=0.5)\nplt.legend([\"Residuals\", \"True error\"])\nplt.show()\n\n```\n\n\n# When the Application Fails\nThe reader should note that I have not included an intercept in the data-generating process.\nBelow, I have conducted the same test, but this time I have added a constant to the expression.\n\n\n```{python}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#Dianostic plot \nimport matplotlib.pyplot as plt\n\nstart_val=500\nx = np.random.rand(1000)  \nerror=np.random.normal(size=1000)  \ny =10+ 2*x + error\n\n# Run the prediction\nresult = rolling_prediction(x, y, start_val)\n\n\nplt.hist(result['los_score'],alpha=0.5)\n\nplt.hist(error[(start_val):],alpha=0.5)\nplt.legend([\"Residuals\", \"True error\"])\nplt.show()\n\n```\n\nAs can be see now the aplication fails.\n\nThis can be fixed by adding a column of ones to the model matrix. I would also need to update all the functions to accommodate this change. Without adjusting for this, the dimensions will not match when adding the point to the inverse.\n\n\nI will stop the project here.\nThis was intended to be a small NumPy project. If I were to create functions that allow specifying a model, I might develop it more properly and let the user specify the model through an expression, similar to how linear models are specified in R. Since all model specifications in linear models are based on modifying the model matrix, this could provide flexibility."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"Running least squares.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","title":"Update rules for least squares"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}