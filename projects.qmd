---
title: "Projects"
---

After completing my education, I have been working independently on some small projects.
I undertake these projects to learn different things and to establish a workflow during the process. This also means that these projects don't need to be perfect—the goal is learning.

I share them so people can get a glimpse of what I'm working on, but also because I often explore others' projects myself to discover small tricks that are rarely shown in articles and are often hidden in the code.

Here are some of my projects:

# Bee a Spelling Genius

[Bee a Spelling Genius](applications.qmd)

This is a live application made with the Shiny for Python version. 

The intention was to create a small project and learn about reactive programming, as well as how to develop small applications and dashboards. By using Shiny for Python, I could run it directly on my GitHub page, making it easier to share with others.

# Mixture of K scaled and shifted t distributions

[Mixture of K scaled and shifted t distributions](Projects/Mixture of K scaled and shifted t-distributions.qmd)

The implementation was created using a combination of R and C++ via the Rcpp library. 

This project is based on an assignment from one of my classes at KU. I enjoyed it because it had a nice mixture of theoretical calculations, numerical methods, and coding. When I first worked on it, I encountered many problems due to the steep gradient. It was rewarding to revisit it and learn how to handle these challenges. I also got to use C++ and see how it improved computation time.

This was one of those projects where the final product was very clean, but the process involved a lot of troubleshooting and learning about program interactions, which was a lot of fun.

For example, I used C++ overloaded functions, but due to R not allowing functions with the same name, I ran into issues. When I tested my R implementation with a set output, it worked, and when I tested my C++ implementation, it also worked. It’s easy to understand once you figure it out, but it was challenging until then.

A small note: this project is a bit technical. If you've stumbled upon it and want some intuition about what's going on, I recommend starting by looking at [Gaussian mixtures](https://stephens999.github.io/fiveMinuteStats/intro_to_em.html). Since Gaussian mixtures have closed-form solutions, the expressions are much simpler, which makes understanding the intuition easier.

For t-distributions, there is no closed form—or at least I couldn’t derive one—so I had to use numerical methods to compensate. This is why the project ends up focusing heavily on implementing fast code using C++. It also involves addressing numerical issues such as unstable gradients and explaining why the function is not convex. If it were convex, I would have used Newton’s method.

This challenge was also what made the project interesting to me. Anyone can fit a Gaussian mixture, which is why it’s so widely used. However, tackling numerical methods and figuring out how to implement solutions when the expressions don’t cooperate is a central skill I wanted to develop.


# VP-search trees for cgMLST sequences

[VP-search trees for cgMLST sequences](Projects/VP_search_trees_for_cgMLST_sequences.html)

This is a project I completed for Statens Serum Institut. The goal was to develop a theoretical approach for more efficient searches in their database for cgMLST analyses. I ended up creating an adaptation of VP-search trees based on Hamming distance. Due to the significant number of missing values in the sequences, I had to modify the method and prove some minor bounds. The underlying problem was how to optimize repeated in-range searches, which are, by nature, very similar to KNN.

I implemented this in Python, making it usable for their system.

The department had a database with the data but conducted data processing in Python. Therefore, it might be more efficient to write code that worked closer to the database. Consequently, the report structure became a bit unconventional.

I wrote extensive documentation for all functions and implemented everything from scratch to ensure it could be adapted to different programming languages where Python libraries might not be flexible.

I also conducted speed testing on a smaller, published dataset to showcase how the implementation could be evaluated.

# Prediction of House Prices

[Prediction of House Prices](Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.qmd)

This is a small project I did because I wanted to learn XG-Boost, Lasso and work on a practical prediction project. [Kaggle](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques) has an open dataset where one can practice. It turns out that their version of advanced regression techniques boils down to a crazy amount of data cleaning and feature engineering, which is common in real-world problems, but not necessarily needed for a practice case. This is a common practice project, and various versions can be found online. I did some visualizations a bit differently, as well as hierarchical clustering to reduce variables.

The feature engineering and cleaning process is extensive but could be improved. Since this was not the main goal of my project. I achieved a score of 0.13755, which was considered a rather good score before people started inflating the scores by exploiting the dataset, which has been used multiple times, and feeding it into the validation set.

I made this notebook using R and created a small interface to interact with the plot. Otherwise, the notebook would have too many plots of variables. 

# sequential hypothesis testing and safe anytime inference

[Sequential Hypothesis Testing and Safe Anytime Inference](Projects/sequential-hypothesis-testing and-safe-anytime-inference.qmd)

In this project, I explore the mSPRT, which is a method for continuously monitoring experiments. It's a kind of "the new black" in A/B testing.
The theory is beautifully simple but has been significantly expanded to account for additional problems.
In this project, I focus only on the base problem.
I also conducted simulations to see how these assumptions affect coverage, especially the Type I error.
Additionally, I rembered how to perform power calculations.


# A Toy Model for Market Dynamics: Building a Agent-Based Frameworks

[A Toy Model for Market Dynamics: Building a Agent-Based Frameworks](Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.qmd)

This project explores the foundational concepts of agent-based simulations through a simplified market model. Using Python, it simulates shoppers navigating a fictional representation of Strøget, Copenhagen’s shopping district. Each shopper is modeled as an agent with specific preferences, budget constraints, and behaviors, while shops act as agents offering products in distinct categories.

The project emphasizes structural design choices, such as implementing interactions (e.g., buying decisions) and tracking aggregated metrics like total sales. It also addresses technical challenges, including managing dynamic effects and considering potential parallelization issues, though these aspects have not yet been fully implemented.

The implementation is written in Python and includes unit tests to verify the functionality of key components. Primarily intended as a learning exercise, this model aims to refine object-oriented programming skills and simulate agent interactions. It serves as a foundation for exploring more complex systems and optimizing efficiency in future iterations.
