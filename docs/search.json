[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Purpose",
    "section": "",
    "text": "This website serves as a platform to showcase the projects I’ve worked on. My hope is that they will contribute to achieving a Pareto optimal solution, in the sense that they may be of use to others and is free for me to share."
  },
  {
    "objectID": "index.html#languages",
    "href": "index.html#languages",
    "title": "Purpose",
    "section": "Languages",
    "text": "Languages\nI am native Danish, so it is possible that some of the content will be in Danish.\nThis also means that the English may not always be flawless"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "After completing my education, I have been working independently on some small projects. I undertake these projects to learn different things and to establish a workflow during the process. This also means that these projects don’t need to be perfect—the goal is learning.\nI share them so people can get a glimpse of what I’m working on, but also because I often explore others’ projects myself to discover small tricks that are rarely shown in articles and are often hidden in the code.\nHere are some of my projects:\n\nBee a Spelling Genius (Dashboard)\nBee a Spelling Genius\nThis is a live application made with the Shiny for Python version.\nThe intention was to create a small project and learn about reactive programming, as well as how to develop small applications and dashboards. By using Shiny for Python, I could run it directly on my GitHub page, making it easier to share with others.\n\n\nMixture of K scaled and shifted t distributions (Theoretical and Implementation)\nMixture of K scaled and shifted t distributions\nThe implementation was created using a combination of R and C++ via the Rcpp library.\nThis project is based on an assignment from one of my classes at KU. I enjoyed it because it had a nice mixture of theoretical calculations, numerical methods, and coding. When I first worked on it, I encountered many problems due to the steep gradient. It was rewarding to revisit it and learn how to handle these challenges. I also got to use C++ and see how it improved computation time.\nThis was one of those projects where the final product was very clean, but the process involved a lot of troubleshooting and learning about program interactions, which was a lot of fun.\nFor example, I used C++ overloaded functions, but due to R not allowing functions with the same name, I ran into issues. When I tested my R implementation with a set output, it worked, and when I tested my C++ implementation, it also worked. It’s easy to understand once you figure it out, but it was challenging until then.\nA small note: this project is a bit technical. If you’ve stumbled upon it and want some intuition about what’s going on, I recommend starting by looking at Gaussian mixtures. Since Gaussian mixtures have closed-form solutions, the expressions are much simpler, which makes understanding the intuition easier.\nFor t-distributions, there is no closed form—or at least I couldn’t derive one—so I had to use numerical methods to compensate. This is why the project ends up focusing heavily on implementing fast code using C++. It also involves addressing numerical issues such as unstable gradients and explaining why the function is not convex. If it were convex, I would have used Newton’s method.\nThis challenge was also what made the project interesting to me. Anyone can fit a Gaussian mixture, which is why it’s so widely used. However, tackling numerical methods and figuring out how to implement solutions when the expressions don’t cooperate is a central skill I wanted to develop.\n\n\nVP-search trees for cgMLST sequences (Theoretical and Implementation)\nVP-search trees for cgMLST sequences\nThis is a project I completed for Statens Serum Institut. The goal was to develop a theoretical approach for more efficient searches in their database for cgMLST analyses. I ended up creating an adaptation of VP-search trees based on Hamming distance. Due to the significant number of missing values in the sequences, I had to modify the method and prove some minor bounds. The underlying problem was how to optimize repeated in-range searches, which are, by nature, very similar to KNN.\nI implemented this in Python, making it usable for their system.\nThe department had a database with the data but conducted data processing in Python. Therefore, it might be more efficient to write code that worked closer to the database. Consequently, the report structure became a bit unconventional.\nI wrote extensive documentation for all functions and implemented everything from scratch to ensure it could be adapted to different programming languages where Python libraries might not be flexible.\nI also conducted speed testing on a smaller, published dataset to showcase how the implementation could be evaluated.\n\n\nPrediction of House Prices (Applied Project)\nPrediction of House Prices\nThis is a small project I did because I wanted to learn XG-Boost, Lasso and work on a practical prediction project. Kaggle has an open dataset where one can practice. It turns out that their version of advanced regression techniques boils down to a crazy amount of data cleaning and feature engineering, which is common in real-world problems, but not necessarily needed for a practice case. This is a common practice project, and various versions can be found online. I did some visualizations a bit differently, as well as hierarchical clustering to reduce variables.\nThe feature engineering and cleaning process is extensive but could be improved. Since this was not the main goal of my project. I achieved a score of 0.13755, which was considered a rather good score before people started inflating the scores by exploiting the dataset, which has been used multiple times, and feeding it into the validation set.\nI made this notebook using R and created a small interface to interact with the plot. Otherwise, the notebook would have too many plots of variables.\n\n\nSequential Hypothesis Testing and Safe Anytime Inference (Theoretical and Simulation)\nSequential Hypothesis Testing and Safe Anytime Inference\nIn this project, I explore the mSPRT, which is a method for continuously monitoring experiments. It’s a kind of “the new black” in A/B testing. The theory is beautifully simple but has been significantly expanded to account for additional problems. In this project, I focus only on the base problem. I also conducted simulations to see how these assumptions affect coverage, especially the Type I error. Additionally, I rembered how to perform power calculations.\n\n\nUpdate rules for least squares (Theoretical/Conceptual)\nUpdate rules for least squares\nThis was a small project to practice some linear algebra and NumPy, as well as Matplotlib. I implemented a rolling prediction for a simple linear model fitted with least squares and used update rules so that the function doesn’t need to be refitted from scratch. Least squares is often used, so this method can be applied to many other applications in live settings, allowing the model to be updated as data flows in.\n\n\nA Toy Model for Market Dynamics: Building a Agent-Based Frameworks (Conceptual)\nA Toy Model for Market Dynamics: Building a Agent-Based Frameworks\nThis project explores the foundational concepts of agent-based simulations through a simplified market model. Using Python, it simulates shoppers navigating a fictional representation of Strøget, Copenhagen’s shopping district. Each shopper is modeled as an agent with specific preferences, budget constraints, and behaviors, while shops act as agents offering products in distinct categories.\nThe project emphasizes structural design choices, such as implementing interactions (e.g., buying decisions) and tracking aggregated metrics like total sales. It also addresses technical challenges, including managing dynamic effects and considering potential parallelization issues, though these aspects have not yet been fully implemented.\nThe implementation is written in Python and includes unit tests to verify the functionality of key components. Primarily intended as a learning exercise, this model aims to refine object-oriented programming skills and simulate agent interactions. It serves as a foundation for exploring more complex systems and optimizing efficiency in future iterations."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m interested in the topics of economics, machine learning(prediction), statistics(inference), and coding."
  },
  {
    "objectID": "about.html#my-background",
    "href": "about.html#my-background",
    "title": "About",
    "section": "My background",
    "text": "My background\nMaster of Science (statistics)\nBachelor math economics\nBoth from University of Copenhagen(KU)"
  },
  {
    "objectID": "Bee_a_Spelling_Guinness.html",
    "href": "Bee_a_Spelling_Guinness.html",
    "title": "Projects",
    "section": "",
    "text": "Here are some of my projects:\n\nMixture of K scaled and shifted t distributions\n\nMixture of K scaled and shifted t distributions\n\nThe implementation is made using a combination of R and C++ via the Rcpp library.\n\n\nVP-search trees for cgMLST sequences\n\nVP-search trees for cgMLST sequences\n\nThe implementation is made in python and the report is written in jupyter notebooks."
  },
  {
    "objectID": "docs/projects.html",
    "href": "docs/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here are some of my projects:\n\nMixture of K scaled and shifted t distributions\n\nMixture of K scaled and shifted t distributions\n\nThe implementation is made using a combination of R and C++ via the Rcpp library.\n\n\nVP-search trees for cgMLST sequences\n\nVP-search trees for cgMLST sequences\n\nThe implementation is made in python and the report is written in jupyter notebooks."
  },
  {
    "objectID": "aplications.html",
    "href": "aplications.html",
    "title": "Applications",
    "section": "",
    "text": "For now, this site contains only one application built with Shiny. I may extend it in the future if I create more."
  },
  {
    "objectID": "aplications.html#about",
    "href": "aplications.html#about",
    "title": "Applications",
    "section": "About",
    "text": "About\nThis is a interactive app made to, help/improve player’s of New York times Spelling Bee.\nIn the spelling bee, the player is given seven letters. One letter is marked in yellow.\nThe goal is to make as many words as possible from the seven letters. Every word should contain the yellow letter.\nScoring\n\nA word of 4 letters earns one point.\nWords longer than 4 letters earn additional points (the exact amount is to be determined).\n\nThe dictionary used is provided Here under an open license.\nThe Spelling Bee uses a list of curated words. The dictionary is more like a reference for how words are spelled. For example, “AAAA” might appear in the dictionary but is unlikely to be included in the Spelling Bee.\nThere may also be words that are offensive or outdated. While they are correctly spelled, they probably won’t appear in the Spelling Bee."
  },
  {
    "objectID": "aplications.html#version-with-the-code",
    "href": "aplications.html#version-with-the-code",
    "title": "Applications",
    "section": "Version with the code",
    "text": "Version with the code\nHere is a version where the code can be seen\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n#| components: [editor, viewer]\n\nfrom shiny import *\n# For plot\nimport matplotlib.pyplot as plt\n# Get all words in the english diconary\nimport numpy as np\nimport requests\n# URL of the text file on GitHub\nurl = \"https://raw.githubusercontent.com/dwyl/english-words/refs/heads/master/words.txt\"\n\n# Fetch the file content\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    # Read the content of the file\n    file_content = response.text\n    # Split the content into a list of words\n    words_list = file_content.splitlines()\n    print(words_list)\nelse:\n    print(\"Failed to retrieve the file:\", response.status_code)\n\ndef check_word_canidat_early_stopping(\n        word:str,\n        yellow_chr:chr,\n        chr_list:list):\n    \"\"\"\n    This function return True ore False.\n    To be valid candidat a get true retrun, the input word\n    need to contain the yellow chr, and can not contain other chr that yellow and chr from the list of charater list\n    \n    Args:\n        word (str): The word that need to be ckecked.\n        yellow_chr (chr): a chr that all word must contain\n        chr_list (list): a list of chr that the word can be contain in.\n    Returns:\n        bool: True ore False based on if the word forfiels the conditions\n   \n    \"\"\"\n    lover_yellow =yellow_chr.lower()\n    return_obj=False\n    is_word_combination_of_chr_list=False\n    is_yellow_chr_contained= False\n    lower_chars = [char.lower() for char in chr_list]\n    \n    for letter in word:\n        is_letter_contained= letter.lower() in lower_chars\n        if (is_letter_contained==False):\n            is_word_combination_of_chr_list=False\n            break\n        else:\n            is_word_combination_of_chr_list=True\n        if (is_yellow_chr_contained==False):\n            is_yellow_chr_contained=(lover_yellow==letter.lower())\n    \n    if(is_word_combination_of_chr_list==True and is_yellow_chr_contained==True):\n        return_obj =True\n    return return_obj\n\n\n\ndef get_word_candidates(\n        word_list:list,\n        yellow_chr: chr,\n        chr_list:list,\n        min_word_length:int = 4):\n    \"\"\"\n    This returns list containg alle candidat words, for the spelling bee game.\n\n    All word contain the yellow letter, and consist of combination yello and letter from the letter list.\n    All words is longer ore equal to min_word_length\n    \n    Args:\n        word_list (list): a list with string containg words to serach though\n        yellow_chr (chr): all word in return should contain this letter\n        chr_list (list): a list of chr that can be conatin a word in return\n    Returns:\n        list: a list of strings. all word should be combination of yellow chr and letter from the list.\n        \n    \"\"\"\n    return_objet = []\n    index_list=[] #this list contais the index of wich words is potensiel words in the index list.\n    allowed_chartes= chr_list + [yellow_chr]\n    # len word is not calculated but store in the type string so it fast to check if len of the word is les than min first\n    for i in range(len(word_list)):\n        word_for_examination=word_list[i]\n        if(len(word_for_examination)&gt;=min_word_length):\n            if(check_word_canidat_early_stopping(word_for_examination,yellow_chr,allowed_chartes)==True):\n                return_objet.append(word_for_examination)\n                index_list.append(i)\n    return(return_objet)\n\n\n# Generat plots\n\ndef get_len_word_list(words_list:int):\n    list_len=[]\n    for word in words_list: list_len.append(len(word))\n    return list_len\n\n\n\n\n# Define the app UI\napp_ui = ui.page_fluid(\n    ui.input_selectize(\"yellow_chr\", \"Choose one yellow letter\", \n                       [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"], \n                       multiple=False),\n    ui.output_text_verbatim(\"yellow_chr_output\"),\n    ui.input_selectize(\"grey_chr\", \"Choose six grey letters\", \n                       [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"], \n                       multiple=True),\n    ui.output_text_verbatim(\"grey_chr_output\"),\n    ui.output_text_verbatim(\"len_data_txt\"),\n    ui.output_plot(\"plot\"),\n    ui.output_ui(\"output_content\"),\n    ui.input_switch(\"switch\", \"Show word results\", False),  \n    ui.output_text_verbatim(\"word_list_output\")\n)\n\n# Define the app server logic\ndef server(input, output, session):\n    @reactive.Calc\n    def dataset():\n        # This function returns a dataset when 6 grey letters are selected\n        if len(input.grey_chr()) == 6:\n            return get_word_candidates(words_list, input.yellow_chr(),list(input.grey_chr()))\n        else:\n            return []\n    @reactive.Calc\n    def len_data():\n        len_data=get_len_word_list(dataset())\n        return len_data\n        \n    @output\n    @render.text\n    def yellow_chr_output():\n        return f\"The chosen yellow letter is {input.yellow_chr()}\"\n    @output\n    @render.text\n    def grey_chr_output():\n        if len(input.grey_chr()) != 6:\n            return f\"You need to choose 6 grey letters; currently, you have chosen {len(input.grey_chr())}.\"\n        else:\n            return \"You have chosen 6 grey letters.\"\n    @output\n    @render.text\n    def len_data_txt():\n        ret_ob=\"The data has not bin computed\"\n        if(len(input.grey_chr())==6):\n            ret_ob=f\"There are {len(dataset())} possible words with 4 or more letters\"\n        return ret_ob\n    @output\n    @render.plot(alt=\"A histogram of dataset values\")\n    def plot():    \n        fig, ax = plt.subplots()\n        if len(input.grey_chr()) == 6:\n            unique_values, counts = np.unique(len_data(), return_counts=True)\n            ax.bar(unique_values, counts, color='skyblue', edgecolor='black')\n            ax.set_title(\"Histogram of Word Candidates\")\n            ax.set_xlabel(\"Length of word\")\n            ax.set_ylabel(\"Count\")\n        else:\n            ax.text(0.5, 0.5, \"Please select 6 grey letters\", horizontalalignment='center', verticalalignment='center')\n        return fig\n    @output\n    @render.text\n    def word_list_output():\n        ret_ob=\"\"\n        if(input.switch()==True):\n            ret_ob=\"\\n\".join(dataset())\n        return ret_ob\n        \n    \n\napp = App(app_ui, server)"
  },
  {
    "objectID": "aplications.html#made-with-shiny-for-python-shiny-live",
    "href": "aplications.html#made-with-shiny-for-python-shiny-live",
    "title": "Applications",
    "section": "Made with Shiny for Python (Shiny Live)",
    "text": "Made with Shiny for Python (Shiny Live)\nThis application is created using Shiny for Python (Shiny Live). There is also an R version available.\nWhat makes the Shiny Live version so appealing is that it allows embedding Python code into WebAssembly. This application runs Python directly from your personal computer, with the dictionary downloaded from a GitHub link.\nThe library support is somewhat limited.\nFor small projects that should be accessible to the public, I like Shiny Live because it doesn’t require hosting a server for the backend.\nEstially the backend runs on you pc.\nTheir is som drawback, every aplication need the data to load in seperatly. Alle code and data is exeable for the user, documentasion is spars. It hard to debug, since it reaktiv programing."
  },
  {
    "objectID": "applications.html",
    "href": "applications.html",
    "title": "Applications",
    "section": "",
    "text": "For now, this site contains only one application built with Shiny. I may extend it in the future if I create more."
  },
  {
    "objectID": "applications.html#about",
    "href": "applications.html#about",
    "title": "Applications",
    "section": "About",
    "text": "About\nThis is a interactive app made to, help/improve player’s of New York times Spelling Bee.\nIn the spelling bee, the player is given seven letters. One letter is marked in yellow.\nThe goal is to make as many words as possible from the seven letters. Every word should contain the yellow letter.\nScoring\n\nA word of 4 letters earns one point.\nWords longer than 4 letters earn additional points (the exact amount is to be determined).\n\nThe dictionary used is provided Here under an open license.\nThe Spelling Bee uses a list of curated words. The dictionary is more like a reference for how words are spelled. For example, “AAAA” might appear in the dictionary but is unlikely to be included in the Spelling Bee.\nThere may also be words that are offensive or outdated. While they are correctly spelled, they probably won’t appear in the Spelling Bee.\nThe application can take a little while to load. The data is downloaded from a GitHub repository, so if you have browser extensions that might prevent this, it’s a good idea to temporarily turn them off.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n\n\nfrom shiny import *\n# For plot\nimport matplotlib.pyplot as plt\n# Get all words in the english diconary\nimport numpy as np\nimport requests\n# URL of the text file on GitHub\nurl = \"https://raw.githubusercontent.com/dwyl/english-words/refs/heads/master/words.txt\"\n\n# Fetch the file content\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    # Read the content of the file\n    file_content = response.text\n    # Split the content into a list of words\n    words_list = file_content.splitlines()\n    print(words_list)\nelse:\n    print(\"Failed to retrieve the file:\", response.status_code)\n\ndef check_word_canidat_early_stopping(\n        word:str,\n        yellow_chr:chr,\n        chr_list:list):\n    \"\"\"\n    This function returns True or False.\n    To be a valid candidate for a True return, the input word \n    must contain the yellow character and cannot contain any characters \n    other than the yellow character and characters from the provided character list.\n    Args:\n        word (str): The word to be checked.\n        yellow_chr (str): A character that all valid words must contain.\n        chr_list (list): A list of characters that the word is allowed to contain.\n    Returns:\n        bool: True or False depending on whether the word fulfills the conditions.\n\n    \"\"\"\n    lover_yellow =yellow_chr.lower()\n    return_obj=False\n    is_word_combination_of_chr_list=False\n    is_yellow_chr_contained= False\n    lower_chars = [char.lower() for char in chr_list]\n    \n    for letter in word:\n        is_letter_contained= letter.lower() in lower_chars\n        if (is_letter_contained==False):\n            is_word_combination_of_chr_list=False\n            break\n        else:\n            is_word_combination_of_chr_list=True\n        if (is_yellow_chr_contained==False):\n            is_yellow_chr_contained=(lover_yellow==letter.lower())\n    \n    if(is_word_combination_of_chr_list==True and is_yellow_chr_contained==True):\n        return_obj =True\n    return return_obj\n\n\n\ndef get_word_candidates(\n        word_list:list,\n        yellow_chr: chr,\n        chr_list:list,\n        min_word_length:int = 4):\n    \"\"\"\n    This returns list containg alle candidat words, for the spelling bee game.\n\n    All word contain the yellow letter, and consist of combination yello and letter from the letter list.\n    All words is longer ore equal to min_word_length\n    \n    Args:\n        word_list (list): a list with string containg words to serach though\n        yellow_chr (chr): all word in return should contain this letter\n        chr_list (list): a list of chr that can be conatin a word in return\n    Returns:\n        list: a list of strings. all word should be combination of yellow chr and letter from the list.\n        \n    \"\"\"\n    return_objet = []\n    index_list=[] #this list contais the index of wich words is potensiel words in the index list.\n    allowed_chartes= chr_list + [yellow_chr]\n    # len word is not calculated but store in the type string so it fast to check if len of the word is les than min first\n    for i in range(len(word_list)):\n        word_for_examination=word_list[i]\n        if(len(word_for_examination)&gt;=min_word_length):\n            if(check_word_canidat_early_stopping(word_for_examination,yellow_chr,allowed_chartes)==True):\n                return_objet.append(word_for_examination)\n                index_list.append(i)\n    return(return_objet)\n\n\n# Generat plots\n\ndef get_len_word_list(words_list:int):\n    list_len=[]\n    for word in words_list: list_len.append(len(word))\n    return list_len\n\n\n\n\n# Define the app UI\napp_ui = ui.page_fluid(\n    ui.input_selectize(\"yellow_chr\", \"Choose one yellow letter\", \n                       [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"], \n                       multiple=False),\n    ui.output_text_verbatim(\"yellow_chr_output\"),\n    ui.input_selectize(\"grey_chr\", \"Choose six grey letters\", \n                       [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"], \n                       multiple=True),\n    ui.output_text_verbatim(\"grey_chr_output\"),\n    ui.output_text_verbatim(\"len_data_txt\"),\n    ui.output_plot(\"plot\"),\n    ui.output_ui(\"output_content\"),\n    ui.input_switch(\"switch\", \"Show word results\", False),  \n    ui.output_text_verbatim(\"word_list_output\")\n)\n\n# Define the app server logic\ndef server(input, output, session):\n    @reactive.Calc\n    def dataset():\n        # This function returns a dataset when 6 grey letters are selected\n        if len(input.grey_chr()) == 6:\n            return get_word_candidates(words_list, input.yellow_chr(),list(input.grey_chr()))\n        else:\n            return []\n    @reactive.Calc\n    def len_data():\n        len_data=get_len_word_list(dataset())\n        return len_data\n        \n    @output\n    @render.text\n    def yellow_chr_output():\n        return f\"The chosen yellow letter is {input.yellow_chr()}\"\n    @output\n    @render.text\n    def grey_chr_output():\n        if len(input.grey_chr()) != 6:\n            return f\"You need to choose 6 grey letters; currently, you have chosen {len(input.grey_chr())}.\"\n        else:\n            return \"You have chosen 6 grey letters.\"\n    @output\n    @render.text\n    def len_data_txt():\n        ret_ob=\"The data has not bin computed\"\n        if(len(input.grey_chr())==6):\n            ret_ob=f\"There are {len(dataset())} possible words with 4 or more letters\"\n        return ret_ob\n    @output\n    @render.plot(alt=\"A histogram of dataset values\")\n    def plot():    \n        fig, ax = plt.subplots()\n        if len(input.grey_chr()) == 6:\n            unique_values, counts = np.unique(len_data(), return_counts=True)\n            ax.bar(unique_values, counts, color='skyblue', edgecolor='black')\n            ax.set_title(\"Histogram of Word Candidates\")\n            ax.set_xlabel(\"Length of word\")\n            ax.set_ylabel(\"Count\")\n        else:\n            ax.text(0.5, 0.5, \"Please select 6 grey letters\", horizontalalignment='center', verticalalignment='center')\n        return fig\n    @output\n    @render.text\n    def word_list_output():\n        ret_ob=\"\"\n        if(input.switch()==True):\n            ret_ob=\"\\n\".join(dataset())\n        return ret_ob\n        \n    \n\napp = App(app_ui, server)"
  },
  {
    "objectID": "applications.html#version-with-the-code",
    "href": "applications.html#version-with-the-code",
    "title": "Applications",
    "section": "Version with the code",
    "text": "Version with the code\nHere is a version where the code can be seen\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n#| components: [editor, viewer]\n\nfrom shiny import *\n# For plot\nimport matplotlib.pyplot as plt\n# Get all words in the english diconary\nimport numpy as np\nimport requests\n# URL of the text file on GitHub\nurl = \"https://raw.githubusercontent.com/dwyl/english-words/refs/heads/master/words.txt\"\n\n# Fetch the file content\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    # Read the content of the file\n    file_content = response.text\n    # Split the content into a list of words\n    words_list = file_content.splitlines()\n    print(words_list)\nelse:\n    print(\"Failed to retrieve the file:\", response.status_code)\n\ndef check_word_canidat_early_stopping(\n        word:str,\n        yellow_chr:chr,\n        chr_list:list):\n    \"\"\"\n    This function returns True or False.\n    To be a valid candidate for a True return, the input word \n    must contain the yellow character and cannot contain any characters \n    other than the yellow character and characters from the provided character list.\n    Args:\n        word (str): The word to be checked.\n        yellow_chr (str): A character that all valid words must contain.\n        chr_list (list): A list of characters that the word is allowed to contain.\n    Returns:\n        bool: True or False depending on whether the word fulfills the conditions.\n\n    \"\"\"\n    lover_yellow =yellow_chr.lower()\n    return_obj=False\n    is_word_combination_of_chr_list=False\n    is_yellow_chr_contained= False\n    lower_chars = [char.lower() for char in chr_list]\n    \n    for letter in word:\n        is_letter_contained= letter.lower() in lower_chars\n        if (is_letter_contained==False):\n            is_word_combination_of_chr_list=False\n            break\n        else:\n            is_word_combination_of_chr_list=True\n        if (is_yellow_chr_contained==False):\n            is_yellow_chr_contained=(lover_yellow==letter.lower())\n    \n    if(is_word_combination_of_chr_list==True and is_yellow_chr_contained==True):\n        return_obj =True\n    return return_obj\n\n\n\ndef get_word_candidates(\n        word_list:list,\n        yellow_chr: chr,\n        chr_list:list,\n        min_word_length:int = 4):\n    \"\"\"\n    This returns list containg alle candidat words, for the spelling bee game.\n\n    All word contain the yellow letter, and consist of combination yellow and letter from the letter list.\n    All words is longer ore equal to min_word_length\n    \n    Args:\n        word_list (list): a list with string containg words to serach though\n        yellow_chr (chr): all word in return should contain this letter\n        chr_list (list): a list of chr that can be conatin a word in return\n    Returns:\n        list: a list of strings. all word should be combination of yellow chr and letter from the list.\n        \n    \"\"\"\n    return_objet = []\n    index_list=[] #this list contais the index of wich words is potensiel words in the index list.\n    allowed_chartes= chr_list + [yellow_chr]\n    # len word is not calculated but store in the type string so it fast to check if len of the word is les than min first\n    for i in range(len(word_list)):\n        word_for_examination=word_list[i]\n        if(len(word_for_examination)&gt;=min_word_length):\n            if(check_word_canidat_early_stopping(word_for_examination,yellow_chr,allowed_chartes)==True):\n                return_objet.append(word_for_examination)\n                index_list.append(i)\n    return(return_objet)\n\n\n# Generat plots\n\ndef get_len_word_list(words_list:int):\n    list_len=[]\n    for word in words_list: list_len.append(len(word))\n    return list_len\n\n\n\n\n# Define the app UI\napp_ui = ui.page_fluid(\n    ui.input_selectize(\"yellow_chr\", \"Choose one yellow letter\", \n                       [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"], \n                       multiple=False),\n    ui.output_text_verbatim(\"yellow_chr_output\"),\n    ui.input_selectize(\"grey_chr\", \"Choose six grey letters\", \n                       [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"], \n                       multiple=True),\n    ui.output_text_verbatim(\"grey_chr_output\"),\n    ui.output_text_verbatim(\"len_data_txt\"),\n    ui.output_plot(\"plot\"),\n    ui.output_ui(\"output_content\"),\n    ui.input_switch(\"switch\", \"Show word results\", False),  \n    ui.output_text_verbatim(\"word_list_output\")\n)\n\n# Define the app server logic\ndef server(input, output, session):\n    @reactive.Calc\n    def dataset():\n        # This function returns a dataset when 6 grey letters are selected\n        if len(input.grey_chr()) == 6:\n            return get_word_candidates(words_list, input.yellow_chr(),list(input.grey_chr()))\n        else:\n            return []\n    @reactive.Calc\n    def len_data():\n        len_data=get_len_word_list(dataset())\n        return len_data\n        \n    @output\n    @render.text\n    def yellow_chr_output():\n        return f\"The chosen yellow letter is {input.yellow_chr()}\"\n    @output\n    @render.text\n    def grey_chr_output():\n        if len(input.grey_chr()) != 6:\n            return f\"You need to choose 6 grey letters; currently, you have chosen {len(input.grey_chr())}.\"\n        else:\n            return \"You have chosen 6 grey letters.\"\n    @output\n    @render.text\n    def len_data_txt():\n        ret_ob=\"The data has not bin computed\"\n        if(len(input.grey_chr())==6):\n            ret_ob=f\"There are {len(dataset())} possible words with 4 or more letters\"\n        return ret_ob\n    @output\n    @render.plot(alt=\"A histogram of dataset values\")\n    def plot():    \n        fig, ax = plt.subplots()\n        if len(input.grey_chr()) == 6:\n            unique_values, counts = np.unique(len_data(), return_counts=True)\n            ax.bar(unique_values, counts, color='skyblue', edgecolor='black')\n            ax.set_title(\"Histogram of Word Candidates\")\n            ax.set_xlabel(\"Length of word\")\n            ax.set_ylabel(\"Count\")\n        else:\n            ax.text(0.5, 0.5, \"Please select 6 grey letters\", horizontalalignment='center', verticalalignment='center')\n        return fig\n    @output\n    @render.text\n    def word_list_output():\n        ret_ob=\"\"\n        if(input.switch()==True):\n            ret_ob=\"\\n\".join(dataset())\n        return ret_ob\n        \n    \n\napp = App(app_ui, server)"
  },
  {
    "objectID": "applications.html#made-with-shiny-for-python-shiny-live",
    "href": "applications.html#made-with-shiny-for-python-shiny-live",
    "title": "Applications",
    "section": "Made with Shiny for Python (Shiny Live)",
    "text": "Made with Shiny for Python (Shiny Live)\nThis application is created using Shiny for Python (Shiny Live). There is also an R version available.\nWhat makes the Shiny Live version so appealing is that it allows embedding Python code into WebAssembly. This application runs Python directly from your personal computer, with the dictionary downloaded from a GitHub link.\nThe library support is somewhat limited.\nFor small projects that should be accessible to the public, I like Shiny Live because it doesn’t require hosting a server for the backend.\nEstially the backend runs on you pc.\nTheir is som drawback, every aplication need the data to load in seperatly. Alle code and data is exeable for the user, documentasion is spars. It hard to debug, since it reaktiv programing.\nBut overall, being able to create applications without needing to host a server is pretty cool."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "",
    "text": "I am applying for a job where the primary task involves working on and implementing agent-based simulations. It has been a while since I last worked with object-oriented coding, so I thought I would brush up on my basic skills by creating a small project based on a simple market simulation.\nMy background is primarily in mathematics, economics, and statistics. At the computer science department at KU, there don’t seem to be many courses that focus on agent-based simulation approaches. However, I recall attending a talk organized by the group “Kritiske Politter” a long time ago. The speaker was a professor from the physics department who, as a hobby, created simulations of markets. His main critique of traditional economics was the reliance on steady-state assumptions, which are quite common in the field. He argued that natural systems often exhibit fluctuations, even when they are in balance.\n\n\nTo give an example: If one simulates an island with rabbits and foxes, there will be fluctuations in the populations. When there are many rabbits, the fox population increases, which then leads to a decline in rabbits. This, in turn, causes a decline in foxes, allowing the rabbit population to grow again, and the cycle continues. In this case, a perfect equilibrium is unrealistic; instead, the equilibrium is a range in which the number of foxes and rabbits alternates over time.\nI find this field fascinating. One of my primary concerns is not just creating rational agents but making them realistic. In my economics courses, a lot of different techniques and models were presented to answer central questions. The argument often was that if these models capture the essence of some problem, they are useful. To some extent, I agree with this argument. However, I disagree with making assumptions solely for the purpose of solving the steady state of a system. This is why the agent-based approach is so interesting to me—it resembles the way physicists would approach a problem.\n\n\n\nIn this small project, I will attempt to create a basic framework for simulating agents. When I was first introduced to object-oriented programming, we worked on something similar to the rabbit-and-fox scenario.\nThe Code Will Be Written in Python Since the company appears to use Python, I will carry out the simulation using this language.\nIf I were building an extensive simulation framework for agents, I might consider using C++ due to its performance advantages. The key idea behind agent-based simulations is the interaction between agents, which makes it an iterative process. While Python may not be the fastest for such loops, parallelized programming could improve performance.\nAlthough I have experience with parallelization in R, I imagine Python offers more sophisticated methods. For instance, dividing agents into clusters that interact and processing these simultaneously could be an interesting approach. Learning and implementing this would be a valuable experience, although it exceeds my current skill level. In R, parallelization often involves copying the entire dataset to avoid writing conflicts; it would be exciting to explore more efficient solutions.\n\n\n\nIn summary, agent-based approaches are incredibly interesting and would be enjoyable to work on. I believe this problem provides a great opportunity to refresh my object-oriented programming skills. These simulations rely on Monte Carlo methods as their underlying mathematical approach, which makes them challenging to test."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#rabbits-and-foxes",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#rabbits-and-foxes",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "",
    "text": "To give an example: If one simulates an island with rabbits and foxes, there will be fluctuations in the populations. When there are many rabbits, the fox population increases, which then leads to a decline in rabbits. This, in turn, causes a decline in foxes, allowing the rabbit population to grow again, and the cycle continues. In this case, a perfect equilibrium is unrealistic; instead, the equilibrium is a range in which the number of foxes and rabbits alternates over time.\nI find this field fascinating. One of my primary concerns is not just creating rational agents but making them realistic. In my economics courses, a lot of different techniques and models were presented to answer central questions. The argument often was that if these models capture the essence of some problem, they are useful. To some extent, I agree with this argument. However, I disagree with making assumptions solely for the purpose of solving the steady state of a system. This is why the agent-based approach is so interesting to me—it resembles the way physicists would approach a problem."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#this-project",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#this-project",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "",
    "text": "In this small project, I will attempt to create a basic framework for simulating agents. When I was first introduced to object-oriented programming, we worked on something similar to the rabbit-and-fox scenario.\nThe Code Will Be Written in Python Since the company appears to use Python, I will carry out the simulation using this language.\nIf I were building an extensive simulation framework for agents, I might consider using C++ due to its performance advantages. The key idea behind agent-based simulations is the interaction between agents, which makes it an iterative process. While Python may not be the fastest for such loops, parallelized programming could improve performance.\nAlthough I have experience with parallelization in R, I imagine Python offers more sophisticated methods. For instance, dividing agents into clusters that interact and processing these simultaneously could be an interesting approach. Learning and implementing this would be a valuable experience, although it exceeds my current skill level. In R, parallelization often involves copying the entire dataset to avoid writing conflicts; it would be exciting to explore more efficient solutions."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#summary",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#summary",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "",
    "text": "In summary, agent-based approaches are incredibly interesting and would be enjoyable to work on. I believe this problem provides a great opportunity to refresh my object-oriented programming skills. These simulations rely on Monte Carlo methods as their underlying mathematical approach, which makes them challenging to test."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#the-environment-strøget",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#the-environment-strøget",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "The Environment (Strøget)",
    "text": "The Environment (Strøget)\nSince the company is located in Copenhagen, I thought it would be fun to use the city center as the topic of this exercise.\nIf we limit Strøget to the area between “Storkespringvandet” and the entrance near City Hall, we can imagine it as a long corridor with shops on both sides. To simplify, we’ll assume there’s one shop at each location along this corridor.\nThus, the environment can be represented as a long chain of shops. Agents can start from either end and pass through the corridor."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#the-shopper",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#the-shopper",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "The Shopper",
    "text": "The Shopper\nFrom personal experience, it seems that nobody goes to Strøget to buy necessities—it’s more about purchasing nice-to-have items and doing light shopping. I’ll assume people arrive at Strøget with a certain amount of money (cash) they want to spend.\nI created some preference classes, such as Food, Clothing, and Books. By making these preferences into classes, I can tailor purchase patterns for each category.\nFood: This is consumed only once, meaning if someone buys food, they won’t purchase it again. Cash: The shopper’s cash balance decreases with each purchase. Position: Tracks where the shopper is located in the chain of shops. Presence: Indicates whether the shopper is still on the street. When a shopper encounters a shop that matches their preferences, they will buy something with a given probability—but only if they can afford it."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#the-shop-agents",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#the-shop-agents",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "The Shop Agents",
    "text": "The Shop Agents\nShop agents will offer products at a given price in specific categories. To keep things simple, each shop offers one product in one category.\nSimulations The simulation is managed via the Street class.\nTo simplify, shops are ordered by their placement along the street. I haven’t implemented checks for errors in the ordering since this is just a toy example.\nThe simulation progresses with a tick function, allowing the system to update step by step.\nOverall Overall, I like this construction. Instead of storing positions in a variable, I could have moved the object itself or used pointers to save computational costs related to copying."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#classes",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#classes",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "Classes",
    "text": "Classes\nBelow, my classes and the central construction can be found.\n\n\nCode\nimport random\n\nclass Shop:\n    def __init__(self, placement: int, category: str, price: int):\n        \"\"\"\n        Represents a shop in the market.\n        :param placement: Position of the shop along the market.\n        :param category: The type of items sold by the shop (e.g., 'Food', 'Books').\n        :param price: The price of the items sold.\n        \"\"\"\n        self.placement = placement\n        self.category = category\n        self.price = price\n        self.units_sold=0\n    \n    def sold_unit(self):\n        self.units_sold +=1\n\n    def turnover(self):\n        return self.units_sold*self.price\n\n\"\"\"The belov class represent prefference a cosumer could have\"\"\"\nclass food:\n    def __init__(self,type,buy_prop,have_eaten=False,units_bought=0):\n        self.type=type\n        self.buy_prop=buy_prop\n        self.have_eaten=have_eaten\n        self.units_bought=units_bought\n    def will_buy(self):\n        return_obj=False\n        if(self.have_eaten==False):\n            return_obj= random.random()&lt; self.buy_prop\n            if(return_obj==True):\n                self.have_eaten=True\n                self.units_bought+=1\n        return (return_obj)\n\nclass cloth:\n    def __init__(self,type,buy_prop,units_bought=0):\n        self.type=type\n        self.buy_prop=buy_prop\n        self.units_bought=units_bought\n    def will_buy(self): \n        return_obj=random.random()&lt; self.buy_prop\n        if return_obj== True: self.units_bought+=1\n        return return_obj\n\nclass book:\n    def __init__(self,type,buy_prop,units_bought=0):\n        self.type=type\n        self.buy_prop=buy_prop\n        self.units_bought=units_bought\n    def will_buy(self):\n        return_obj=random.random()&lt; self.buy_prop\n        if return_obj== True: self.units_bought+=1\n        return return_obj\n\n\nclass Shopper:\n    def __init__(self,placement: int,direction,cash: float,preferences: list):\n        \"\"\"\n        Represents a shopper in the market.\n        :param placement: Current position of the shopper.\n        :param direction: Direction of movement (-1 for left, 1 for right).\n        :param cash: Amount of money the shopper has.\n        :param gender: Gender of the shopper (used for preferences).\n        :param preferences: Dictionary of categories and buy probabilities.\n        \"\"\"\n        self.placement = placement\n        self.direction = direction\n        self.cash = cash\n        self.preferences = preferences  \n        self.has_eaten = False\n        self.has_eaten_dessert = False\n        self.shoper_pressent_at_market=True\n        \n    def update_position(self):\n        \"\"\"This function update the postion of the shopper and updates if their are shoper pressent at market\"\"\"\n        self.placement=self.placement+self.direction\n        return\n    \n    def visit_shop(self, shop: Shop):\n        \"\"\"\n        Interact with a shop and decide whether to buy.\n        :param shop: The shop to interact with.\n        \"\"\"\n        # find if shope is in the prefrence\n        did_buy=False\n        for index in range(len(self.preferences)):\n            if(self.preferences[index].type==shop.category):\n                if(shop.price&lt;self.cash):\n                    did_buy=self.preferences[index].will_buy()\n            if (did_buy==True):\n                self.cash=self.cash-shop.price\n                shop.sold_unit()\n        self.update_position()\n        return did_buy\n        \n    \nclass shopping_street:\n    def __init__(self,list_of_shops,list_of_shopers):\n        # List of shoops should be ordere by placement\n        self.list_of_shops=list_of_shops\n        self.list_of_shopers=list_of_shopers\n        self.min=list_of_shops[0].placement\n        self.max=list_of_shops[(len(list_of_shops)-1)].placement\n\n    def run_tick_shopper(self,index):\n        \"\"\"This function will run one iteration of the simulation for a given agent\"\"\"\n        if (self.list_of_shopers[index].shoper_pressent_at_market)==True:\n            self.list_of_shopers[index].visit_shop(self.list_of_shops[index])\n        if(self.min&lt;(self.list_of_shopers[index].placement) or self.list_of_shopers[index].placement&lt;(self.max)):\n            self.list_of_shopers[index].shoper_pressent_at_market=False\n    def run_tick(self):\n        \"\"\"This function will run one iteration of the simulation for all agents\"\"\"\n        for ind in range(len(self.list_of_shopers)):\n            self.run_tick_shopper(ind)\n    def get_unit(self):\n        unit_sold=0\n        for ind in range(len(self.list_of_shops)):\n            unit_sold +=self.list_of_shops[ind].units_sold\n        return(unit_sold)\n    def get_total_turnover(self):\n        turnover=0\n        for ind in range(len(self.list_of_shops)):\n            turnover +=self.list_of_shops[ind].turnover()\n        return(turnover)"
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#unit-testing",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#unit-testing",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "Unit Testing",
    "text": "Unit Testing\nBelow, my Unit test, can be seen\n\n\nCode\n# unit test\nimport  unittest\n\ndef test_Shop_class():\n    test_Shop = Shop(2,\"men cloth\",100)\n    test_Shop.sold_unit()\n    assert(test_Shop.units_sold==1)\n    assert(test_Shop.units_sold*test_Shop.price==test_Shop.turnover())\n    \n    return\ntest_Shop_class()\n\ndef test_cloth_class():\n    test_cloth = cloth(type=\"men cloth\", buy_prop=1)\n    assert test_cloth.will_buy() == True\n    assert test_cloth.units_bought==1\n    return\ntest_cloth_class()\ndef test_book_class():\n    test_book = book(type=\"book store\", buy_prop=1)\n    assert test_book.will_buy() == True\n    assert test_book.units_bought==1\n    return\ntest_book_class()\ndef test_food_class():\n    test_food=food(type=\"Fastfood\",buy_prop=1)\n    assert(test_food.have_eaten)==False\n    # Should buy since buy prop = 1 \n    assert(test_food.will_buy()==True)\n    #\n    assert(test_food.units_bought==1)\n    # have eaten = True \n    assert(test_food.have_eaten)==True\n    #should not buy since have eaten\n    assert(test_food.will_buy()==False)\n    return \ntest_food_class()\n    \ndef test_Shopper_class():\n    test_Shop = Shop(2,\"men cloth\",100)\n    test_food=food(type=\"Fastfood\",buy_prop=1)\n    test_cloth = cloth(type=\"men cloth\", buy_prop=1)\n    test_Shopper=Shopper(placement=2,direction=-1,cash=1000,preferences=[test_food,test_cloth])\n    assert test_Shopper.placement==2\n    # The visit function should change the cash and update the units sold if the shop is in the preferences\n    assert(test_Shopper.visit_shop(test_Shop))\n    assert test_Shopper.placement==1\n    assert test_Shopper.preferences[1].units_bought==1\n    assert test_Shopper.cash==1000-test_Shop.price\n    assert test_Shopper.placement==1\n    # Test of shope not in preferences\n    test_Shop_2=Shop(1,\"book\",100)\n    assert test_Shopper.visit_shop(test_Shop_2) ==False\n    # since\n    assert test_Shopper.cash==1000-test_Shop.price\n    assert test_Shopper.placement==0\n    return \n    \ntest_Shopper_class()\n\ndef test_shopping_street_class():\n    test_Shop_1 = Shop(2,\"men cloth\",100)\n    test_shop_2=Shop(1,\"book\",50) \n    test_food=food(type=\"Fastfood\",buy_prop=1)\n    test_cloth = cloth(type=\"men cloth\", buy_prop=1)\n    test_Shopper_1=Shopper(placement=2,direction=-1,cash=1000,preferences=[test_food,test_cloth])\n    test_Shopper_2=Shopper(placement=1,direction=1,cash=1000,preferences=[test_food,test_cloth])\n    test_shopping_street=shopping_street(list_of_shops=[test_Shop_1,test_shop_2],list_of_shopers=[test_Shopper_1,test_Shopper_2])\n    \n    # No book should be hold sold \n    # The mens cloth should be sold\n    assert test_shopping_street.list_of_shopers[0].placement==2\n    assert test_shopping_street.list_of_shopers[1].placement==1\n    assert test_shopping_street.list_of_shops[0].units_sold==0\n    assert test_shopping_street.list_of_shops[1].units_sold==0\n    test_shopping_street.run_tick()\n    assert test_shopping_street.list_of_shopers[0].placement==1\n    assert test_shopping_street.list_of_shopers[1].placement==2\n    assert test_shopping_street.list_of_shops[0].units_sold==1\n    assert test_shopping_street.list_of_shops[1].units_sold==0\n    test_shopping_street.run_tick()\n    test_shopping_street.run_tick()\n    assert test_shopping_street.list_of_shopers[0].placement==0\n    assert test_shopping_street.list_of_shopers[1].placement==3\n    assert test_shopping_street.list_of_shops[0].units_sold==2\n    assert test_shopping_street.list_of_shops[1].units_sold==0\n    test_shopping_street.run_tick()\n    assert test_shopping_street.list_of_shopers[0].placement==0\n    assert test_shopping_street.list_of_shopers[1].placement==3\n    assert test_shopping_street.list_of_shops[0].units_sold==2\n    assert test_shopping_street.list_of_shops[1].units_sold==0\n    assert test_shopping_street.get_total_turnover()==2*100\n    assert test_shopping_street.get_unit()\n\n    return\ntest_shopping_street_class()"
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#simulation",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#simulation",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "Simulation",
    "text": "Simulation\nIn this code, I implemented a small simulation to validate the framework. I plotted the units sold and the turnover. Since this is a Monte Carlo simulation, there is no theoretical distribution for comparison\n\n\nCode\nimport matplotlib as plt\n\ndef simulate_street():\n    shops = [\n        Shop(placement=0, category=\"food\", price=50),\n        Shop(placement=1, category=\"cloth\", price=200),\n        Shop(placement=2, category=\"book\", price=150),\n        Shop(placement=3, category=\"food\", price=100),\n        Shop(placement=4, category=\"cloth\", price=300),\n        Shop(placement=5, category=\"book\", price=250)\n        ]\n    ticks=len(shops)\n    preferences_1 = [\n        food(type=\"food\", buy_prop=0.8),\n        cloth(type=\"cloth\", buy_prop=0.5),\n        book(type=\"book\", buy_prop=0.6)\n        ]\n    preferences_2 = [\n        food(type=\"food\", buy_prop=0.6),\n        cloth(type=\"cloth\", buy_prop=0.3),\n        book(type=\"book\", buy_prop=0.9)\n        ]\n    shoppers = [\n        Shopper(placement=0, direction=1, cash=500, preferences=random.choice([preferences_1, preferences_2])),\n        Shopper(placement=5, direction=-1, cash=800, preferences=random.choice([preferences_1, preferences_2])),\n        Shopper(placement=2, direction=1, cash=300, preferences=random.choice([preferences_1, preferences_2]))\n        ]\n    # Initialize the shopping street\n    street = shopping_street(list_of_shops=shops, list_of_shopers=shoppers)\n    for _ in range(ticks):\n        street.run_tick()\n\n    # Return a dictionary with the results\n    return {\n        \"Total_units\": street.get_unit(),\n        \"Total_turnover\": street.get_total_turnover()\n    }\n\n\n#plot distibution of sale and and units sold\nTotal_units=[]\nTotal_turnover=[]\nfor i in range(100):\n    sim_obj=simulate_street()\n    Total_units.append(sim_obj[\"Total_units\"])\n    Total_turnover.append(sim_obj[\"Total_turnover\"])\n\n\n\n\nimport matplotlib.pyplot as plt\n\n\n\n# Create two subplots\nfig, axes = plt.subplots(2, 1, figsize=(8, 10))  # 2 rows, 1 column\n\n# Histogram for List A\naxes[0].hist(Total_units, bins=5, color='blue', alpha=0.7)\naxes[0].set_title('Histogram of Total units')\naxes[0].set_xlabel('Value')\naxes[0].set_ylabel('Frequency')\n\n# Histogram for List B\naxes[1].hist(Total_turnover, bins=5, color='orange', alpha=0.7)\naxes[1].set_title('Histogram of Total turnover')\naxes[1].set_xlabel('Value')\naxes[1].set_ylabel('Frequency')\n\n# Adjust layout\nplt.tight_layout()\n\n# Show plots\nplt.show()"
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#downstream-effects",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#downstream-effects",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "Downstream Effects",
    "text": "Downstream Effects\nI believe downstream effects would be simpler to implement. To track these efficiently, I would use a directed graph or a reactive graph (as described in Mastering Shiny). The reactive graph approach could be more efficient for managing dependencies and dynamic changes in the system."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#parallelization-challenges",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#parallelization-challenges",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "Parallelization Challenges",
    "text": "Parallelization Challenges\nA minor technical detail: as the code is written now, parallelizing the simulation would be challenging. Although the shoppers do not interact directly, they interact with the same shops. This creates a potential issue when writing to shared variables, such as the units sold in a shop. If multiple shoppers update this variable simultaneously, errors could occur.\nI am particularly interested in learning how to address these kinds of problems effectively. For example, using locks or other concurrency control methods might help, but exploring more sophisticated solutions could be valuable."
  },
  {
    "objectID": "Projects/House-prices-project.html",
    "href": "Projects/House-prices-project.html",
    "title": "Housing pricing project",
    "section": "",
    "text": "#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(purrr)\n\n\ndata_before &lt;- read.csv(\"https://github.com/Missing-almost-everywhere/Missing-almost-everywhere.io/blob/main/Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/data_before_celan.csv\", stringsAsFactors = FALSE)\ndata_before&lt;-as.data.frame(data_before)\n# Define UI\nui &lt;- fluidPage(\n  titlePanel(\"Dynamic Plotting with Shiny\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"variable\", \"Select Variable:\", \n                  choices = names(data_before))\n    ),\n    mainPanel(\n      plotOutput(\"dynamicPlot\")\n    )\n  )\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n  \n  output$dynamicPlot &lt;- renderPlot({\n    col &lt;- input$variable\n    \n    if (is.numeric(df[[col]])) {\n      # For numeric variables, create a histogram\n      n_unique &lt;- length(unique(data_before[[col]]))\n      ggplot(data_before, aes_string(x = col)) + \n        geom_histogram(bins = min(n_unique, 30), fill = 'skyblue', color = 'black', alpha = 0.7) + \n        labs(title = paste('Histogram of', col), x = col, y = 'Frequency') + \n        theme_minimal()\n    } else {\n      # For non-numeric variables, create a bar plot\n      ggplot(data_before, aes_string(x = col)) + \n        geom_bar(fill = 'orange', color = 'black', alpha = 0.7) + \n        labs(title = paste('Bar Plot of', col), x = col, y = 'Count') + \n        theme_minimal()\n    }\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html",
    "title": "House Pricing Project",
    "section": "",
    "text": "This is practics case based on some data from kaggle\nI mostly made this to play around with some data and learn new methods. When working with data, like many others, I like to use either R Markdown or Jupyter Notebook. I also enjoy writing small notes and comments so that if I need to revisit the project later, I can easily see what I did. This approach is really helpful in terms of reproducibility. Unfortunately, people often share notebooks without adding any text or comments.\nWhile I’ve written out some comments here, I don’t go too deep. However, I do show parts of my workflow. The code is written in R, and I consider it what I call “one-time code.” It’s not meant to be read by others. It’s not optimized either—I’ve simply chosen the fastest code I could think of or find to solve a problem. I ende op using Chat-gbt for some code aswell then correcting the mistakes. The methology and the models chosen is all me."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#comments-on-available-data",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#comments-on-available-data",
    "title": "House Pricing Project",
    "section": "Comments on Available Data",
    "text": "Comments on Available Data\nAll the houses are located in the Ames city area. There is information on the month and year sold. (In both the training and test sets, the years have the same values, meaning there’s no need to estimate values forward in time. This is a bit unusual—what is the purpose of the model in such a case?) Most of the data is categorical or discrete in nature. Some of the data has a natural order or rank. When examining the available data, it’s worth noting that there is no information on earlier sale prices—why is this missing? Information on “time on market” is also missing."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#data-cleaning-and-exploration",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#data-cleaning-and-exploration",
    "title": "House Pricing Project",
    "section": "Data cleaning and exploration",
    "text": "Data cleaning and exploration\n\n\nCode\nlibrary(readr)\ndf=as.data.frame(read.csv(\"train.csv\"))\n\n#split &lt;- sample(1:nrow(df), size = 0.8 * nrow(df))\n\n# Create training and testing sets\n#df &lt;- df[split, ]  # 80% of the data\n#df_test_for_choice &lt;- df[-split, ]  # other 20%\n\n#\ndf_test_preformence=as.data.frame(read.csv(\"test.csv\"))"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#recoding-na",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#recoding-na",
    "title": "House Pricing Project",
    "section": "Recoding NA",
    "text": "Recoding NA\nIn the description, I noticed that some of the variables use NA as a category.\nVaribels where NA is a catgori is listed below.\n\nAlley\nBsmtQual\nBsmtCond\nBsmtExposure\nBsmtFinType1\nBsmtFinType2\nFireplaceQu\nGarageType\nGarageFinish\nGarageQual\nGarageCond\nPoolQC\nFence\nMiscFeature\n\nThese variables need to be recoded, as the values are not missing—they are simply in the wrong format.\n\n\nCode\n#change \nchang_to_df&lt;-function(DF){\n  DF$Alley&lt;-DF$Alley[is.na(DF$Alley)] &lt;- \"No alley\"\n  DF$BsmtQual[is.na(DF$BsmtQual)]&lt;-\"No Basement\"\n  DF$BsmtCond[is.na(DF$BsmtCond)]&lt;-\"No Basement\"\n  DF$BsmtExposure[is.na(DF$BsmtExposure)]&lt;-\"No Basement\"\n  DF$BsmtFinType1[is.na(DF$BsmtFinType1)]&lt;- \"No Basement\"\n  DF$BsmtFinType2[is.na(DF$BsmtFinType2)]&lt;- \"No Basement\"\n  DF$FireplaceQu[is.na(DF$FireplaceQu)]&lt;-\"No Fireplace\"\n  DF$GarageType[is.na(DF$GarageType)]&lt;-\"No Garage\"\n  DF$GarageFinish[is.na(DF$GarageFinish)]&lt;-\"No Garage\"\n  DF$GarageQual[is.na(DF$GarageQual)]&lt;-\"No Garage\"\n  DF$GarageCond[is.na(DF$GarageCond)]&lt;-\"No Garage\"\n  DF$GarageYrBlt[is.na(DF$GarageYrBlt)]&lt;-0 # all values that is NA corespond wither other showing that their is no garage.\n  DF$PoolQC[is.na(DF$PoolQC)]&lt;-\"No Pool\"\n  DF$Fence[is.na(DF$Fence)]&lt;-\"No Fence\"\n  DF$MiscFeature[is.na(DF$MiscFeature)]&lt;-\"None\"\n  return(DF)\n}\n\ndf&lt;-chang_to_df(df)\n#df_test_for_choice&lt;-chang_to_df(df_test_for_choice)\ndf_test_preformence&lt;-chang_to_df(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#examining-missing-values",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#examining-missing-values",
    "title": "House Pricing Project",
    "section": "Examining Missing Values",
    "text": "Examining Missing Values\nAfter recoding cases where NA does not represent missing values, we will now analyze the actual missing values.\nBelow is a plot showing the missing values. I had to split the plot into two parts; otherwise, the variable names would not be readable.\n\n\nCode\nlibrary(naniar)\nvis_miss(df[,1:40])\n\n\n\n\n\n\n\n\n\nCode\nvis_miss(df[,40:81])\n\n\n\n\n\n\n\n\n\nLotFrontage missing values look weird.\nThe definition is Linear feet of street connected to property. Below i have printed uniqe entreances in LotFrontage.\n\n\nCode\ntable((df$LotFrontage))\n\n\n\n 21  24  30  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48 \n 23  19   6   5   1  10   9   6   5   1   1  12   6   4  12   9   3   1   5   6 \n 49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68 \n  4  57  15  14  10   6  17   5  12   7  13 143   8   9  17  19  44  15  12  19 \n 69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88 \n 11  70  12  17  18  15  53  11   9  25  17  69   6  12   5   9  40  10   5  10 \n 89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 \n  6  23   6  10   8   6   7   8   2   8   3  16   2   4   3   3   6   1   7   3 \n109 110 111 112 114 115 116 118 120 121 122 124 128 129 130 134 137 138 140 141 \n  2   6   1   1   2   2   2   2   7   2   2   2   1   2   3   2   1   1   1   1 \n144 149 150 152 153 160 168 174 182 313 \n  1   1   1   1   1   1   1   2   1   2 \n\n\nFrom the definition, it could refer to farms, but there are no farms in the dataset.\nIf we plot the house types against the number of missing values, we see that most of them come from single-family detached houses.\n\n\nCode\nlibrary(dplyr)\n# Test farm theory\ntest_LotFrontage &lt;- df$LotFrontage\ntest_LotFrontage[is.na(test_LotFrontage)] &lt;- 0\ntest_BldgType &lt;- factor(df$BldgType)\n\n\n\n# Create a data frame\ndata &lt;- data.frame(test_BldgType, test_LotFrontage)\n\nzero_counts &lt;- data %&gt;%\n  filter(test_LotFrontage == 0) %&gt;%\n  count(test_BldgType)\n\n# Create the bar plot with category labels at the bottom\nbarplot(zero_counts$n, \n        names.arg = zero_counts$test_BldgType,  # Use the correct column for labels\n        main = \"Number of Zeros by Category\",\n        xlab = \"Categories\",\n        ylab = \"Count of Zeros\",\n        col = \"lightblue\",\n        las = 2)  # Rotate labels to vertical\n\n\n\n\n\n\n\n\n\nIt seems like most of the missing values are from single-family detached houses, so it’s unrealistic to assume there is no street connected to the property. This means setting the values to zero is likely a bad option.\nOptions for imputation:\nLinear regression based on other variables. k-nearest neighbors. I could also disregard this variable since I doubt it has high predictive power.\nIt seems like an interactive linear model, incorporating the interaction between LotArea and LotShape, would provide good imputation (this also makes sense conceptually).\n\n\nCode\nImputasion_model &lt;- lm(LotFrontage ~ LotArea * LotShape, df)\n\n# Plot fitted values vs. residuals\nplot(Imputasion_model$fitted.values, Imputasion_model$residuals,\n     xlab = \"Fitted Values\",\n     ylab = \"Residuals\",\n     main = \"Residuals plot (LotFrontage ~ LotArea * LotShape)\")\n\n# Add a horizontal line at the mean of the residuals (which should be zero)\nabline(h = mean(Imputasion_model$residuals), col = \"red\", lwd = 2, lty = 2)\nmean_value &lt;- mean(Imputasion_model$residuals)\ntext(x = max(Imputasion_model$fitted.values), \n     y = mean_value, \n     labels = \"Mean\", \n     col = \"red\")\n\n\n\n\n\n\n\n\n\nMissing values for LotFrontage will be imputed using the linear model described above.\nThis also means that if LotFrontage is used in the model, the imputation will need to be performed on the test set as well. I have internally debated whether I should include the test set when building the imputation model—essentially, creating a model based on both the training and test sets for this purpose.\nSince I’m a little new to Kaggle, if the test set contains covariates and I only need to upload the fitted values, I would proceed with this approach. Otherwise, I would not. For now, I will just use the model based only on the training set.\n\n\nCode\n# imputatsion\nImputasion_model &lt;- lm(LotFrontage ~ LotArea * LotShape, df)\n\nInput_value&lt;-function(DF){\n  for (i in 1:length(DF$LotFrontage)){\n    if(is.na(DF$LotFrontage[i])){\n      DF$LotFrontage[i]&lt;-predicted_value &lt;- predict(Imputasion_model, newdata = list(\n        LotArea = DF$LotArea[i],\n        LotShape = DF$LotShape[i] ))\n    }\n  }\n  return(DF)\n  }\n\n#input df for all input values\ndf&lt;-Input_value(df)\n#df_test_for_choice&lt;-Input_value(df_test_for_choice)\ndf_test_preformence&lt;-Input_value(df_test_preformence)\n\n\nThere are still 1% of missing values in MasVnrArea and MasVnrType. I will discard the last row containing these missing values."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#recoding-of-variables",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#recoding-of-variables",
    "title": "House Pricing Project",
    "section": "Recoding of Variables",
    "text": "Recoding of Variables\nSome of these variables have a ranking, which makes them well-suited for recoding. By recoding them as numerical values, the ranking becomes more obvious.\n\n\nClick to expand/collapse varibel recoeding\n\nTo give a exampel of the recoding. PoolQC: Pool quality\n   Ex   Excellent         (4)\n   Gd   Good              (3)\n   TA   Average/Typical   (2)   (TA is never obsevered in pool varibel for traning set)\n   Fa   Fair              (1)\n   NA   No Pool           (0)\ncan be codes as 0-4\nBelowe here i have made list of how the varibels is code. I have made is so no pressent cagories NA is code to 0. Meaning 0 is only a value if the item is not pressent. and otherwise it starts from 1.\nLotShape: General shape of property Reg Regular (4) IR1 Slightly irregular (3) IR2 Moderately Irregular (2) IR3 Irregular (1)\nLandContour: Flatness of the property\n   Lvl  Near Flat/Level                                                     (4)\n   Bnk  Banked - Quick and significant rise from street grade to building   (3)\n   HLS  Hillside - Significant slope from side to side                      (2)\n   Low  Depression                                                          (1)\nLandSlope: Slope of property\n   Gtl  Gentle slope    (3)\n   Mod  Moderate Slope  (2)\n   Sev  Severe Slope    (1)\nExterQual: Evaluates the quality of the material on the exterior\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\n   \nExterCond: Evaluates the present condition of the material on the exterior\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\n   \nBsmtQual: Evaluates the height of the basement\n   Ex   Excellent (100+ inches)   (5)\n   Gd   Good (90-99 inches)       (4)\n   TA   Typical (80-89 inches)    (3)\n   Fa   Fair (70-79 inches)       (2)\n   Po   Poor (&lt;70 inches          (1)\n   NA   No Basement               (0)\nBsmtCond: Evaluates the general condition of the basement\n   Ex   Excellent                                     (5)\n   Gd   Good                                          (4)\n   TA   Typical - slight dampness allowed             (3)\n   Fa   Fair - dampness or some cracking or settling  (2)\n   Po   Poor - Severe cracking, settling, or wetness  (1)\n   NA   No Basement                                   (0)\nBsmtExposure: Refers to walkout or garden level walls\n   Gd   Good Exposure                                                               (4)\n   Av   Average Exposure (split levels or foyers typically score average or above)  (3) \n   Mn   Mimimum Exposure                                                            (2)\n   No   No Exposure                                                                 (1)\n   NA   No Basement                                                                 (0)\nBsmtFinType1: Rating of basement finished area\n   GLQ  Good Living Quarters            (6)\n   ALQ  Average Living Quarters         (5)\n   BLQ  Below Average Living Quarters     (4)\n   Rec  Average Rec Room                (3)\n   LwQ  Low Quality                     (2)\n   Unf  Unfinshed                       (1)\n   NA   No Basement                       (0)\nBsmtFinType2: Rating of basement finished area (if multiple types)\n   GLQ  Good Living Quarters            (6)\n   ALQ  Average Living Quarters         (5)\n   BLQ  Below Average Living Quarters     (4)\n   Rec  Average Rec Room                (3)\n   LwQ  Low Quality                     (2)\n   Unf  Unfinshed                       (1)\n   NA   No Basement                       (0)\nHeatingQC: Heating quality and condition\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\nKitchenQual: Kitchen quality\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\nFunctional: Home functionality (Assume typical unless deductions are warranted)\n   Typ  Typical Functionality   (7)\n   Min1 Minor Deductions 1      (6)\n   Min2 Minor Deductions 2      (5)\n   Mod  Moderate Deductions     (4)\n   Maj1 Major Deductions 1      (3)\n   Maj2 Major Deductions 2      (2)\n   Sev  Severely Damaged        (1)\n   Sal  Salvage only            (0)\n   \nFireplaceQu: Fireplace quality\n   Ex   Excellent - Exceptional Masonry Fireplace                                               (5)\n   Gd   Good - Masonry Fireplace in main level                                                  (4)\n   TA   Average - Prefabricated Fireplace in main living area or Masonry Fireplace in basement  (3)\n   Fa   Fair - Prefabricated Fireplace in basement                                              (2)\n   Po   Poor - Ben Franklin Stove                                                               (1)\n   NA   No Fireplace                                                                            (0)\nGarageFinish: Interior finish of the garage\n   Fin  Finished        (3)\n   RFn  Rough Finished  (2) \n   Unf  Unfinished      (1)\n   NA   No Garage         (0)\n   \nGarageQual: Garage quality\n   Ex   Excellent         (5)\n   Gd   Good              (4)\n   TA   Typical/Average   (3)\n   Fa   Fair              (2)\n   Po   Poor              (1)\n   NA   No Garage         (0)\nGarageCond: Garage condition\n   Ex   Excellent         (5)\n   Gd   Good              (4)\n   TA   Typical/Average   (3)\n   Fa   Fair              (2)\n   Po   Poor              (1)\n   NA   No Garage         (0)\nPoolQC: Pool quality\n   Ex   Excellent         (4)\n   Gd   Good              (3)\n   TA   Average/Typical   (2)   (TA is never obsevered in pool varibel for traning set)\n   Fa   Fair              (1)\n   NA   No Pool           (0)\n   \nFence: Fence quality\n   GdPrv    Good Privacy    (4)\n   MnPrv    Minimum Privacy (3)\n   GdWo Good Wood         (2)\n   MnWw Minimum Wood/Wire (1)\n   NA   No Fence            (0)\nTheir properly also some rank to other varibels, varibels like building matrials must have a ranking in terms of price. But i dont have any idear about whese.\n\n\n\nCode\n# Reencoding can be don via match.\n\n# recoding\nrecoding&lt;-function(DF){\n  DF$LotShape&lt;-match(DF$LotShape,c(\"IR3\",\"IR2\",\"IR1\",\"Reg\"))\n  #DF$LandContour&lt;-match(DF$LandContour,c(\"Low\",\"HLS\",\"Bnk\",\"Lvl\"))\n  #DF$LandSlope&lt;-match(DF$LandSlope,c(\"Sev\",\"Mod\",\"Gtl\"))\n  DF$ExterQual&lt;-match(DF$ExterQual,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$ExterCond&lt;-match(DF$ExterCond,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$BsmtQual&lt;-match(DF$BsmtQual,c(\"No Basement\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$BsmtCond&lt;-match(DF$BsmtCond,c(\"No Basement\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$BsmtExposure&lt;-match(DF$BsmtExposure,c(\"No Basement\",\"No\",\"Mn\",\"Av\",\"Gd\"))-1\n  DF$BsmtFinType1&lt;-match(DF$BsmtFinType1,c(\"No Basement\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"))-1\n  DF$BsmtFinType2&lt;-match(DF$BsmtFinType2,c(\"No Basement\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"))-1\n  DF$HeatingQC&lt;-match(DF$HeatingQ,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$KitchenQual&lt;-match(DF$KitchenQual,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$Functional&lt;-match(DF$Functional,c(\"Sal\",\"Sev\",\"Maj2\",\"Maj1\",\"Mod\",\"Min2\",\"Min1\",\"Typ\"))-1\n  DF$FireplaceQu&lt;-match(DF$FireplaceQu,c(\"No Fireplace\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$GarageFinish&lt;-match(DF$GarageFinish,c(\"No Garage\",\"Unf\",\"RFn\",\"Fin\"))-1\n  DF$GarageQual&lt;-match(DF$GarageQual,c(\"No Garage\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$GarageCond&lt;-match(DF$GarageCond,c(\"No Garage\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$PoolQC&lt;-match(DF$PoolQC, c(\"No Pool\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  #DF$Fence&lt;-match(DF$Fence,c(\"No Fence\",\"MnWw\",\"GdWo\",\"MnPrv\",\"GdPrv\"))-1\n  return(DF)\n}\n\ndf&lt;-recoding(df)\n#df_test_for_choice&lt;-recoding(df_test_for_choice)\ndf_test_preformence&lt;-recoding(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#feature-transformations",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#feature-transformations",
    "title": "House Pricing Project",
    "section": "Feature transformations",
    "text": "Feature transformations\nIn this section i will list the varibels i have change and what the changes was.\n\n\nClick to expand/collapse varibel Changes\n\nLooking at the description of the data the varibesl with lowe count, is\n\n40: 1-STORY W/FINISHED ATTIC ALL AGES\n45: 1-1/2 STORY - UNFINISHED ALL AGES\n75: 2-1/2 STORY ALL AGES\n85: SPLIT FOYER\n185: PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n\nHere is what i will combine them to.\n40 will be combined with 50 - 50: 1-1/2 STORY FINISHED ALL AGES Making 1-1/2 STORY FINISHED ALL AGES\n85 will be combined with 80 - 80: SPLIT OR MULTI-LEVEL Making Split\n185 will be combined with 160 - 160: 2-STORY PUD - 1946 & NEWER Making PUD - MULTILEVEL\nThis only leaves 45: UNFINISHED ALL AGES\nwith a vary small number of obsevations, in this section we looking at the data but this kind of asumption will affect models choice, what is the value of unfinished house, alle ages. A unfinish house can either be a cheap way to get new house ore a extra expens since it proberly should be removed. Both of these would proberly have lower value than the 1-1/2 STORY FINISHED ALL AGES. So if i inclued them i can drag down the estimat for this groups, and this can be unproporsional if the model is fitted with least sqaur. If i dont comined it with a varibel want to use a interaction effect in linear model I will get model wiht a lot NA, where this the combinations is not represented, i can get around this by tackling overide the model, so basically get to make geuss in whose case, that could be the average house price, something more cleaver, so for now i will leave it in. If i have to do the overird i will tell, this is better than a zero score. In the total linear model this is not a problem, so i may be the case this not problem.\n\nYearBuilt Som of this varibel is allready pressent in MSSubClass\nGarageCars I have think about, it would give nice distribution if it was seplified belove ore equalt 2 cars with true and false. but i think their is more infomation in o zeror so the the varibel vill be zero, one, two, above 2\n\nLooking at this it seam like it would be a god idear to cagaorise/colabse some of these varibel. This is to try to simplify the information. The problem that can arise is that with some of these varibels, they have such ueven distribution that, one can end up fitting a combination to uniqe house. this could be good fine but, in some case by colsaping one can better overall predictiv results. Later I will proberly reduce the dimension based on either spearman ore kendall tau corelation so one would want to ensure that if their is rank in the data is preserved. To give a exampel number of fireplace can be reduces to Yes or No. Yes would stille be higer than No.\n\nfireplace siplified to binary yes and NO\nPoolArea will be change to yes no their is not alot of information -pool yes No\n\nFor these i could it could potensiel be a good idear to eihter make fullbath total Halfbath total. ore colabs them in to yes no\n\nFullbath\nHalfbath\n\nbsmtFullBath\nBsmtHalfBath\nFenche will be change to True ore FAlSE\nSaleCondition will be normal True ore False\nLandContour will be change to level ore not. I will stil count this haveing a order, meaning level wich is equal to one is prefered\nLandSlope will be change to ground level ore not.\n\nFoundation will be combined by combin all other than “PConc” ore “CBlock” to “other”\nLotConfig all other than Corner Inside will be combined\nRoofStyle will be “Gable” or “other”\nThe one data point in MiscFeature that is tenis court will go under other\nIn Condition1 the following RRNe Within 200’ of East-West Railroad RRAe Adjacent to East-West Railroad RRNn Within 200’ of North-South Railroad RRAn Adjacent to North-South Railroad\nWill be combied to (Near Railroad)\nIn the Exterior1st and Exterior2nd. Their is some catagories with single obsevations in, I will put them in the closet cagori.\n\nI have rapt all the change in a function so it esay to aplly to the test dataset aswell\n\n\nCode\nmodify_df &lt;- function(DF){\n  DF$MSSubClass[DF$MSSubClass == 40] &lt;- 50\n  DF$MSSubClass[DF$MSSubClass == 85] &lt;- 80\n  DF$MSSubClass[DF$MSSubClass == 185] &lt;- 160\n  DF$have_Garage&lt;-DF$GarageCars&gt;0\n  DF$GarageCars&lt;-NULL\n  # is Fireplace pressent\n  DF$Fireplaces_present&lt;-DF$Fireplaces&gt;0\n  DF$Fireplaces&lt;-NULL\n  DF$FireplaceQu&lt;-NULL\n  # is pool pressent\n  DF$pool_present&lt;-DF$PoolArea&gt;0\n  DF$PoolArea&lt;-NULL\n  # Full bath pressent above ground\n  DF$FullBath_total&lt;-DF$BsmtFullBath+DF$FullBath\n  DF$BsmtFullBath&lt;-NULL\n  DF$FullBath&lt;-NULL\n  DF$HalfBath_total&lt;-DF$HalfBath+DF$BsmtHalfBath\n  DF$HalfBath&lt;-NULL\n  DF$BsmtHalfBath&lt;-NULL\n  DF$Alley&lt;-NULL\n  DF$Fence&lt;-DF$Fence!=\"No Fence\"\n  DF$SaleCondition&lt;-DF$SaleCondition==\"Normal\"\n  DF$LandContour&lt;-DF$LandContour==\"Lvl\"\n  DF$LandSlope&lt;-DF$LandSlope==\"Gtl\"\n  DF$Foundation[!(DF$Foundation %in% c(\"PConc\", \"CBlock\"))] &lt;- \"Other\"\n  DF$LotConfig[!(DF$LotConfig %in% c(\"Corner\",\"Inside\"))]&lt;-\"Other\"\n  DF$RoofStyle&lt;-DF$RoofStyle!=\"Gable\"\n  DF$MiscFeature[DF$MiscFeature==\"TenC\"]&lt;-\"Othr\"\n  DF$Condition1[DF$Condition1 %in% c(\"RRNe\",\"RRAe\",\"RRNn\",\"RRAn\")]&lt;-\"NR\"\n  DF$Exterior1st[DF$Exterior1st==\"AsphShn\"]&lt;-\"AsbShng\"\n  DF$Exterior1st[DF$Exterior1st==\"CBlock\"]&lt;-\"CemntBd\"\n  DF$Exterior1st[DF$Exterior1st==\"ImStucc\"]&lt;-\"Stucco\"\n  DF$Exterior2nd[DF$Exterior2nd==\"CBlock\"]&lt;-\"CemntBd\"\n  DF$Exterior2nd[DF$Exterior2nd==\"Other\"]&lt;-\"plywood\"\n  return(DF)\n}\n\ndf=modify_df(df)\n#df_test_for_choice&lt;-modify_df(df_test_for_choice)\ndf_test_preformence&lt;-modify_df(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#remove-varibels",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#remove-varibels",
    "title": "House Pricing Project",
    "section": "Remove varibels",
    "text": "Remove varibels\nIn this section, I will list the variables I plan to remove or disregard. The reason for their removal is that their distributions are very skewed compared to other variables, which I deem similar in the information they provide.\nThe main reason for disregarding these variables is that I do not believe they have high predictive strength, and I need to move forward with the data cleaning process. Also, I’m not getting paid for this.\n\n\nClick to expand/collapse varibel removed\n\n\nX3SsnPorch no real information will be removed (removed)\nScreenPorch no information aviable will be removed\nAlley contain no information and will be removed\nstreet contain not information and will be removed\nUtilities contain not information and will be removed\nCondition2 contain not information and will be removed\nRoofMatl contain not information and will be removed will be removed\nHeating contain not information and will be removed will be removed\nCentralAir contain not information and will be removed will be removed\nPoolQC contain not information and will be removed will be removed\nMiscFeature contain not information and will be removed will be removed\nGarageQual\nGarageCond\nGarageType\nElectrical\n\n\n\n\nCode\nremove_varibels&lt;-function(DF){\n  DF$X3SsnPorch&lt;-NULL\n  DF$ScreenPorch&lt;-NULL\n  DF$Alley&lt;-NULL\n  DF$Street&lt;-NULL\n  DF$Utilities&lt;-NULL\n  DF$Condition2&lt;-NULL\n  DF$RoofMatl&lt;-NULL\n  DF$Heating&lt;-NULL\n  DF$CentralAir&lt;-NULL\n  DF$PoolQC&lt;-NULL\n  DF$SaleType&lt;-NULL\n  DF$GarageQual&lt;-NULL\n  DF$GarageCond&lt;-NULL\n  DF$GarageType&lt;-NULL\n  DF$Electrical&lt;-NULL\n  return(DF)\n}\n\ndf=remove_varibels(df)\n#df_test_for_choice&lt;-remove_varibels(df_test_for_choice)\ndf_test_preformence&lt;-remove_varibels(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#variables-after-cleanup",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#variables-after-cleanup",
    "title": "House Pricing Project",
    "section": "Variables After Cleanup",
    "text": "Variables After Cleanup\nBelow, I have plotted the variables remaining after the cleanup.\n\n\nCode\nwrite.csv(df, \"data_after_clean.csv\", row.names = FALSE)\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(purrr)\n\nfor (col in names(df)) {\n  if (is.numeric(df[[col]])) {\n    # Numeriske variabler - Lav histogram\n    n_unique &lt;- length(unique(df[[col]]))\n    p &lt;- ggplot(df, aes_string(x = col)) + \n         geom_histogram(bins = min(n_unique, 30), fill = 'skyblue', color = 'black', alpha = 0.7) + \n         labs(title = paste('Histogram of', col), x = col, y = 'Frequency') + \n         theme_minimal()\n  } else {\n    # Ikke-numeriske variabler - Lav bar plot\n    p &lt;- ggplot(df, aes_string(x = col)) + \n         geom_bar(fill = 'orange', color = 'black', alpha = 0.7) + \n         labs(title = paste('Bar Plot of', col), x = col, y = 'Count') + \n         theme_minimal()\n  }\n  print(p)  # Vis plottet\n}\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(ggplot2)\n\ndata_before &lt;- read.csv(\"https://raw.githubusercontent.com/Missing-almost-everywhere/Missing-almost-everywhere.io/main/Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/data_after_clean.csv\")\n\nui &lt;- fluidPage(\n  titlePanel(\"House Prices Data Visualization\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"variable\", \"Select Variable:\", choices = names(data_before))\n    ),\n    mainPanel(\n      plotOutput(\"dynamicPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$dynamicPlot &lt;- renderPlot({\n    col &lt;- input$variable\n    \n    if (is.numeric(data_before[[col]])) {\n      ggplot(data_before, aes(x = .data[[col]])) + \n        geom_histogram(bins = 30, fill = 'skyblue', color = 'black', alpha = 0.7) +\n        labs(title = paste('Histogram of', col), x = col, y = 'Frequency') +\n        theme_minimal()\n    } else {\n      ggplot(data_before, aes(x = .data[[col]])) + \n        geom_bar(fill = 'orange', color = 'black', alpha = 0.7) +\n        labs(title = paste('Bar Plot of', col), x = col, y = 'Count') +\n        theme_minimal()\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#looking-into-cross-correlation",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#looking-into-cross-correlation",
    "title": "House Pricing Project",
    "section": "Looking Into Cross-Correlation",
    "text": "Looking Into Cross-Correlation\nBelow, I have plotted the correlation matrix for the variables. Since many of these variables are categorical but have a ranking, I used Spearman correlation.\n\n\nCode\ndf=na.omit(df)\nlibrary(corrplot)\n\n\ncorrplot 0.95 loaded\n\n\nCode\nlibrary(Hmisc)\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nCode\nnumeric_df &lt;- df[sapply(na.omit(df), is.numeric)]\n\n# Compute Spearman correlation matrix\nspearman_corr &lt;- cor(numeric_df, method = \"spearman\")\n\n# Compute p-values (optional)\nlibrary(Hmisc)\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n# Plot the correlation matrix\ncorrplot(spearman_corr, method = \"color\",tl.col = \"black\", tl.srt = 60, insig = \"blank\",tl.cex = 0.5,title = \"Spearman Correlation Plot\")\n\n\n\n\n\n\n\n\n\nNotes on the Plot Overall, the internal correlation is a lot smaller than I thought, which is good. This should make it easier to find good predictors.\nThe third variable from the bottom is the sales price. From this, one can see which variables could be potential predictors. It seems like OverallQual could be a really good predictor.\nHowever, correlation only captures linear effects, so one should be cautious about relying solely on correlation for feature selection. Imagine there was a feature with a sinusoidal curve that perfectly described the data—it would have a correlation of zero."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#reducing-variables-based-on-multicollinearity",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#reducing-variables-based-on-multicollinearity",
    "title": "House Pricing Project",
    "section": "Reducing Variables Based on Multicollinearity",
    "text": "Reducing Variables Based on Multicollinearity\nIn this section, I will reduce some variables, as multicollinearity can pose challenges for certain models. While some partitioning-based models are less sensitive to multicollinearity, linear models can be significantly impacted. In a simple linear model with two inputs, the variance of a parameter can be expressed as:\n\\[\\text{Var}(\\beta_1) = \\frac{\\sigma^2}{1 - \\text{cor}(x_1, x_2)}\\]\nThis means that as the correlation between two variables increases, the variance of the estimate explodes. Consequently, certain models are sensitive to multicollinearity.\nOne approach to addressing this is to remove variables with a correlation above a certain threshold. However, determining the optimal threshold is not straightforward. For a specific model, simulations can be conducted to estimate a suitable value. In this case, I want to perform some reduction before applying any models. I have chosen a cutoff point of (|0.7|). If there is a high correlation between two variables, I will remove the one with the weaker correlation to the log of the sale price.\nSince the variables are not numeric but do have a ranking, PCA is not an option. However, Spearman correlation can still be used.\nFor variable reduction, hierarchical clustering can be applied using the absolute value of the Spearman correlations as a distance measure. Single-linkage clustering can then be employed, with a slight reformulation of the problem. Hierarchical clustering requires a distance metric, and for this purpose, the distance between two variables based on correlation can be defined as:\n\\(d(x_1, x_2) = 1 - |\\text{cor}(x_1, x_2)|\\)\nGiven the cutoff of (|0.7|) for correlation, the corresponding distance after reformulation would be:\n\\(1 - 0.7 = 0.3\\)\n\n\nCode\n# The code here is not optimized but it was fast to write\nnumeric_df &lt;- df[sapply(na.omit(df), is.numeric)]\n\nnumeric_saleprice&lt;-numeric_df$SalePrice\nnumeric_df$SalePrice&lt;-NULL # remove from dataframe\n\nlibrary(Hmisc)\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n\ndist_matrix &lt;- as.dist(1-abs(spearman_corr))\n\nhc &lt;- hclust(dist_matrix, method = \"single\")\n\n\nclusters &lt;- cutree(hc, h = 0.3) #\n\n# Visualize dendrogram\nplot(hc, main = \"Hierarchical Clustering Dendrogram\")\nabline(h = 0.3, col = \"red\") # Add cutoff line\n\n\n\n\n\n\n\n\n\nCode\n# reduce varibels\nnumeric_saleprice_corr &lt;- rcorr(as.matrix(numeric_df), numeric_saleprice, type = \"spearman\")#$r[, \"numeric_saleprice\"]\n\n\n# Step 2: Identify clusters with multiple variables\nduplicates &lt;- table(clusters)[table(clusters) &gt; 1]\nclusters_with_duplicates &lt;- names(duplicates)\n\n\nfor (cluster in clusters_with_duplicates) {\n  clusters_with_duplicates\n  n_cluster=length(names(clusters[clusters==cluster]))\n  spear_var=rep(NA,n_cluster)\n  names_cluster=names(clusters[clusters==cluster])\n  for (i in 1:n_cluster){\n    names(clusters[clusters==cluster])[i]\n    spear_var[i]=rcorr(as.matrix(numeric_df[names(clusters[clusters==cluster])[i]]),log(numeric_saleprice),type=\"spearman\")$r[1,2]\n  }\n  index_of_max &lt;- which.max(spear_var)\n  names_cluster[-index_of_max]\n  #numeric_df[names_cluster[-index_of_max]]&lt;-NULL\n  df[names_cluster[-index_of_max]]&lt;-NULL\n  #df_test_for_choice[names_cluster[-index_of_max]]&lt;-NULL\n  df_test_preformence[names_cluster[-index_of_max]]&lt;-NULL\n}\n\n\nWith the clusters found, for each cluster, the feature with the highest Spearman correlation to the log of the sale price will be chosen.\nAs a result, there was a reduction from 61 to 41 features.\n\n\nCode\nnumeric_df &lt;- df[sapply(na.omit(df), is.numeric)]\n\nnumeric_saleprice&lt;-numeric_df$SalePrice\nnumeric_df$SalePrice&lt;-NULL # remove from dataframe\n\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n\ndist_matrix &lt;- as.dist(1-abs(spearman_corr))\n\nhc &lt;- hclust(dist_matrix, method = \"single\")\n\n\nclusters &lt;- cutree(hc, h = 0.3) #\n\n# Visualize dendrogram\nplot(hc, main = \"Hierarchical Clustering Dendrogram\")\nabline(h = 0.3, col = \"red\") # Add cutoff line\n\n\n\n\n\n\n\n\n\nAs can be seen in the new dendrogram, the dataframe has been reduced to meet the requirements."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#overview-of-variables-with-the-highest-correlation-to-log-of-sale-price",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#overview-of-variables-with-the-highest-correlation-to-log-of-sale-price",
    "title": "House Pricing Project",
    "section": "Overview of Variables with the Highest Correlation to Log of Sale Price",
    "text": "Overview of Variables with the Highest Correlation to Log of Sale Price\nBelow, I have ordered the variables based on the absolute value of their Spearman correlation.\n\n\nCode\n# Required libraries\nlibrary(corrplot)\nlibrary(Hmisc)\n\n# Subset the dataframe to numeric columns\nnumeric_df$log_saleprice&lt;-log(df$SalePrice)\nnumeric_df$SalePrice&lt;-NULL\n# Ensure the target variable is present\ntarget_variable &lt;- \"log_saleprice\"\nif (!target_variable %in% colnames(numeric_df)) {\n  stop(\"Target variable not found in numeric dataframe.\")\n}\n\n# Compute Spearman correlation matrix and p-values\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n# Extract correlation with the target variable\ntarget_corr &lt;- spearman_corr[, target_variable, drop = FALSE]\ntarget_corr &lt;- as.data.frame(target_corr)\ncolnames(target_corr) &lt;- \"Spearman_Correlation\"\n\n# Add a column indicating if the correlation could be computed\ntarget_corr$Computed &lt;- !is.na(target_corr$Spearman_Correlation)\n\n# Sort variables by Spearman correlation in descending order\nsorted_corr &lt;- target_corr[order(abs(target_corr$Spearman_Correlation), decreasing = TRUE), ]\n\n# Display sorted variables with their correlation and computed status\nprint(sorted_corr[1:10,])\n\n\n               Spearman_Correlation Computed\nlog_saleprice             1.0000000     TRUE\nOverallQual               0.8087095     TRUE\nGrLivArea                 0.7304531     TRUE\nBsmtQual                  0.6760372     TRUE\nGarageArea                0.6477811     TRUE\nFullBath_total            0.6371933     TRUE\nGarageFinish              0.6323531     TRUE\nTotalBsmtSF               0.6023901     TRUE\nYearRemodAdd              0.5688537     TRUE\nHeatingQC                 0.4906598     TRUE\n\n\nLook at the Year sold the collation is really low. the spand of the years it relly lowe, this would idicate that the price do not raise a lot over time in this area. I would have expted some kind of increase to compensate for inflation.\nBelow i have recoded the Time as months since 2006, 1 being january. I also plottet logsale price as agianst the month\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nnumeric_df$time&lt;-NA\nfor (i in 1:length(df$MoSold)){\n  numeric_df$time[i]&lt;-(numeric_df$YrSold[i]-2006)*12+numeric_df$MoSold[i]\n}\n\nyearly_means &lt;- numeric_df %&gt;%\n  group_by(time) %&gt;%\n  summarise(mean_value = mean(log_saleprice))\n\n# Plot residuals against time\nggplot(data=numeric_df[!is.na(numeric_df$log_saleprice),], aes(x=numeric_df$time, y=log_saleprice)) +\n  geom_point(alpha = 0.6, color = \"black\") +  # Individual data points\n  geom_line(data = yearly_means, aes(x = time, y = mean_value, color = \"Mean Sold Price\"), \n            size = 1, show.legend = TRUE) +\n  geom_point(data = yearly_means, aes(x = (time), y = mean_value),\n             color = \"red\", size = 2) +\n  labs(\n    x = \"Month\",\n    y = \"Value\",\n    title = \"Monthly Data with Mean Overlay\"\n  ) +\n  theme_minimal()# caluculate mean \n\n\n\n\n\n\n\n\n\nCode\nmonthly_means &lt;- numeric_df %&gt;%\n  group_by(MoSold) %&gt;%\n  summarise(mean_value = mean(log_saleprice))\n\n\n\nggplot(data=numeric_df[!is.na(numeric_df$log_saleprice),], aes(x=numeric_df$MoSold, y=log_saleprice)) +\n  geom_point(alpha = 0.6, color = \"black\") +  # Individual data points\n  geom_line(data = monthly_means, aes(x = MoSold, y = mean_value, color = \"Mean Sold Price\"), \n            size = 1, show.legend = TRUE) +\n  geom_point(data = monthly_means, aes(x = (MoSold), y = mean_value),\n             color = \"red\", size = 2) +\n  labs(\n    x = \"Month\",\n    y = \"Value\",\n    title = \"Monthly Data with Mean Overlay\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOverall their dont seam to be any big conettiction between pice and time. From the moth plot it clear if ther varinace of the price is dependen on the time ore if the salevolum changes letting to less spread."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#overallqual",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#overallqual",
    "title": "House Pricing Project",
    "section": "OverallQual",
    "text": "OverallQual\nOverallQual has high corelation with saleprice.\n\n\nCode\nggplot(data=df[!is.na(df$SalePrice),], aes(x=factor(OverallQual), y=log(SalePrice)))+\n        geom_point()+ labs(title=\"OverallQual\")\n\n\n\n\n\n\n\n\n\nlog price seam linear related to overall quality."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#ground-lvving-area",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#ground-lvving-area",
    "title": "House Pricing Project",
    "section": "Ground lvving area",
    "text": "Ground lvving area\nGround living area.\n\n\nCode\nggplot(data=df[!is.na(df$SalePrice),], aes(x=factor(GrLivArea), y=log(SalePrice)))+\n        geom_point()+ labs(title=\"GrLivArea\")"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#looking-at-some-no-numeric-varibels",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#looking-at-some-no-numeric-varibels",
    "title": "House Pricing Project",
    "section": "Looking at some no numeric varibels",
    "text": "Looking at some no numeric varibels\nSome of these varibels do not have natrual ordering, wich mean computing spearman corelation is not posibel. In that case the best option (I know) is to plot the target varibels against the cagories in the varibels, theirs differenct forms, such boxbolot ore violin plot. I have chosen violin plot since it hase more resempelens with a historgram, wich i prefere over boxplot.\n\n\nCode\nwrite.csv(df, \"data_for_final_model.csv\", row.names = FALSE)\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nnon_numeric_cols &lt;- df %&gt;%\n  select_if(~!is.numeric(.)) %&gt;%\n  colnames()\n\nfor (col in non_numeric_cols) {\n  p &lt;- ggplot(df, aes_string(x = col, y = \"log(SalePrice)\")) +  \n    geom_violin() +\n    stat_summary(fun = \"mean\", geom = \"point\", color = \"red\", size = 3) +  # Tilføj mean som rødt punkt\n    labs(title = paste(\"Violin plot for\", col, \"vs log(SalePrice)\")) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  \n  print(p)  # Udskriv violin plot for hver iteration\n}\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata_final &lt;- read.csv(\"https://raw.githubusercontent.com/Missing-almost-everywhere/Missing-almost-everywhere.io/main/Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/data_for_final_model.csv\")\n\nui &lt;- fluidPage(\n  titlePanel(\"None Numerical varibels\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"variable\", \"Select Variable:\", \n                  choices = names(data_final))\n    ),\n    mainPanel(\n      plotOutput(\"dynamicPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$dynamicPlot &lt;- renderPlot({\n    col &lt;- input$variable\n    \n    if (is.numeric(data_final[[col]])) {\n      if(col != \"SalePrice\") {\n        ggplot(data_final, aes(x = .data[[col]], y = log(SalePrice))) +\n          geom_point(alpha = 0.5) +\n          labs(title = paste(col, \"vs log(SalePrice)\")) +\n          theme_minimal()\n      } else {\n        ggplot(data_final, aes(x = .data[[col]])) +\n          geom_histogram(bins = 30, fill = 'skyblue', color = 'black', alpha = 0.7) +\n          labs(title = paste('Histogram of', col)) +\n          theme_minimal()\n      }\n    } else {\n      ggplot(data_final, aes(x = .data[[col]], y = log(SalePrice))) +\n        geom_violin(fill = 'skyblue', alpha = 0.7) +\n        stat_summary(fun = mean, geom = \"point\", color = \"red\", size = 3) +\n        labs(title = paste(col, \"vs log(SalePrice)\")) +\n        theme_minimal() +\n        theme(axis.text.x = element_text(angle = 45, hjust = 1))\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\nSo what I am looking for is something their it seam like the means i sale price is different between the factors. Neighborhood, seam like a good varibel to include, in many intances on can see that the their is not even a overlab in the observed price.\nThe roofing seam like it could have potesial"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#base-line-mocel",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#base-line-mocel",
    "title": "House Pricing Project",
    "section": "Base line mocel",
    "text": "Base line mocel\nAfter Plotting different models and looking the residual i found the below model. Where the saleprice pr sqft of living area is combination the Neighborhood the type of dwelling (MSSubClass).\nWich could be good candidate if one look the residuals being drawn from af t distribusion. So more heavy tale distribution, than a normal distribution. In many ways this model seam resanbull from a intuition point of view.\n\n\nCode\n# Plot original data\n\nmodel_baseline &lt;-lm((SalePrice/GrLivArea) ~ TotalBsmtSF*MSSubClass:Neighborhood:GrLivArea , data = df) # lm((SalePrice / GrLivArea) ~ TotalBsmtSF+MSSubClass+Neighborhood,data=df)\n\n\nplot(df$SalePrice,log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea))\n\n\n\n\n\n\n\n\n\nCode\nhist(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea),breaks=50)\n\n\n\n\n\n\n\n\n\nCode\n# The histogram almost look like a t distribution\nlibrary(MASS)\nfitdistr(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea), \"t\",start = list(m=mean(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea)),s=sd(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea)), df=2),lower=c(-1, 0.001,1))\n\n\n        m              s              df     \n  -0.009049068    0.156394619    4.526172860 \n ( 0.004804025) ( 0.005221797) ( 0.565497406)\n\n\nCode\nlibrary(Dowd)\nTQQPlot(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea),df=4.5 )"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#a-linear-interaction-model.",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#a-linear-interaction-model.",
    "title": "House Pricing Project",
    "section": "A linear interaction model.",
    "text": "A linear interaction model.\nBelove I have fitted a interaction model where log saleprice is a interactiv funtion between, The type of house, the neighborhood and the size of the dewelling+ sum effect from the size of the basement. It seam reasonbell and is somewat how I would estimate a type of house for the overall price.\n\n\nCode\nModel_second_baseline&lt;- lm(log(SalePrice)~TotalBsmtSF+MSSubClass:GrLivArea:Neighborhood,data=df)\n\n\nlibrary(Dowd)\nTQQPlot(Model_second_baseline$residuals,df=4.5 )\n\n\n\n\n\n\n\n\n\nCode\n# Define the custom metric for CV evaluation\nlog_diff_metric_model_baseline &lt;- function(data, label) {\n  groups=unique(label)\n  groups_score=rep(NA,length(groups))\n  \n  for (i in 1:length(groups)){\n    groupe_for_test=groups[i]\n    partion_vector_train &lt;- which(label!= groupe_for_test)\n    partion_vector_test &lt;- which(label== groupe_for_test)\n    train_data &lt;- data[partion_vector_train, ] \n    test_data &lt;- data[partion_vector_test, ] \n    # fit model \n    model_baseline &lt;-lm(log(SalePrice)~TotalBsmtSF+MSSubClass:GrLivArea:Neighborhood,data=train_data)\n    # get predition \n    predicted_SalePrice &lt;- predict(model_baseline, newdata = test_data) \n    # get log diff\n    log_diff &lt;- log(test_data$SalePrice) - (predicted_SalePrice)\n    # save results\n    groups_score[i]&lt;-sum(log_diff**2)\n  }\n  \n  return(sum(groups_score)/length(data$SalePrice))\n}\n\n# make labels\n\nn_cv=50\n\nlabels=sample(rep(seq(1,n_cv),floor(length(df$Id)/n_cv)))\nif(length(df$Id)-floor(length(df$Id)/n_cv)*n_cv!=0){\n  labels&lt;-c(labels,seq(1:(length(df$Id)-floor(length(df$Id)/n_cv)*n_cv)))\n}\nlabels=sample(labels)\n\n\nprint(\"RMSE of log prices\")\n\n\n[1] \"RMSE of log prices\"\n\n\nCode\nprint(log_diff_metric_model_baseline(df,labels))\n\n\n[1] 0.06427054\n\n\nOverall, the predictive strength looks good, but the residuals of the model resemble a t-distribution more than a normal distribution."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#lasso",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#lasso",
    "title": "House Pricing Project",
    "section": "Lasso",
    "text": "Lasso\nIn this chunck of code i fit lasso model for the dataframe, by asuming a linear model over all the vearibels in the datafram penlized by the size of beta. The Lasso solves the following\n\\[\\underset{\\beta}{argmin} ||log(Y)-\\beta X||_{2}^2+\\lambda||\\beta||_1\\].\nIt penalizes the size of the coefficients (beta). Since the penalty is based on the \\(L_1\\) -norm, it tends to set some coefficients to zero. This encourages sparse solutions, where only the most relevant features are retained.\n\n\nCode\nlibrary(glmnet)\n\n# Prepare the data\ndf_lasso &lt;- df\nlog_saleprice &lt;- log(df_lasso$SalePrice)\ndf_lasso$SalePrice &lt;- NULL\ndf_lasso$log_saleprice &lt;- log_saleprice  # Add log_saleprice to df_lasso\ndf_lasso_id&lt;-df_lasso$Id\ndf_lasso$Id&lt;-NULL\n# Remove rows with NA values\ndf_lasso &lt;- na.omit(df_lasso)\n\n# Split data into folds\nk &lt;- 10\nfolds &lt;- sample(1:k, size = nrow(df_lasso), replace = TRUE)\n\n# Initialize a vector to store MSEs for each fold\ncv_mse &lt;- numeric(k)\nbest_lambda_vector &lt;- rep(NA, k)\n\nfor (i in 1:k) {\n  # Split data into training and test sets\n  train_indices &lt;- which(folds != i)\n  test_indices &lt;- which(folds == i)\n  \n  train_data &lt;- df_lasso[train_indices, ]\n  test_data &lt;- df_lasso[test_indices, ]\n  \n  # Create design matrices\n  X_train &lt;- model.matrix(log_saleprice ~ . , data = train_data)\n  X_test &lt;- model.matrix(log_saleprice ~ . , data = test_data)\n  \n  # Fix column mismatch between X_train and X_test\n  common_columns &lt;- intersect(colnames(X_train), colnames(X_test))\n  X_test &lt;- X_test[, common_columns, drop = FALSE]\n  X_train &lt;- X_train[, common_columns, drop = FALSE]\n  \n  # Fit LASSO model with cross-validation to choose lambda\n  lasso_model &lt;- cv.glmnet(X_train, train_data$log_saleprice, alpha = 1, family = \"gaussian\")\n  best_lambda &lt;- lasso_model$lambda.min\n  best_lambda_vector[i] &lt;- best_lambda\n  \n  # Predict on test data\n  predictions &lt;- predict(lasso_model, s = best_lambda, newx = X_test)\n  \n  # Compute MSE for this fold\n  cv_mse[i] &lt;- mean((test_data$log_saleprice - predictions)^2)\n}\n\n# Average MSE across all folds\nmean_cv_mse &lt;- mean(cv_mse)\ncat(\"Mean Cross-Validated MSE:\", mean_cv_mse, \"\\n\")\n\n\nMean Cross-Validated MSE: 0.02111107 \n\n\nCode\n# Fit final LASSO model with the average best lambda\nmean_best_lambda &lt;- mean(best_lambda_vector, na.rm = TRUE)  # Calculate mean lambda\nX_full &lt;- model.matrix(log_saleprice ~ . , data = df_lasso)\nfinal_lasso_model &lt;- glmnet(X_full, log_saleprice, alpha = 1, lambda = mean_best_lambda, family = \"gaussian\")\n\n# Print final model coefficients\nprint(mean_cv_mse)\n\n\n[1] 0.02111107\n\n\nAs observed, the predictive strength of the model is better than the baseline model, but not by a lot in terms of predictive performance."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#xgboost",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#xgboost",
    "title": "House Pricing Project",
    "section": "XGBoost",
    "text": "XGBoost\nXGBoost is an ensemble learning method that uses repeated decision trees, where each tree is fitted on the residuals of the previous one. Essentially, it’s a sophisticated method of partitioning the dataset.\nThis approach has gained significant popularity, so I decided to give it a try. Since XGBoost relies on tree structures for partitioning, the data must have an inherent ordering. To include categorical variables, I applied one-hot encoding to transform them into a suitable format.\n\n\nCode\nlibrary(xgboost)\nlibrary(data.table)\n\n# Prepare the dataset\ndf_xgb &lt;- df\ntarget &lt;- log(df_xgb$SalePrice)  # Log-transform the target variable\ndf_xgb$SalePrice &lt;- NULL         # Remove the target variable from features\ndf_xgb_Id &lt;- df_xgb$Id\ndf_xgb$Id &lt;- NULL\n\n# One-hot encode categorical variables\ndf_xgb &lt;- as.data.table(df_xgb)\ndf_xgb &lt;- model.matrix(~ . - 1, data = df_xgb)  # Perform one-hot encoding and remove intercept\n\n# Create DMatrix\nddata &lt;- xgb.DMatrix(data = df_xgb, label = target)\n\n# Set parameters for the XGBoost model\nparams &lt;- list(\n  objective = \"reg:squarederror\",  # Regression task\n  eta = 0.1,                       # Learning rate\n  max_depth = 6,                   # Maximum depth of trees\n  subsample = 0.8,                 # Subsampling ratio\n  colsample_bytree = 0.8           # Feature subsampling ratio\n)\n\n# Perform 10-fold cross-validation\nset.seed(123)  # For reproducibility\ncv_results &lt;- xgb.cv(\n  params = params,\n  data = ddata,\n  nrounds = 100,                    # Number of boosting rounds\n  nfold = 10,                       # Number of folds for cross-validation\n  metrics = \"rmse\",                 # Evaluation metric\n  early_stopping_rounds = 10,       # Stop early if no improvement\n  print_every_n = 10,               # Print progress every 10 rounds\n  verbose = TRUE\n)\n\n\n[1] train-rmse:10.379960+0.002592   test-rmse:10.379961+0.026888 \nMultiple eval metrics are present. Will use test_rmse for early stopping.\nWill train until test_rmse hasn't improved in 10 rounds.\n\n[11]    train-rmse:3.637180+0.001203    test-rmse:3.637569+0.017285 \n[21]    train-rmse:1.286535+0.000669    test-rmse:1.287011+0.011197 \n[31]    train-rmse:0.469571+0.000640    test-rmse:0.476945+0.007929 \n[41]    train-rmse:0.190574+0.000775    test-rmse:0.215138+0.012590 \n[51]    train-rmse:0.102444+0.000572    test-rmse:0.147398+0.018300 \n[61]    train-rmse:0.075914+0.000609    test-rmse:0.132825+0.019453 \n[71]    train-rmse:0.065099+0.000603    test-rmse:0.129387+0.019381 \n[81]    train-rmse:0.057990+0.001078    test-rmse:0.128064+0.019680 \n[91]    train-rmse:0.052257+0.001140    test-rmse:0.127166+0.019725 \n[100]   train-rmse:0.047739+0.001064    test-rmse:0.126684+0.019586 \n\n\nCode\n# Extract the best number of boosting rounds\nbest_nrounds &lt;- cv_results$best_iteration\ncat(\"Best number of rounds:\", best_nrounds, \"\\n\")\n\n\nBest number of rounds: 98 \n\n\nCode\n# Train final model using the best number of rounds\nfinal_model &lt;- xgb.train(\n  params = params,\n  data = ddata,\n  nrounds = best_nrounds\n)\n\n# Evaluate final model on entire dataset (or split as desired)\npredictions &lt;- predict(final_model, ddata)\noverall_rmse &lt;- sqrt(mean((target - predictions)^2))\ncat(\"RMSE on entire dataset:\", overall_rmse, \"\\n\")\n\n\nRMSE on entire dataset: 0.05299007"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#prepration-of-test-set.",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#prepration-of-test-set.",
    "title": "House Pricing Project",
    "section": "Prepration of test set.",
    "text": "Prepration of test set.\nThere are some missing values, which I address using the K-Nearest Neighbors (KNN) algorithm. This method is particularly effective for handling missing values in categorical variables.\n\n\nCode\nprint(\"df_test_preformence\")\n\n\n[1] \"df_test_preformence\"\n\n\nCode\ntable(df_test_preformence$MSSubClass)\n\n\n\n 20  30  45  50  60  70  75  80  90 120 150 160 180 190 \n543  70   6 145 276  68   7  88  57  95   1  65   7  31 \n\n\nCode\nprint(\"df\")\n\n\n[1] \"df\"\n\n\nCode\ntable(df$MSSubClass)\n\n\n\n 20  30  45  50  60  70  75  80  90 120 160 180 190 \n532  67  12 148 296  60  16  78  52  86  63  10  30 \n\n\n\n\nCode\ndf_test_preformence$MSSubClass[df_test_preformence$MSSubClass==150]=160\n\n\nmissing values\n\n\nCode\nvis_miss(df_test_preformence)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(VIM)\n\n\nWarning: package 'VIM' was built under R version 4.4.2\n\n\nLoading required package: colorspace\n\n\nLoading required package: grid\n\n\nVIM is ready to use.\n\n\nSuggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues\n\n\n\nAttaching package: 'VIM'\n\n\nThe following object is masked from 'package:bootstrap':\n\n    diabetes\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\n\nCode\n# Impute missing values using KNN\nimputed_data &lt;- kNN(df_test_preformence, k = 5)\n\n# the imputed_data ad a new colum to say what vaules is imputed i am remvoving them.\ndf_test_preformence&lt;-imputed_data[,1:dim(df_test_preformence)[2]]\n\n# Check the updated dataframe\ndim(df_test_preformence)\n\n\n[1] 1459   54\n\n\nTheir is one Plywood in test set\n\n\nCode\nsetdiff(unique(df$Exterior2nd),unique(df_test_preformence$Exterior2nd))\n\n\n[1] \"plywood\"\n\n\nCode\ntable(df_test_preformence$Exterior2nd)\n\n\n\nAsbShng AsphShn Brk Cmn BrkFace CemntBd CmentBd HdBoard ImStucc MetalSd Plywood \n     18       1      15      22       2      66     199       5     233     128 \n  Stone  Stucco VinylSd Wd Sdng Wd Shng \n      1      21     511     194      43 \n\n\nCode\ntable(df$Exterior2nd)\n\n\n\nAsbShng AsphShn Brk Cmn BrkFace CemntBd CmentBd HdBoard ImStucc MetalSd plywood \n     20       3       7      25       1      58     207      10     213       1 \nPlywood   Stone  Stucco VinylSd Wd Sdng Wd Shng \n    142       4      26     499     196      38 \n\n\n\n\nCode\nX_full &lt;- model.matrix(log_saleprice ~ . , data = df_lasso)\nfinal_lasso_model &lt;- glmnet(X_full, df_lasso$log_saleprice, alpha = 1, lambda = mean_best_lambda, family = \"gaussian\")\n\ndf_lasso_evalatio &lt;- df_test_preformence\ndf_lasso_evalatio_id&lt;-df_lasso_evalatio$Id\ndf_lasso_evalatio$Id&lt;-NULL\n# Create design matrix for prediction from test data\nX_predict &lt;- model.matrix(~., data = df_lasso_evalatio)\n\n# Identify any missing columns between X_full and the prediction data (X_predict)\nmissing_cols &lt;- setdiff(colnames(X_full), colnames(X_predict))\n\n# Add missing columns to X_predict with zero values (align columns)\nX_predict &lt;- cbind(X_predict, matrix(0, nrow = nrow(X_predict), ncol = length(missing_cols),\n                                     dimnames = list(NULL, missing_cols)))\n\n# Ensure column order matches X_full\nX_predict &lt;- X_predict[, colnames(X_full)]\n\n# Predict log_saleprice for the test dataset\npredictions &lt;- predict(final_lasso_model, newx = X_predict)\n\n# Convert log predictions to SalePrice scale (exp of log predictions)\nSalePrice_pred &lt;- exp(c(predictions))\n\n# Create a data frame with Id and SalePrice\nprediction_df &lt;- data.frame(Id = df_lasso_evalatio_id, SalePrice = SalePrice_pred)\n\n# Write predictions to a CSV file\n#write.csv(prediction_df, \"sale_prices.csv\", row.names = FALSE)\n\n# Check the length of SalePrice predictions\nlength(prediction_df$SalePrice)\n\n\n[1] 1459"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html",
    "title": "House Pricing Project",
    "section": "",
    "text": "This is practics case based on some data from kaggle\nI mostly made this to play around with some data and learn new methods. When working with data, like many others, I like to use either R Markdown or Jupyter Notebook. I also enjoy writing small notes and comments so that if I need to revisit the project later, I can easily see what I did. This approach is really helpful in terms of reproducibility. Unfortunately, people often share notebooks without adding any text or comments.\nWhile I’ve written out some comments here, I don’t go too deep. However, I do show parts of my workflow. The code is written in R, and I consider it what I call “one-time code.” It’s not meant to be read by others, so there aren’t a lot of comments. It’s not optimized either—I’ve simply chosen the fastest code I could think of or find to solve a problem. I ende op using Chat-gbt for some code aswell then correcting the mistakes. The methology and the models chosen is all me."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#comments-on-available-data",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#comments-on-available-data",
    "title": "House Pricing Project",
    "section": "Comments on Available Data",
    "text": "Comments on Available Data\nAll the houses are located in the Ames city area. There is information on the month and year sold. (In both the training and test sets, the years have the same values, meaning there’s no need to estimate values forward in time. This is a bit unusual—what is the purpose of the model in such a case?) Most of the data is categorical or discrete in nature. Some of the data has a natural order or rank. When examining the available data, it’s worth noting that there is no information on earlier sale prices—why is this missing? Information on “time on market” is also missing."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#data-cleaning-and-exploration",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#data-cleaning-and-exploration",
    "title": "House Pricing Project",
    "section": "Data cleaning and exploration",
    "text": "Data cleaning and exploration\n\n\nCode\nlibrary(readr)\ndf=as.data.frame(read.csv(\"train.csv\"))\n\n#split &lt;- sample(1:nrow(df), size = 0.8 * nrow(df))\n\n# Create training and testing sets\n#df &lt;- df[split, ]  # 80% of the data\n#df_test_for_choice &lt;- df[-split, ]  # other 20%\n\n#\ndf_test_preformence=as.data.frame(read.csv(\"test.csv\"))"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#recoding-na",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#recoding-na",
    "title": "House Pricing Project",
    "section": "Recoding NA",
    "text": "Recoding NA\nIn the description, I noticed that some of the variables use NA as a category.\nVaribels where NA is a catgori is listed below.\n\nAlley\nBsmtQual\nBsmtCond\nBsmtExposure\nBsmtFinType1\nBsmtFinType2\nFireplaceQu\nGarageType\nGarageFinish\nGarageQual\nGarageCond\nPoolQC\nFence\nMiscFeature\n\nThese variables need to be recoded, as the values are not missing—they are simply in the wrong format.\n\n\nCode\n#change \nchang_to_df&lt;-function(DF){\n  DF$Alley&lt;-DF$Alley[is.na(DF$Alley)] &lt;- \"No alley\"\n  DF$BsmtQual[is.na(DF$BsmtQual)]&lt;-\"No Basement\"\n  DF$BsmtCond[is.na(DF$BsmtCond)]&lt;-\"No Basement\"\n  DF$BsmtExposure[is.na(DF$BsmtExposure)]&lt;-\"No Basement\"\n  DF$BsmtFinType1[is.na(DF$BsmtFinType1)]&lt;- \"No Basement\"\n  DF$BsmtFinType2[is.na(DF$BsmtFinType2)]&lt;- \"No Basement\"\n  DF$FireplaceQu[is.na(DF$FireplaceQu)]&lt;-\"No Fireplace\"\n  DF$GarageType[is.na(DF$GarageType)]&lt;-\"No Garage\"\n  DF$GarageFinish[is.na(DF$GarageFinish)]&lt;-\"No Garage\"\n  DF$GarageQual[is.na(DF$GarageQual)]&lt;-\"No Garage\"\n  DF$GarageCond[is.na(DF$GarageCond)]&lt;-\"No Garage\"\n  DF$GarageYrBlt[is.na(DF$GarageYrBlt)]&lt;-0 # all values that is NA corespond wither other showing that their is no garage.\n  DF$PoolQC[is.na(DF$PoolQC)]&lt;-\"No Pool\"\n  DF$Fence[is.na(DF$Fence)]&lt;-\"No Fence\"\n  DF$MiscFeature[is.na(DF$MiscFeature)]&lt;-\"None\"\n  return(DF)\n}\n\ndf&lt;-chang_to_df(df)\n#df_test_for_choice&lt;-chang_to_df(df_test_for_choice)\ndf_test_preformence&lt;-chang_to_df(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#examining-missing-values",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#examining-missing-values",
    "title": "House Pricing Project",
    "section": "Examining Missing Values",
    "text": "Examining Missing Values\nAfter recoding cases where NA does not represent missing values, we will now analyze the actual missing values.\nBelow is a plot showing the missing values. I had to split the plot into two parts; otherwise, the variable names would not be readable.\n\n\nCode\nlibrary(naniar)\nvis_miss(df[,1:40])\n\n\n\n\n\n\n\n\n\nCode\nvis_miss(df[,40:81])\n\n\n\n\n\n\n\n\n\nLotFrontage missing values look weird.\nThe definition is Linear feet of street connected to property. Below i have printed uniqe entreances in LotFrontage.\n\n\nCode\ntable((df$LotFrontage))\n\n\n\n 21  24  30  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48 \n 23  19   6   5   1  10   9   6   5   1   1  12   6   4  12   9   3   1   5   6 \n 49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68 \n  4  57  15  14  10   6  17   5  12   7  13 143   8   9  17  19  44  15  12  19 \n 69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88 \n 11  70  12  17  18  15  53  11   9  25  17  69   6  12   5   9  40  10   5  10 \n 89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 \n  6  23   6  10   8   6   7   8   2   8   3  16   2   4   3   3   6   1   7   3 \n109 110 111 112 114 115 116 118 120 121 122 124 128 129 130 134 137 138 140 141 \n  2   6   1   1   2   2   2   2   7   2   2   2   1   2   3   2   1   1   1   1 \n144 149 150 152 153 160 168 174 182 313 \n  1   1   1   1   1   1   1   2   1   2 \n\n\nFrom the definition, it could refer to farms, but there are no farms in the dataset.\nIf we plot the house types against the number of missing values, we see that most of them come from single-family detached houses.\n\n\nCode\nlibrary(dplyr)\n# Test farm theory\ntest_LotFrontage &lt;- df$LotFrontage\ntest_LotFrontage[is.na(test_LotFrontage)] &lt;- 0\ntest_BldgType &lt;- factor(df$BldgType)\n\n\n\n# Create a data frame\ndata &lt;- data.frame(test_BldgType, test_LotFrontage)\n\nzero_counts &lt;- data %&gt;%\n  filter(test_LotFrontage == 0) %&gt;%\n  count(test_BldgType)\n\n# Create the bar plot with category labels at the bottom\nbarplot(zero_counts$n, \n        names.arg = zero_counts$test_BldgType,  # Use the correct column for labels\n        main = \"Number of Zeros by Category\",\n        xlab = \"Categories\",\n        ylab = \"Count of Zeros\",\n        col = \"lightblue\",\n        las = 2)  # Rotate labels to vertical\n\n\n\n\n\n\n\n\n\nIt seems like most of the missing values are from single-family detached houses, so it’s unrealistic to assume there is no street connected to the property. This means setting the values to zero is likely a bad option.\nOptions for imputation:\nLinear regression based on other variables. k-nearest neighbors. I could also disregard this variable since I doubt it has high predictive power.\nIt seems like an interactive linear model, incorporating the interaction between LotArea and LotShape, would provide good imputation (this also makes sense conceptually).\n\n\nCode\nImputasion_model &lt;- lm(LotFrontage ~ LotArea * LotShape, df)\n\n# Plot fitted values vs. residuals\nplot(Imputasion_model$fitted.values, Imputasion_model$residuals,\n     xlab = \"Fitted Values\",\n     ylab = \"Residuals\",\n     main = \"Residuals plot (LotFrontage ~ LotArea * LotShape)\")\n\n# Add a horizontal line at the mean of the residuals (which should be zero)\nabline(h = mean(Imputasion_model$residuals), col = \"red\", lwd = 2, lty = 2)\nmean_value &lt;- mean(Imputasion_model$residuals)\ntext(x = max(Imputasion_model$fitted.values), \n     y = mean_value, \n     labels = \"Mean\", \n     col = \"red\")\n\n\n\n\n\n\n\n\n\nMissing values for LotFrontage will be imputed using the linear model described above.\nThis also means that if LotFrontage is used in the model, the imputation will need to be performed on the test set as well. I have internally debated whether I should include the test set when building the imputation model—essentially, creating a model based on both the training and test sets for this purpose.\nSince I’m a little new to Kaggle, if the test set contains covariates and I only need to upload the fitted values, I would proceed with this approach. Otherwise, I would not. For now, I will just use the model based only on the training set.\n\n\nCode\n# imputatsion\nImputasion_model &lt;- lm(LotFrontage ~ LotArea * LotShape, df)\n\nInput_value&lt;-function(DF){\n  for (i in 1:length(DF$LotFrontage)){\n    if(is.na(DF$LotFrontage[i])){\n      DF$LotFrontage[i]&lt;-predicted_value &lt;- predict(Imputasion_model, newdata = list(\n        LotArea = DF$LotArea[i],\n        LotShape = DF$LotShape[i] ))\n    }\n  }\n  return(DF)\n  }\n\n#input df for all input values\ndf&lt;-Input_value(df)\n#df_test_for_choice&lt;-Input_value(df_test_for_choice)\ndf_test_preformence&lt;-Input_value(df_test_preformence)\n\n\nThere are still 1% of missing values in MasVnrArea and MasVnrType. I will discard the last row containing these missing values."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#recoding-of-variables",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#recoding-of-variables",
    "title": "House Pricing Project",
    "section": "Recoding of Variables",
    "text": "Recoding of Variables\nSome of these variables have a ranking, which makes them well-suited for recoding. By recoding them as numerical values, the ranking becomes more obvious.\nNote: This GPT is best used alongside the editGPT Browser extension.\n\n\nClick to expand/collapse varibel recoeding\n\nTo give a exampel of the recoding. PoolQC: Pool quality\n   Ex   Excellent         (4)\n   Gd   Good              (3)\n   TA   Average/Typical   (2)   (TA is never obsevered in pool varibel for traning set)\n   Fa   Fair              (1)\n   NA   No Pool           (0)\ncan be codes as 0-4\nBelowe here i have made list of how the varibels is code. I have made is so no pressent cagories NA is code to 0. Meaning 0 is only a value if the item is not pressent. and otherwise it starts from 1.\nLotShape: General shape of property Reg Regular (4) IR1 Slightly irregular (3) IR2 Moderately Irregular (2) IR3 Irregular (1)\nLandContour: Flatness of the property\n   Lvl  Near Flat/Level                                                     (4)\n   Bnk  Banked - Quick and significant rise from street grade to building   (3)\n   HLS  Hillside - Significant slope from side to side                      (2)\n   Low  Depression                                                          (1)\nLandSlope: Slope of property\n   Gtl  Gentle slope    (3)\n   Mod  Moderate Slope  (2)\n   Sev  Severe Slope    (1)\nExterQual: Evaluates the quality of the material on the exterior\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\n   \nExterCond: Evaluates the present condition of the material on the exterior\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\n   \nBsmtQual: Evaluates the height of the basement\n   Ex   Excellent (100+ inches)   (5)\n   Gd   Good (90-99 inches)       (4)\n   TA   Typical (80-89 inches)    (3)\n   Fa   Fair (70-79 inches)       (2)\n   Po   Poor (&lt;70 inches          (1)\n   NA   No Basement               (0)\nBsmtCond: Evaluates the general condition of the basement\n   Ex   Excellent                                     (5)\n   Gd   Good                                          (4)\n   TA   Typical - slight dampness allowed             (3)\n   Fa   Fair - dampness or some cracking or settling  (2)\n   Po   Poor - Severe cracking, settling, or wetness  (1)\n   NA   No Basement                                   (0)\nBsmtExposure: Refers to walkout or garden level walls\n   Gd   Good Exposure                                                               (4)\n   Av   Average Exposure (split levels or foyers typically score average or above)  (3) \n   Mn   Mimimum Exposure                                                            (2)\n   No   No Exposure                                                                 (1)\n   NA   No Basement                                                                 (0)\nBsmtFinType1: Rating of basement finished area\n   GLQ  Good Living Quarters            (6)\n   ALQ  Average Living Quarters         (5)\n   BLQ  Below Average Living Quarters     (4)\n   Rec  Average Rec Room                (3)\n   LwQ  Low Quality                     (2)\n   Unf  Unfinshed                       (1)\n   NA   No Basement                       (0)\nBsmtFinType2: Rating of basement finished area (if multiple types)\n   GLQ  Good Living Quarters            (6)\n   ALQ  Average Living Quarters         (5)\n   BLQ  Below Average Living Quarters     (4)\n   Rec  Average Rec Room                (3)\n   LwQ  Low Quality                     (2)\n   Unf  Unfinshed                       (1)\n   NA   No Basement                       (0)\nHeatingQC: Heating quality and condition\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\nKitchenQual: Kitchen quality\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\nFunctional: Home functionality (Assume typical unless deductions are warranted)\n   Typ  Typical Functionality   (7)\n   Min1 Minor Deductions 1      (6)\n   Min2 Minor Deductions 2      (5)\n   Mod  Moderate Deductions     (4)\n   Maj1 Major Deductions 1      (3)\n   Maj2 Major Deductions 2      (2)\n   Sev  Severely Damaged        (1)\n   Sal  Salvage only            (0)\n   \nFireplaceQu: Fireplace quality\n   Ex   Excellent - Exceptional Masonry Fireplace                                               (5)\n   Gd   Good - Masonry Fireplace in main level                                                  (4)\n   TA   Average - Prefabricated Fireplace in main living area or Masonry Fireplace in basement  (3)\n   Fa   Fair - Prefabricated Fireplace in basement                                              (2)\n   Po   Poor - Ben Franklin Stove                                                               (1)\n   NA   No Fireplace                                                                            (0)\nGarageFinish: Interior finish of the garage\n   Fin  Finished        (3)\n   RFn  Rough Finished  (2) \n   Unf  Unfinished      (1)\n   NA   No Garage         (0)\n   \nGarageQual: Garage quality\n   Ex   Excellent         (5)\n   Gd   Good              (4)\n   TA   Typical/Average   (3)\n   Fa   Fair              (2)\n   Po   Poor              (1)\n   NA   No Garage         (0)\nGarageCond: Garage condition\n   Ex   Excellent         (5)\n   Gd   Good              (4)\n   TA   Typical/Average   (3)\n   Fa   Fair              (2)\n   Po   Poor              (1)\n   NA   No Garage         (0)\nPoolQC: Pool quality\n   Ex   Excellent         (4)\n   Gd   Good              (3)\n   TA   Average/Typical   (2)   (TA is never obsevered in pool varibel for traning set)\n   Fa   Fair              (1)\n   NA   No Pool           (0)\n   \nFence: Fence quality\n   GdPrv    Good Privacy    (4)\n   MnPrv    Minimum Privacy (3)\n   GdWo Good Wood         (2)\n   MnWw Minimum Wood/Wire (1)\n   NA   No Fence            (0)\nTheir properly also some rank to other varibels, varibels like building matrials must have a ranking in terms of price. But i dont have any idear about whese.\n\n\n\nCode\n# Reencoding can be don via match.\n\n# recoding\nrecoding&lt;-function(DF){\n  DF$LotShape&lt;-match(DF$LotShape,c(\"IR3\",\"IR2\",\"IR1\",\"Reg\"))\n  #DF$LandContour&lt;-match(DF$LandContour,c(\"Low\",\"HLS\",\"Bnk\",\"Lvl\"))\n  #DF$LandSlope&lt;-match(DF$LandSlope,c(\"Sev\",\"Mod\",\"Gtl\"))\n  DF$ExterQual&lt;-match(DF$ExterQual,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$ExterCond&lt;-match(DF$ExterCond,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$BsmtQual&lt;-match(DF$BsmtQual,c(\"No Basement\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$BsmtCond&lt;-match(DF$BsmtCond,c(\"No Basement\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$BsmtExposure&lt;-match(DF$BsmtExposure,c(\"No Basement\",\"No\",\"Mn\",\"Av\",\"Gd\"))-1\n  DF$BsmtFinType1&lt;-match(DF$BsmtFinType1,c(\"No Basement\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"))-1\n  DF$BsmtFinType2&lt;-match(DF$BsmtFinType2,c(\"No Basement\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"))-1\n  DF$HeatingQC&lt;-match(DF$HeatingQ,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$KitchenQual&lt;-match(DF$KitchenQual,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$Functional&lt;-match(DF$Functional,c(\"Sal\",\"Sev\",\"Maj2\",\"Maj1\",\"Mod\",\"Min2\",\"Min1\",\"Typ\"))-1\n  DF$FireplaceQu&lt;-match(DF$FireplaceQu,c(\"No Fireplace\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$GarageFinish&lt;-match(DF$GarageFinish,c(\"No Garage\",\"Unf\",\"RFn\",\"Fin\"))-1\n  DF$GarageQual&lt;-match(DF$GarageQual,c(\"No Garage\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$GarageCond&lt;-match(DF$GarageCond,c(\"No Garage\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$PoolQC&lt;-match(DF$PoolQC, c(\"No Pool\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  #DF$Fence&lt;-match(DF$Fence,c(\"No Fence\",\"MnWw\",\"GdWo\",\"MnPrv\",\"GdPrv\"))-1\n  return(DF)\n}\n\ndf&lt;-recoding(df)\n#df_test_for_choice&lt;-recoding(df_test_for_choice)\ndf_test_preformence&lt;-recoding(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#feature-transformations",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#feature-transformations",
    "title": "House Pricing Project",
    "section": "Feature transformations",
    "text": "Feature transformations\nIn this section i will list the varibels i have change and what the changes was.\n\n\nClick to expand/collapse varibel Changes\n\nLooking at the description of the data the varibesl with lowe count, is\n\n40: 1-STORY W/FINISHED ATTIC ALL AGES\n45: 1-1/2 STORY - UNFINISHED ALL AGES\n75: 2-1/2 STORY ALL AGES\n85: SPLIT FOYER\n185: PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n\nHere is what i will combine them to.\n40 will be combined with 50 - 50: 1-1/2 STORY FINISHED ALL AGES Making 1-1/2 STORY FINISHED ALL AGES\n85 will be combined with 80 - 80: SPLIT OR MULTI-LEVEL Making Split\n185 will be combined with 160 - 160: 2-STORY PUD - 1946 & NEWER Making PUD - MULTILEVEL\nThis only leaves 45: UNFINISHED ALL AGES\nwith a vary small number of obsevations, in this section we looking at the data but this kind of asumption will affect models choice, what is the value of unfinished house, alle ages. A unfinish house can either be a cheap way to get new house ore a extra expens since it proberly should be removed. Both of these would proberly have lower value than the 1-1/2 STORY FINISHED ALL AGES. So if i inclued them i can drag down the estimat for this groups, and this can be unproporsional if the model is fitted with least sqaur. If i dont comined it with a varibel want to use a interaction effect in linear model I will get model wiht a lot NA, where this the combinations is not represented, i can get around this by tackling overide the model, so basically get to make geuss in whose case, that could be the average house price, something more cleaver, so for now i will leave it in. If i have to do the overird i will tell, this is better than a zero score. In the total linear model this is not a problem, so i may be the case this not problem.\n\nYearBuilt Som of this varibel is allready pressent in MSSubClass\nGarageCars I have think about, it would give nice distribution if it was seplified belove ore equalt 2 cars with true and false. but i think their is more infomation in o zeror so the the varibel vill be zero, one, two, above 2\n\nLooking at this it seam like it would be a god idear to cagaorise/colabse some of these varibel. This is to try to simplify the information. The problem that can arise is that with some of these varibels, they have such ueven distribution that, one can end up fitting a combination to uniqe house. this could be good fine but, in some case by colsaping one can better overall predictiv results. Later I will proberly reduce the dimension based on either spearman ore kendall tau corelation so one would want to ensure that if their is rank in the data is preserved. To give a exampel number of fireplace can be reduces to Yes or No. Yes would stille be higer than No.\n\nfireplace siplified to binary yes and NO\nPoolArea will be change to yes no their is not alot of information -pool yes No\n\nFor these i could it could potensiel be a good idear to eihter make fullbath total Halfbath total. ore colabs them in to yes no\n\nFullbath\nHalfbath\n\nbsmtFullBath\nBsmtHalfBath\nFenche will be change to True ore FAlSE\nSaleCondition will be normal True ore False\nLandContour will be change to level ore not. I will stil count this haveing a order, meaning level wich is equal to one is prefered\nLandSlope will be change to ground level ore not.\n\nFoundation will be combined by combin all other than “PConc” ore “CBlock” to “other”\nLotConfig all other than Corner Inside will be combined\nRoofStyle will be “Gable” or “other”\nThe one data point in MiscFeature that is tenis court will go under other\nIn Condition1 the following RRNe Within 200’ of East-West Railroad RRAe Adjacent to East-West Railroad RRNn Within 200’ of North-South Railroad RRAn Adjacent to North-South Railroad\nWill be combied to (Near Railroad)\nIn the Exterior1st and Exterior2nd. Their is some catagories with single obsevations in, I will put them in the closet cagori.\n\nI have rapt all the change in a function so it esay to aplly to the test dataset aswell\n\n\nCode\nmodify_df &lt;- function(DF){\n  DF$MSSubClass[DF$MSSubClass == 40] &lt;- 50\n  DF$MSSubClass[DF$MSSubClass == 85] &lt;- 80\n  DF$MSSubClass[DF$MSSubClass == 185] &lt;- 160\n  DF$have_Garage&lt;-DF$GarageCars&gt;0\n  DF$GarageCars&lt;-NULL\n  # is Fireplace pressent\n  DF$Fireplaces_present&lt;-DF$Fireplaces&gt;0\n  DF$Fireplaces&lt;-NULL\n  DF$FireplaceQu&lt;-NULL\n  # is pool pressent\n  DF$pool_present&lt;-DF$PoolArea&gt;0\n  DF$PoolArea&lt;-NULL\n  # Full bath pressent above ground\n  DF$FullBath_total&lt;-DF$BsmtFullBath+DF$FullBath\n  DF$BsmtFullBath&lt;-NULL\n  DF$FullBath&lt;-NULL\n  DF$HalfBath_total&lt;-DF$HalfBath+DF$BsmtHalfBath\n  DF$HalfBath&lt;-NULL\n  DF$BsmtHalfBath&lt;-NULL\n  DF$Alley&lt;-NULL\n  DF$Fence&lt;-DF$Fence!=\"No Fence\"\n  DF$SaleCondition&lt;-DF$SaleCondition==\"Normal\"\n  DF$LandContour&lt;-DF$LandContour==\"Lvl\"\n  DF$LandSlope&lt;-DF$LandSlope==\"Gtl\"\n  DF$Foundation[!(DF$Foundation %in% c(\"PConc\", \"CBlock\"))] &lt;- \"Other\"\n  DF$LotConfig[!(DF$LotConfig %in% c(\"Corner\",\"Inside\"))]&lt;-\"Other\"\n  DF$RoofStyle&lt;-DF$RoofStyle!=\"Gable\"\n  DF$MiscFeature[DF$MiscFeature==\"TenC\"]&lt;-\"Othr\"\n  DF$Condition1[DF$Condition1 %in% c(\"RRNe\",\"RRAe\",\"RRNn\",\"RRAn\")]&lt;-\"NR\"\n  DF$Exterior1st[DF$Exterior1st==\"AsphShn\"]&lt;-\"AsbShng\"\n  DF$Exterior1st[DF$Exterior1st==\"CBlock\"]&lt;-\"CemntBd\"\n  DF$Exterior1st[DF$Exterior1st==\"ImStucc\"]&lt;-\"Stucco\"\n  DF$Exterior2nd[DF$Exterior2nd==\"CBlock\"]&lt;-\"CemntBd\"\n  DF$Exterior2nd[DF$Exterior2nd==\"Other\"]&lt;-\"plywood\"\n  return(DF)\n}\n\ndf=modify_df(df)\n#df_test_for_choice&lt;-modify_df(df_test_for_choice)\ndf_test_preformence&lt;-modify_df(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#remove-varibels",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#remove-varibels",
    "title": "House Pricing Project",
    "section": "Remove varibels",
    "text": "Remove varibels\nIn this section, I will list the variables I plan to remove or disregard. The reason for their removal is that their distributions are very skewed compared to other variables, which I deem similar in the information they provide.\nThe main reason for disregarding these variables is that I do not believe they have high predictive strength, and I need to move forward with the data cleaning process. Also, I’m not getting paid for this.\n\n\nClick to expand/collapse varibel removed\n\n\nX3SsnPorch no real information will be removed (removed)\nScreenPorch no information aviable will be removed\nAlley contain no information and will be removed\nstreet contain not information and will be removed\nUtilities contain not information and will be removed\nCondition2 contain not information and will be removed\nRoofMatl contain not information and will be removed will be removed\nHeating contain not information and will be removed will be removed\nCentralAir contain not information and will be removed will be removed\nPoolQC contain not information and will be removed will be removed\nMiscFeature contain not information and will be removed will be removed\nGarageQual\nGarageCond\nGarageType\nElectrical\n\n\n\n\nCode\nremove_varibels&lt;-function(DF){\n  DF$X3SsnPorch&lt;-NULL\n  DF$ScreenPorch&lt;-NULL\n  DF$Alley&lt;-NULL\n  DF$Street&lt;-NULL\n  DF$Utilities&lt;-NULL\n  DF$Condition2&lt;-NULL\n  DF$RoofMatl&lt;-NULL\n  DF$Heating&lt;-NULL\n  DF$CentralAir&lt;-NULL\n  DF$PoolQC&lt;-NULL\n  DF$SaleType&lt;-NULL\n  DF$GarageQual&lt;-NULL\n  DF$GarageCond&lt;-NULL\n  DF$GarageType&lt;-NULL\n  DF$Electrical&lt;-NULL\n  return(DF)\n}\n\ndf=remove_varibels(df)\n#df_test_for_choice&lt;-remove_varibels(df_test_for_choice)\ndf_test_preformence&lt;-remove_varibels(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#variables-after-cleanup",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#variables-after-cleanup",
    "title": "House Pricing Project",
    "section": "Variables After Cleanup",
    "text": "Variables After Cleanup\nBelow, I have plotted the variables remaining after the cleanup.\n\n\nCode\nwrite.csv(df, \"data_after_clean.csv\", row.names = FALSE)\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(purrr)\n\nfor (col in names(df)) {\n  if (is.numeric(df[[col]])) {\n    # Numeriske variabler - Lav histogram\n    n_unique &lt;- length(unique(df[[col]]))\n    p &lt;- ggplot(df, aes_string(x = col)) + \n         geom_histogram(bins = min(n_unique, 30), fill = 'skyblue', color = 'black', alpha = 0.7) + \n         labs(title = paste('Histogram of', col), x = col, y = 'Frequency') + \n         theme_minimal()\n  } else {\n    # Ikke-numeriske variabler - Lav bar plot\n    p &lt;- ggplot(df, aes_string(x = col)) + \n         geom_bar(fill = 'orange', color = 'black', alpha = 0.7) + \n         labs(title = paste('Bar Plot of', col), x = col, y = 'Count') + \n         theme_minimal()\n  }\n  print(p)  # Vis plottet\n}\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\nlibrary(shiny)\nlibrary(ggplot2)\n\ndata_before &lt;- read.csv(\"https://raw.githubusercontent.com/Missing-almost-everywhere/Missing-almost-everywhere.io/main/Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/data_after_clean.csv\")\n\nui &lt;- fluidPage(\n  titlePanel(\"House Prices Data Visualization\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"variable\", \"Select Variable:\", choices = names(data_before))\n    ),\n    mainPanel(\n      plotOutput(\"dynamicPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$dynamicPlot &lt;- renderPlot({\n    col &lt;- input$variable\n    \n    if (is.numeric(data_before[[col]])) {\n      ggplot(data_before, aes(x = .data[[col]])) + \n        geom_histogram(bins = 30, fill = 'skyblue', color = 'black', alpha = 0.7) +\n        labs(title = paste('Histogram of', col), x = col, y = 'Frequency') +\n        theme_minimal()\n    } else {\n      ggplot(data_before, aes(x = .data[[col]])) + \n        geom_bar(fill = 'orange', color = 'black', alpha = 0.7) +\n        labs(title = paste('Bar Plot of', col), x = col, y = 'Count') +\n        theme_minimal()\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#looking-into-cross-correlation",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#looking-into-cross-correlation",
    "title": "House Pricing Project",
    "section": "Looking Into Cross-Correlation",
    "text": "Looking Into Cross-Correlation\nBelow, I have plotted the correlation matrix for the variables. Since many of these variables are categorical but have a ranking, I used Spearman correlation.\n\n\nCode\ndf=na.omit(df)\nlibrary(corrplot)\n\n\ncorrplot 0.95 loaded\n\n\nCode\nlibrary(Hmisc)\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nCode\nnumeric_df &lt;- df[sapply(na.omit(df), is.numeric)]\n\n# Compute Spearman correlation matrix\nspearman_corr &lt;- cor(numeric_df, method = \"spearman\")\n\n# Compute p-values (optional)\nlibrary(Hmisc)\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n# Plot the correlation matrix\ncorrplot(spearman_corr, method = \"color\",tl.col = \"black\", tl.srt = 60, insig = \"blank\",tl.cex = 0.5,title = \"Spearman Correlation Plot\")\n\n\n\n\n\n\n\n\n\nNotes on the Plot Overall, the internal correlation is a lot smaller than I thought, which is good. This should make it easier to find good predictors.\nThe third variable from the bottom is the sales price. From this, one can see which variables could be potential predictors. It seems like OverallQual could be a really good predictor.\nHowever, correlation only captures linear effects, so one should be cautious about relying solely on correlation for feature selection. Imagine there was a feature with a sinusoidal curve that perfectly described the data—it would have a correlation of zero."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#reducing-variables-based-on-multicollinearity",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#reducing-variables-based-on-multicollinearity",
    "title": "House Pricing Project",
    "section": "Reducing Variables Based on Multicollinearity",
    "text": "Reducing Variables Based on Multicollinearity\nIn this section, I will reduce some variables, as multicollinearity can pose challenges for certain models. While some partitioning-based models are less sensitive to multicollinearity, linear models can be significantly impacted. In a simple linear model with two inputs, the variance of a parameter can be expressed as:\n\\[\\text{Var}(\\beta_1) = \\frac{\\sigma^2}{1 - \\text{cor}(x_1, x_2)}\\]\nThis means that as the correlation between two variables increases, the variance of the estimate explodes. Consequently, certain models are sensitive to multicollinearity.\nOne approach to addressing this is to remove variables with a correlation above a certain threshold. However, determining the optimal threshold is not straightforward. For a specific model, simulations can be conducted to estimate a suitable value. In this case, I want to perform some reduction before applying any models. I have chosen a cutoff point of (|0.7|). If there is a high correlation between two variables, I will remove the one with the weaker correlation to the log of the sale price.\nSince the variables are not numeric but do have a ranking, PCA is not an option. However, Spearman correlation can still be used.\nFor variable reduction, hierarchical clustering can be applied using the absolute value of the Spearman correlations as a distance measure. Single-linkage clustering can then be employed, with a slight reformulation of the problem. Hierarchical clustering requires a distance metric, and for this purpose, the distance between two variables based on correlation can be defined as:\n\\(d(x_1, x_2) = 1 - |\\text{cor}(x_1, x_2)|\\)\nGiven the cutoff of (|0.7|) for correlation, the corresponding distance after reformulation would be:\n\\(1 - 0.7 = 0.3\\)\n\n\nCode\n# The code here is not optimized but it was fast to write\nnumeric_df &lt;- df[sapply(na.omit(df), is.numeric)]\n\nnumeric_saleprice&lt;-numeric_df$SalePrice\nnumeric_df$SalePrice&lt;-NULL # remove from dataframe\n\nlibrary(Hmisc)\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n\ndist_matrix &lt;- as.dist(1-abs(spearman_corr))\n\nhc &lt;- hclust(dist_matrix, method = \"single\")\n\n\nclusters &lt;- cutree(hc, h = 0.3) #\n\n# Visualize dendrogram\nplot(hc, main = \"Hierarchical Clustering Dendrogram\")\nabline(h = 0.3, col = \"red\") # Add cutoff line\n\n\n\n\n\n\n\n\n\nCode\n# reduce varibels\nnumeric_saleprice_corr &lt;- rcorr(as.matrix(numeric_df), numeric_saleprice, type = \"spearman\")#$r[, \"numeric_saleprice\"]\n\n\n# Step 2: Identify clusters with multiple variables\nduplicates &lt;- table(clusters)[table(clusters) &gt; 1]\nclusters_with_duplicates &lt;- names(duplicates)\n\n\nfor (cluster in clusters_with_duplicates) {\n  clusters_with_duplicates\n  n_cluster=length(names(clusters[clusters==cluster]))\n  spear_var=rep(NA,n_cluster)\n  names_cluster=names(clusters[clusters==cluster])\n  for (i in 1:n_cluster){\n    names(clusters[clusters==cluster])[i]\n    spear_var[i]=rcorr(as.matrix(numeric_df[names(clusters[clusters==cluster])[i]]),log(numeric_saleprice),type=\"spearman\")$r[1,2]\n  }\n  index_of_max &lt;- which.max(spear_var)\n  names_cluster[-index_of_max]\n  #numeric_df[names_cluster[-index_of_max]]&lt;-NULL\n  df[names_cluster[-index_of_max]]&lt;-NULL\n  #df_test_for_choice[names_cluster[-index_of_max]]&lt;-NULL\n  df_test_preformence[names_cluster[-index_of_max]]&lt;-NULL\n}\n\n\nWith the clusters found, for each cluster, the feature with the highest Spearman correlation to the log of the sale price will be chosen.\nAs a result, there was a reduction from 61 to 41 features.\n\n\nCode\nnumeric_df &lt;- df[sapply(na.omit(df), is.numeric)]\n\nnumeric_saleprice&lt;-numeric_df$SalePrice\nnumeric_df$SalePrice&lt;-NULL # remove from dataframe\n\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n\ndist_matrix &lt;- as.dist(1-abs(spearman_corr))\n\nhc &lt;- hclust(dist_matrix, method = \"single\")\n\n\nclusters &lt;- cutree(hc, h = 0.3) #\n\n# Visualize dendrogram\nplot(hc, main = \"Hierarchical Clustering Dendrogram\")\nabline(h = 0.3, col = \"red\") # Add cutoff line\n\n\n\n\n\n\n\n\n\nAs can be seen in the new dendrogram, the dataframe has been reduced to meet the requirements."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#overview-of-variables-with-the-highest-correlation-to-log-of-sale-price",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#overview-of-variables-with-the-highest-correlation-to-log-of-sale-price",
    "title": "House Pricing Project",
    "section": "Overview of Variables with the Highest Correlation to Log of Sale Price",
    "text": "Overview of Variables with the Highest Correlation to Log of Sale Price\nBelow, I have ordered the variables based on the absolute value of their Spearman correlation.\n\n\nCode\n# Required libraries\nlibrary(corrplot)\nlibrary(Hmisc)\n\n# Subset the dataframe to numeric columns\nnumeric_df$log_saleprice&lt;-log(df$SalePrice)\nnumeric_df$SalePrice&lt;-NULL\n# Ensure the target variable is present\ntarget_variable &lt;- \"log_saleprice\"\nif (!target_variable %in% colnames(numeric_df)) {\n  stop(\"Target variable not found in numeric dataframe.\")\n}\n\n# Compute Spearman correlation matrix and p-values\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n# Extract correlation with the target variable\ntarget_corr &lt;- spearman_corr[, target_variable, drop = FALSE]\ntarget_corr &lt;- as.data.frame(target_corr)\ncolnames(target_corr) &lt;- \"Spearman_Correlation\"\n\n# Add a column indicating if the correlation could be computed\ntarget_corr$Computed &lt;- !is.na(target_corr$Spearman_Correlation)\n\n# Sort variables by Spearman correlation in descending order\nsorted_corr &lt;- target_corr[order(abs(target_corr$Spearman_Correlation), decreasing = TRUE), ]\n\n# Display sorted variables with their correlation and computed status\nprint(sorted_corr[1:10,])\n\n\n               Spearman_Correlation Computed\nlog_saleprice             1.0000000     TRUE\nOverallQual               0.8087095     TRUE\nGrLivArea                 0.7304531     TRUE\nBsmtQual                  0.6760372     TRUE\nGarageArea                0.6477811     TRUE\nFullBath_total            0.6371933     TRUE\nGarageFinish              0.6323531     TRUE\nTotalBsmtSF               0.6023901     TRUE\nYearRemodAdd              0.5688537     TRUE\nHeatingQC                 0.4906598     TRUE\n\n\nLook at the Year sold the collation is really low. the spand of the years it relly lowe, this would idicate that the price do not raise a lot over time in this area. I would have expted some kind of increase to compensate for inflation.\nBelow i have recoded the Time as months since 2006, 1 being january. I also plottet logsale price as agianst the month\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nnumeric_df$time&lt;-NA\nfor (i in 1:length(df$MoSold)){\n  numeric_df$time[i]&lt;-(numeric_df$YrSold[i]-2006)*12+numeric_df$MoSold[i]\n}\n\nyearly_means &lt;- numeric_df %&gt;%\n  group_by(time) %&gt;%\n  summarise(mean_value = mean(log_saleprice))\n\n# Plot residuals against time\nggplot(data=numeric_df[!is.na(numeric_df$log_saleprice),], aes(x=numeric_df$time, y=log_saleprice)) +\n  geom_point(alpha = 0.6, color = \"black\") +  # Individual data points\n  geom_line(data = yearly_means, aes(x = time, y = mean_value, color = \"Mean Sold Price\"), \n            size = 1, show.legend = TRUE) +\n  geom_point(data = yearly_means, aes(x = (time), y = mean_value),\n             color = \"red\", size = 2) +\n  labs(\n    x = \"Month\",\n    y = \"Value\",\n    title = \"Monthly Data with Mean Overlay\"\n  ) +\n  theme_minimal()# caluculate mean \n\n\n\n\n\n\n\n\n\nCode\nmonthly_means &lt;- numeric_df %&gt;%\n  group_by(MoSold) %&gt;%\n  summarise(mean_value = mean(log_saleprice))\n\n\n\nggplot(data=numeric_df[!is.na(numeric_df$log_saleprice),], aes(x=numeric_df$MoSold, y=log_saleprice)) +\n  geom_point(alpha = 0.6, color = \"black\") +  # Individual data points\n  geom_line(data = monthly_means, aes(x = MoSold, y = mean_value, color = \"Mean Sold Price\"), \n            size = 1, show.legend = TRUE) +\n  geom_point(data = monthly_means, aes(x = (MoSold), y = mean_value),\n             color = \"red\", size = 2) +\n  labs(\n    x = \"Month\",\n    y = \"Value\",\n    title = \"Monthly Data with Mean Overlay\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOverall their dont seam to be any big conettiction between pice and time. From the moth plot it clear if ther varinace of the price is dependen on the time ore if the salevolum changes letting to less spread."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#overallqual",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#overallqual",
    "title": "House Pricing Project",
    "section": "OverallQual",
    "text": "OverallQual\nOverallQual has high corelation with saleprice.\n\n\nCode\nggplot(data=df[!is.na(df$SalePrice),], aes(x=factor(OverallQual), y=log(SalePrice)))+\n        geom_point()+ labs(title=\"OverallQual\")\n\n\n\n\n\n\n\n\n\nlog price seam linear related to overall quality."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#ground-lvving-area",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#ground-lvving-area",
    "title": "House Pricing Project",
    "section": "Ground lvving area",
    "text": "Ground lvving area\nGround living area.\n\n\nCode\nggplot(data=df[!is.na(df$SalePrice),], aes(x=factor(GrLivArea), y=log(SalePrice)))+\n        geom_point()+ labs(title=\"GrLivArea\")"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#looking-at-some-no-numeric-varibels",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#looking-at-some-no-numeric-varibels",
    "title": "House Pricing Project",
    "section": "Looking at some no numeric varibels",
    "text": "Looking at some no numeric varibels\nSome of these varibels do not have natrual ordering, wich mean computing spearman corelation is not posibel. In that case the best option (I know) is to plot the target varibels against the cagories in the varibels, theirs differenct forms, such boxbolot ore violin plot. I have chosen violin plot since it hase more resempelens with a historgram, wich i prefere over boxplot.\n\n\nCode\nwrite.csv(df, \"data_for_final_model.csv\", row.names = FALSE)\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nnon_numeric_cols &lt;- df %&gt;%\n  select_if(~!is.numeric(.)) %&gt;%\n  colnames()\n\nfor (col in non_numeric_cols) {\n  p &lt;- ggplot(df, aes_string(x = col, y = \"log(SalePrice)\")) +  \n    geom_violin() +\n    stat_summary(fun = \"mean\", geom = \"point\", color = \"red\", size = 3) +  # Tilføj mean som rødt punkt\n    labs(title = paste(\"Violin plot for\", col, \"vs log(SalePrice)\")) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  \n  print(p)  # Udskriv violin plot for hver iteration\n}\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata_final &lt;- read.csv(\"https://raw.githubusercontent.com/Missing-almost-everywhere/Missing-almost-everywhere.io/main/Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/data_for_final_model.csv\")\n\nui &lt;- fluidPage(\n  titlePanel(\"House Price Analysis\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"variable\", \"Select Variable:\", \n                  choices = names(data_final))\n    ),\n    mainPanel(\n      plotOutput(\"dynamicPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$dynamicPlot &lt;- renderPlot({\n    col &lt;- input$variable\n    \n    if (is.numeric(data_final[[col]])) {\n      if(col != \"SalePrice\") {\n        ggplot(data_final, aes(x = .data[[col]], y = log(SalePrice))) +\n          geom_point(alpha = 0.5) +\n          geom_smooth(method = \"lm\", color = \"red\") +\n          labs(title = paste(col, \"vs log(SalePrice)\")) +\n          theme_minimal()\n      } else {\n        ggplot(data_final, aes(x = .data[[col]])) +\n          geom_histogram(bins = 30, fill = 'skyblue', color = 'black', alpha = 0.7) +\n          labs(title = paste('Histogram of', col)) +\n          theme_minimal()\n      }\n    } else {\n      ggplot(data_final, aes(x = .data[[col]], y = log(SalePrice))) +\n        geom_violin(fill = 'skyblue', alpha = 0.7) +\n        stat_summary(fun = mean, geom = \"point\", color = \"red\", size = 3) +\n        labs(title = paste(col, \"vs log(SalePrice)\")) +\n        theme_minimal() +\n        theme(axis.text.x = element_text(angle = 45, hjust = 1))\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\nSo what I am looking for is something their it seam like the means i sale price is different between the factors. Neighborhood, seam like a good varibel to include, in many intances on can see that the their is not even a overlab in the observed price.\nThe roofing seam like it could have potesial"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#base-line-mocel",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#base-line-mocel",
    "title": "House Pricing Project",
    "section": "Base line mocel",
    "text": "Base line mocel\nAfter Plotting different models and looking the residual i found the below model. Where the saleprice pr sqft of living area is combination the Neighborhood the type of dwelling (MSSubClass)\nwich could be good candidate if one look the residuals being drawn from af t distribusion. So more heavy tale distribution,that a normal distribution. In many ways this model seam resanbull from a intuition point of view.\n\n\nCode\n# Plot original data\n\nmodel_baseline &lt;-lm((SalePrice/GrLivArea) ~ TotalBsmtSF*MSSubClass:Neighborhood:GrLivArea , data = df) # lm((SalePrice / GrLivArea) ~ TotalBsmtSF+MSSubClass+Neighborhood,data=df)\n\n\nplot(df$SalePrice,log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea))\n\n\n\n\n\n\n\n\n\nCode\nhist(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea),breaks=50)\n\n\n\n\n\n\n\n\n\nCode\n# The histogram almost look like a t distribution\nlibrary(MASS)\nfitdistr(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea), \"t\",start = list(m=mean(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea)),s=sd(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea)), df=2),lower=c(-1, 0.001,1))\n\n\n        m              s              df     \n  -0.009049068    0.156394619    4.526172860 \n ( 0.004804025) ( 0.005221797) ( 0.565497406)\n\n\nCode\nlibrary(Dowd)\nTQQPlot(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea),df=4.5 )"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#a-linear-interaction-model.",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#a-linear-interaction-model.",
    "title": "House Pricing Project",
    "section": "A linear interaction model.",
    "text": "A linear interaction model.\nBelove I have fitted a interaction model where log saleprice is a interactiv funtion between The type of house, the neighborhood and the size of the dewelling+ sum effect from the size of the basement. It seam reasonbell and is somewat how i would estimate a type of house for the overall price.\n\n\nCode\nModel_second_baseline&lt;- lm(log(SalePrice)~TotalBsmtSF+MSSubClass:GrLivArea:Neighborhood,data=df)\n\n\nlibrary(Dowd)\nTQQPlot(Model_second_baseline$residuals,df=4.5 )\n\n\n\n\n\n\n\n\n\nCode\n# Define the custom metric for CV evaluation\nlog_diff_metric_model_baseline &lt;- function(data, label) {\n  groups=unique(label)\n  groups_score=rep(NA,length(groups))\n  \n  for (i in 1:length(groups)){\n    groupe_for_test=groups[i]\n    partion_vector_train &lt;- which(label!= groupe_for_test)\n    partion_vector_test &lt;- which(label== groupe_for_test)\n    train_data &lt;- data[partion_vector_train, ] \n    test_data &lt;- data[partion_vector_test, ] \n    # fit model \n    model_baseline &lt;-lm(log(SalePrice)~TotalBsmtSF+MSSubClass:GrLivArea:Neighborhood,data=train_data)\n    # get predition \n    predicted_SalePrice &lt;- predict(model_baseline, newdata = test_data) \n    # get log diff\n    log_diff &lt;- log(test_data$SalePrice) - (predicted_SalePrice)\n    # save results\n    groups_score[i]&lt;-sum(log_diff**2)\n  }\n  \n  return(sum(groups_score)/length(data$SalePrice))\n}\n\n# make labels\n\nn_cv=50\n\nlabels=sample(rep(seq(1,n_cv),floor(length(df$Id)/n_cv)))\nif(length(df$Id)-floor(length(df$Id)/n_cv)*n_cv!=0){\n  labels&lt;-c(labels,seq(1:(length(df$Id)-floor(length(df$Id)/n_cv)*n_cv)))\n}\nlabels=sample(labels)\n\n\nprint(\"RMSE of log prices\")\n\n\n[1] \"RMSE of log prices\"\n\n\nCode\nprint(log_diff_metric_model_baseline(df,labels))\n\n\n[1] 0.06415234\n\n\nOver all the predicative stregth look good the reisudall of the model looks more simullaro to t distribuin than a normal."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#lasso",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#lasso",
    "title": "House Pricing Project",
    "section": "Lasso",
    "text": "Lasso\nIn this chunck of code i fit lasso model for the dataframe, by asuming a linear model over all the vearibels in the datafram penlized by the size of beta. The Lasso solves the following\n\\[\\underset{\\beta}{argmin} ||log(Y)-\\beta X||_{2}^2+\\lambda||\\beta||_1\\].\n“It penalizes the size of the coefficients (beta). Since the penalty is based on the \\(L_1\\) -norm, it tends to set some coefficients to zero. This encourages sparse solutions, where only the most relevant features are retained.\n\n\nCode\nlibrary(glmnet)\n\n# Prepare the data\ndf_lasso &lt;- df\nlog_saleprice &lt;- log(df_lasso$SalePrice)\ndf_lasso$SalePrice &lt;- NULL\ndf_lasso$log_saleprice &lt;- log_saleprice  # Add log_saleprice to df_lasso\ndf_lasso_id&lt;-df_lasso$Id\ndf_lasso$Id&lt;-NULL\n# Remove rows with NA values\ndf_lasso &lt;- na.omit(df_lasso)\n\n# Split data into folds\nk &lt;- 10\nfolds &lt;- sample(1:k, size = nrow(df_lasso), replace = TRUE)\n\n# Initialize a vector to store MSEs for each fold\ncv_mse &lt;- numeric(k)\nbest_lambda_vector &lt;- rep(NA, k)\n\nfor (i in 1:k) {\n  # Split data into training and test sets\n  train_indices &lt;- which(folds != i)\n  test_indices &lt;- which(folds == i)\n  \n  train_data &lt;- df_lasso[train_indices, ]\n  test_data &lt;- df_lasso[test_indices, ]\n  \n  # Create design matrices\n  X_train &lt;- model.matrix(log_saleprice ~ . , data = train_data)\n  X_test &lt;- model.matrix(log_saleprice ~ . , data = test_data)\n  \n  # Fix column mismatch between X_train and X_test\n  common_columns &lt;- intersect(colnames(X_train), colnames(X_test))\n  X_test &lt;- X_test[, common_columns, drop = FALSE]\n  X_train &lt;- X_train[, common_columns, drop = FALSE]\n  \n  # Fit LASSO model with cross-validation to choose lambda\n  lasso_model &lt;- cv.glmnet(X_train, train_data$log_saleprice, alpha = 1, family = \"gaussian\")\n  best_lambda &lt;- lasso_model$lambda.min\n  best_lambda_vector[i] &lt;- best_lambda\n  \n  # Predict on test data\n  predictions &lt;- predict(lasso_model, s = best_lambda, newx = X_test)\n  \n  # Compute MSE for this fold\n  cv_mse[i] &lt;- mean((test_data$log_saleprice - predictions)^2)\n}\n\n# Average MSE across all folds\nmean_cv_mse &lt;- mean(cv_mse)\ncat(\"Mean Cross-Validated MSE:\", mean_cv_mse, \"\\n\")\n\n\nMean Cross-Validated MSE: 0.02193118 \n\n\nCode\n# Fit final LASSO model with the average best lambda\nmean_best_lambda &lt;- mean(best_lambda_vector, na.rm = TRUE)  # Calculate mean lambda\nX_full &lt;- model.matrix(log_saleprice ~ . , data = df_lasso)\nfinal_lasso_model &lt;- glmnet(X_full, log_saleprice, alpha = 1, lambda = mean_best_lambda, family = \"gaussian\")\n\n# Print final model coefficients\nprint(mean_cv_mse)\n\n\n[1] 0.02193118\n\n\nAs observed, the predictive strength of the model is better than the baseline model, but not by a lot in terms of predictive performance."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#xgboost",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#xgboost",
    "title": "House Pricing Project",
    "section": "XGBoost",
    "text": "XGBoost\nXGBoost is an ensemble learning method that uses repeated decision trees, where each tree is fitted on the residuals of the previous one. Essentially, it’s a sophisticated method of partitioning the dataset.\nThis approach has gained significant popularity, so I decided to give it a try. Since XGBoost relies on tree structures for partitioning, the data must have an inherent ordering. To include categorical variables, I applied one-hot encoding to transform them into a suitable format.\n\n\nCode\nlibrary(xgboost)\nlibrary(data.table)\n\n# Prepare the dataset\ndf_xgb &lt;- df\ntarget &lt;- log(df_xgb$SalePrice)  # Log-transform the target variable\ndf_xgb$SalePrice &lt;- NULL         # Remove the target variable from features\ndf_xgb_Id &lt;- df_xgb$Id\ndf_xgb$Id &lt;- NULL\n\n# One-hot encode categorical variables\ndf_xgb &lt;- as.data.table(df_xgb)\ndf_xgb &lt;- model.matrix(~ . - 1, data = df_xgb)  # Perform one-hot encoding and remove intercept\n\n# Create DMatrix\nddata &lt;- xgb.DMatrix(data = df_xgb, label = target)\n\n# Set parameters for the XGBoost model\nparams &lt;- list(\n  objective = \"reg:squarederror\",  # Regression task\n  eta = 0.1,                       # Learning rate\n  max_depth = 6,                   # Maximum depth of trees\n  subsample = 0.8,                 # Subsampling ratio\n  colsample_bytree = 0.8           # Feature subsampling ratio\n)\n\n# Perform 10-fold cross-validation\nset.seed(123)  # For reproducibility\ncv_results &lt;- xgb.cv(\n  params = params,\n  data = ddata,\n  nrounds = 100,                    # Number of boosting rounds\n  nfold = 10,                       # Number of folds for cross-validation\n  metrics = \"rmse\",                 # Evaluation metric\n  early_stopping_rounds = 10,       # Stop early if no improvement\n  print_every_n = 10,               # Print progress every 10 rounds\n  verbose = TRUE\n)\n\n\n[1] train-rmse:10.379960+0.002592   test-rmse:10.379961+0.026888 \nMultiple eval metrics are present. Will use test_rmse for early stopping.\nWill train until test_rmse hasn't improved in 10 rounds.\n\n[11]    train-rmse:3.637180+0.001203    test-rmse:3.637569+0.017285 \n[21]    train-rmse:1.286535+0.000669    test-rmse:1.287011+0.011197 \n[31]    train-rmse:0.469571+0.000640    test-rmse:0.476945+0.007929 \n[41]    train-rmse:0.190574+0.000775    test-rmse:0.215138+0.012590 \n[51]    train-rmse:0.102444+0.000572    test-rmse:0.147398+0.018300 \n[61]    train-rmse:0.075914+0.000609    test-rmse:0.132825+0.019453 \n[71]    train-rmse:0.065099+0.000603    test-rmse:0.129387+0.019381 \n[81]    train-rmse:0.057990+0.001078    test-rmse:0.128064+0.019680 \n[91]    train-rmse:0.052257+0.001140    test-rmse:0.127166+0.019725 \n[100]   train-rmse:0.047739+0.001064    test-rmse:0.126684+0.019586 \n\n\nCode\n# Extract the best number of boosting rounds\nbest_nrounds &lt;- cv_results$best_iteration\ncat(\"Best number of rounds:\", best_nrounds, \"\\n\")\n\n\nBest number of rounds: 98 \n\n\nCode\n# Train final model using the best number of rounds\nfinal_model &lt;- xgb.train(\n  params = params,\n  data = ddata,\n  nrounds = best_nrounds\n)\n\n# Evaluate final model on entire dataset (or split as desired)\npredictions &lt;- predict(final_model, ddata)\noverall_rmse &lt;- sqrt(mean((target - predictions)^2))\ncat(\"RMSE on entire dataset:\", overall_rmse, \"\\n\")\n\n\nRMSE on entire dataset: 0.05299007"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#prepration-of-test-set.",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#prepration-of-test-set.",
    "title": "House Pricing Project",
    "section": "Prepration of test set.",
    "text": "Prepration of test set.\nIn this section, I analyze the features in the test set. There are some missing values, which I address using the K-Nearest Neighbors (KNN) algorithm. This method is particularly effective for handling missing values in categorical variables.\n\n\nCode\nprint(\"df_test_preformence\")\n\n\n[1] \"df_test_preformence\"\n\n\nCode\ntable(df_test_preformence$MSSubClass)\n\n\n\n 20  30  45  50  60  70  75  80  90 120 150 160 180 190 \n543  70   6 145 276  68   7  88  57  95   1  65   7  31 \n\n\nCode\nprint(\"df\")\n\n\n[1] \"df\"\n\n\nCode\ntable(df$MSSubClass)\n\n\n\n 20  30  45  50  60  70  75  80  90 120 160 180 190 \n532  67  12 148 296  60  16  78  52  86  63  10  30 \n\n\n\n\nCode\ndf_test_preformence$MSSubClass[df_test_preformence$MSSubClass==150]=160\n\n\nmissing values\n\n\nCode\nvis_miss(df_test_preformence)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(VIM)\n\n\nWarning: package 'VIM' was built under R version 4.4.2\n\n\nLoading required package: colorspace\n\n\nLoading required package: grid\n\n\nVIM is ready to use.\n\n\nSuggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues\n\n\n\nAttaching package: 'VIM'\n\n\nThe following object is masked from 'package:bootstrap':\n\n    diabetes\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\n\nCode\n# Impute missing values using KNN\nimputed_data &lt;- kNN(df_test_preformence, k = 5)\n\n# the imputed_data ad a new colum to say what vaules is imputed i am remvoving them.\ndf_test_preformence&lt;-imputed_data[,1:dim(df_test_preformence)[2]]\n\n# Check the updated dataframe\ndim(df_test_preformence)\n\n\n[1] 1459   54\n\n\nTheir is one Plywood in test set\n\n\nCode\nsetdiff(unique(df$Exterior2nd),unique(df_test_preformence$Exterior2nd))\n\n\n[1] \"plywood\"\n\n\nCode\ntable(df_test_preformence$Exterior2nd)\n\n\n\nAsbShng AsphShn Brk Cmn BrkFace CemntBd CmentBd HdBoard ImStucc MetalSd Plywood \n     18       1      15      22       2      66     199       5     233     128 \n  Stone  Stucco VinylSd Wd Sdng Wd Shng \n      1      21     511     194      43 \n\n\nCode\ntable(df$Exterior2nd)\n\n\n\nAsbShng AsphShn Brk Cmn BrkFace CemntBd CmentBd HdBoard ImStucc MetalSd plywood \n     20       3       7      25       1      58     207      10     213       1 \nPlywood   Stone  Stucco VinylSd Wd Sdng Wd Shng \n    142       4      26     499     196      38 \n\n\n\n\nCode\nX_full &lt;- model.matrix(log_saleprice ~ . , data = df_lasso)\nfinal_lasso_model &lt;- glmnet(X_full, df_lasso$log_saleprice, alpha = 1, lambda = mean_best_lambda, family = \"gaussian\")\n\ndf_lasso_evalatio &lt;- df_test_preformence\ndf_lasso_evalatio_id&lt;-df_lasso_evalatio$Id\ndf_lasso_evalatio$Id&lt;-NULL\n# Create design matrix for prediction from test data\nX_predict &lt;- model.matrix(~., data = df_lasso_evalatio)\n\n# Identify any missing columns between X_full and the prediction data (X_predict)\nmissing_cols &lt;- setdiff(colnames(X_full), colnames(X_predict))\n\n# Add missing columns to X_predict with zero values (align columns)\nX_predict &lt;- cbind(X_predict, matrix(0, nrow = nrow(X_predict), ncol = length(missing_cols),\n                                     dimnames = list(NULL, missing_cols)))\n\n# Ensure column order matches X_full\nX_predict &lt;- X_predict[, colnames(X_full)]\n\n# Predict log_saleprice for the test dataset\npredictions &lt;- predict(final_lasso_model, newx = X_predict)\n\n# Convert log predictions to SalePrice scale (exp of log predictions)\nSalePrice_pred &lt;- exp(c(predictions))\n\n# Create a data frame with Id and SalePrice\nprediction_df &lt;- data.frame(Id = df_lasso_evalatio_id, SalePrice = SalePrice_pred)\n\n# Write predictions to a CSV file\n#write.csv(prediction_df, \"sale_prices.csv\", row.names = FALSE)\n\n# Check the length of SalePrice predictions\nlength(prediction_df$SalePrice)\n\n\n[1] 1459"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#plotting-variables-against-logsaleprice",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#plotting-variables-against-logsaleprice",
    "title": "House Pricing Project",
    "section": "Plotting Variables Against logsaleprice",
    "text": "Plotting Variables Against logsaleprice\nSome of these variables do not have natural ordering, which means computing Spearman correlation is not possible. In that case, the best option (I know) is to plot the target variable against the categories in the variables. There are different forms, such as boxplots or violin plots. I have chosen violin plots since they have more resemblance to a histogram, which I prefer over boxplots.\n\n\nCode\nwrite.csv(df, \"data_for_final_model.csv\", row.names = FALSE)\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nnon_numeric_cols &lt;- df %&gt;%\n  select_if(~!is.numeric(.)) %&gt;%\n  colnames()\n\nfor (col in non_numeric_cols) {\n  p &lt;- ggplot(df, aes_string(x = col, y = \"log(SalePrice)\")) +  \n    geom_violin() +\n    stat_summary(fun = \"mean\", geom = \"point\", color = \"red\", size = 3) +  # Tilføj mean som rødt punkt\n    labs(title = paste(\"Violin plot for\", col, \"vs log(SalePrice)\")) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  \n  print(p)  # Udskriv violin plot for hver iteration\n}\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata_final &lt;- read.csv(\"https://raw.githubusercontent.com/Missing-almost-everywhere/Missing-almost-everywhere.io/main/Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/data_for_final_model.csv\")\n\nui &lt;- fluidPage(\n  titlePanel(\"None Numerical varibels\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"variable\", \"Select Variable:\", \n                  choices = names(data_final))\n    ),\n    mainPanel(\n      plotOutput(\"dynamicPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$dynamicPlot &lt;- renderPlot({\n    col &lt;- input$variable\n    \n    if (is.numeric(data_final[[col]])) {\n      if(col != \"SalePrice\") {\n        ggplot(data_final, aes(x = .data[[col]], y = log(SalePrice))) +\n          geom_point(alpha = 0.5) +\n          labs(title = paste(col, \"vs log(SalePrice)\")) +\n          theme_minimal()\n      } else {\n        ggplot(data_final, aes(x = .data[[col]])) +\n          geom_histogram(bins = 30, fill = 'skyblue', color = 'black', alpha = 0.7) +\n          labs(title = paste('Histogram of', col)) +\n          theme_minimal()\n      }\n    } else {\n      ggplot(data_final, aes(x = .data[[col]], y = log(SalePrice))) +\n        geom_violin(fill = 'skyblue', alpha = 0.7) +\n        stat_summary(fun = mean, geom = \"point\", color = \"red\", size = 3) +\n        labs(title = paste(col, \"vs log(SalePrice)\")) +\n        theme_minimal() +\n        theme(axis.text.x = element_text(angle = 45, hjust = 1))\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\nSo what I am looking for is something their it seam like the means i sale price is different between the factors. Neighborhood, seam like a good varibel to include, in many intances on can see that the their is not even a overlab in the observed price.\nThe roofing seam like it could have potesial"
  },
  {
    "objectID": "Projects/sequential hypothesis testing and safe anytime inference.html",
    "href": "Projects/sequential hypothesis testing and safe anytime inference.html",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "",
    "text": "This project was one of those rabbit-hole endeavors. I wanted to learn about A/B testing, which turned out to be a straightforward hypothesis test with some power calculations.\nInterestingly, the calculations for power analysis were concepts I had already learned during my bachelor’s degree. So, I decided to explore some newer methods instead. That led me to sequential testing—a fascinating field. It’s this intriguing mix of betting strategies, martingales, and a lot of other fun concepts.\nI’m mostly writing this project for myself, so I have something to work on and a way to document my learning process. But if someone happens to stumble across this, they should be aware that it can get a bit technical. I’ve included references to the math so I can easily find it again, and I’ve conducted a few small simulation studies. These simulations help me get a sense of the performance of the methods while also allowing me to bend the assumptions a little to see what happens."
  },
  {
    "objectID": "Projects/sequential hypothesis testing and safe anytime inference.html#formally",
    "href": "Projects/sequential hypothesis testing and safe anytime inference.html#formally",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "Formally",
    "text": "Formally\nLet \\(H_0\\) be the null and let \\(H_1\\) be other hyposis\n\\(\\alpha\\) is the proberbillaty of making a type one error meaning\n\\(P(acepting\\: H_1\\: when\\: H_0\\: is\\: true)\\)\n\\(\\beta\\) is the probillaty of type 2 error\n\\(P(acepting\\: H_0\\: when\\: H_1\\: is\\: true)\\)\nIf the distribtion is nice meaning law of large number insure it conveges to normal distribuion.\n\\[n\\geq 2(\\frac{\\phi^{-1}(1-\\alpha-\\phi^{-1}(1-\\beta)}{\\Delta/\\sigma})^2\\]\n\\(\\phi^{-1}\\) is the inverse cumulative standard normal distribution,\n\\(\\Delta\\) is the abeslut differnece in effect betwwen the null \\(H_0\\) and \\(H_1\\).\n\\(\\sigma\\) is the variance."
  },
  {
    "objectID": "Projects/sequential hypothesis testing and safe anytime inference.html#peeking",
    "href": "Projects/sequential hypothesis testing and safe anytime inference.html#peeking",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "Peeking",
    "text": "Peeking\nPeeking is when a one looks at the data as they come in, using standart hyposis test method, and make a conclusion if they dont see a effect. The problem is that it destrou the coverage gaurenty. So if one look at the data as they flow in and end the experiment and before if their is no significans.\nThis idear of if we have hundret experiement, in \\((1-\\alpha)\\) the right hypothesis identified is not valid.\nBelow i have made some small simulations to ilustrate the problem\n\\(A=a_1,\\dots,a_n\\) and \\(B=b_1,\\dots,b_n\\) let \\(a_i\\) and \\(b_i\\) iid and let \\(a_i\\sim\\mathbb{N}(0,1)\\) and \\(b_i\\sim\\mathbb{N}(0,1)\\).\nSince \\(A\\) and \\(B\\) has the same distribuion their is no effect. This is sometime calle a A/A test. Sometimes this is used to test method in a live environment. If a useres test method, they should make get type 1 error cover from this.\nAt each point the simulation will draw with no replacment from A ore B calulat the p values and see if the true mean is in the confiden intervall.\n\n\nCode\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nrandom.seed(42)\n# Parameters\nn = 100  # Number of samples in A and B\nnum_simulations = 1  # Number of simulations\nalpha = 0.05  # Significance level\ntrue_mean = 0  # True mean to check against\nvariance=1 # varince know\na=np.random.normal(true_mean,variance,n) # draw\nb=np.random.normal(true_mean,variance,n) # draw\n\ndef get_sigficanlevet(A:list,B:list,A_variance:float=1,B_variance:float=1):\n    mean_A=np.mean(A)\n    mean_B=np.mean(B)\n    Z = (mean_A - mean_B) / np.sqrt(A_variance / len(A) + B_variance/ len(B))\n    p_value = 2 * (1 - norm.cdf(abs(Z)))\n    return p_value\n\n\ndef get_significant_seq(A: list, B: list, A_variance: float = 1, B_variance: float = 1):\n    # Initialize an empty list to store the significant values\n    sig_seq = []\n\n    # Create copies of A and B for sampling without replacement\n    A_copy = np.copy(A)\n    B_copy = np.copy(B)\n\n    # Initialize vectors for the selected samples\n    A_vector = []\n    B_vector = []\n\n    # Sample the first element from A and B\n    A_vector.append(A_copy[0])\n    A_copy = np.delete(A_copy, 0)\n\n    B_vector.append(B_copy[0])\n    B_copy = np.delete(B_copy, 0)\n\n    # Initial significance level calculation\n    sig_seq.append(get_sigficanlevet(A_vector, B_vector))  # Assuming this function exists\n\n    # Run the loop until both lists are empty\n    while len(A_copy) &gt; 0 or len(B_copy) &gt; 0:\n        # Randomly choose whether to sample from A or B\n        if np.random.rand() &gt; 0.5 and len(A_copy) &gt; 0:\n            A_vector.append(A_copy[0])  # Append the first element from A\n            A_copy = np.delete(A_copy, 0)  # Remove the sampled element from A\n        elif len(B_copy) &gt; 0:\n            B_vector.append(B_copy[0])  # Append the first element from B\n            B_copy = np.delete(B_copy, 0)  # Remove the sampled element from B\n        \n        # Calculate and store the significance level at this step\n        sig_seq.append(get_sigficanlevet(A_vector, B_vector))  # Assuming this function exists\n\n    return sig_seq\n\n\ndef plot_significance_seq(sig_seq):\n    # Plot the significance sequence\n    plt.figure(figsize=(10, 6))\n    plt.plot(sig_seq, marker='o', linestyle='-', color='b', label='Significance Level')\n    \n    # Add horizontal line at y=0.05 for the threshold\n    plt.axhline(y=0.05, color='r', linestyle='-', label='Significance Threshold (0.05)')\n    \n    # Add small green line segments for significance values below 0.05\n    for i, value in enumerate(sig_seq):\n        if value &lt; 0.05:\n            plt.plot([i, i], [0, value], color='green', lw=2)  # Line from 0 to the value\n\n    # Set limits for the y-axis between 0 and 1\n    plt.ylim(0, 1)\n    \n    # Adding labels and title\n    plt.xlabel('Sample Number')\n    plt.ylabel('Significance Level')\n    plt.title('Significance Level Sequence with Threshold at 0.05')\n    \n    # Adding a legend\n    plt.legend()\n\nresult=get_significant_seq(a,b)\nplot_significance_seq(result)\n\n\n\n\n\n\n\n\n\nAs can be see from the plot the p values goes up and down doing the experiment, So if one was to stop the experiement when the p value cross a threshold. The cover is not correct, this means in this case that tyoe 1 error alot higer than one exptes.\nSo let look at how the coverage is affected by the peaking belove i have made small simulation that corespond make the plot above a 1000 times and with a wish effect p value \\(0.05\\). If doing the experiment detects an effect out of \\(0.05\\), it will stop (this is called a decision rule). This setup enables the simulation of coverage during the experimental process.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n\n\ndef get_sigficanlevet(A:list,B:list,A_variance:float=1,B_variance:float=1):\n    mean_A=np.mean(A)\n    mean_B=np.mean(B)\n    Z = (mean_A - mean_B) / np.sqrt(A_variance / len(A) + B_variance/ len(B))\n    p_value = 2 * (1 - norm.cdf(abs(Z)))\n    return p_value\n\n\ndef run_significant_seq(A: list, B: list, A_variance: float = 1, B_variance: float = 1):\n    # Initialize an empty list to store the significant values\n\n    # Create copies of A and B for sampling without replacement\n    A_copy = np.copy(A)\n    B_copy = np.copy(B)\n\n    # Initialize vectors for the selected samples\n    A_vector = []\n    B_vector = []\n\n    # Sample the first element from A and B\n    A_vector.append(A_copy[0])\n    A_copy = np.delete(A_copy, 0)\n\n    B_vector.append(B_copy[0])\n    B_copy = np.delete(B_copy, 0)\n    \n    sig_erro=0\n    # Run the loop until both lists are empty\n    while len(A_copy) &gt; 0 or len(B_copy) &gt; 0:\n        # Randomly choose whether to sample from A or B\n        if np.random.rand() &gt; 0.5 and len(A_copy) &gt; 0:\n            A_vector.append(A_copy[0])  # Append the first element from A\n            A_copy = np.delete(A_copy, 0)  # Remove the sampled element from A\n        elif len(B_copy) &gt; 0:\n            B_vector.append(B_copy[0])  # Append the first element from B\n            B_copy = np.delete(B_copy, 0)  # Remove the sampled element from B\n        \n        # Calculate and store the significance level at this step\n        if(get_sigficanlevet(A_vector, B_vector)&lt;0.05):\n            sig_erro=1\n            break \n    return sig_erro\n\n\n\nn = 100  # Number of samples in A and B\nnum_simulations = 1000  # Number of simulations\nalpha = 0.05  # Significance level\ntrue_mean = 0  # True mean to check against\nvariance=1 # varince know\n\nestimated=[]\nfor i in range(1,num_simulations):\n    a=np.random.normal(true_mean,variance,n) # draw\n    b=np.random.normal(true_mean,variance,n) # draw\n    estimated.append(run_significant_seq(a,b))\n\nprint(\"cover\")\nprint(np.mean(estimated))\n\n\ncover\n0.3883883883883884\n\n\nFor these settings, I usually obtain an estimate of around 0.4, which, to me, seems high. In practice, a person would likely not peek at the data every time a new point arrives. This is merely intended to illustrate the problem of peeking.\nIf peeking causes problems, the solution seems obvious: avoid peeking. However, in practice, people often do.\nThe literature on the method introduced below discusses designing systems to account for user behavior. This includes cases where users receive credit for findings, even when peeking leads to results that may not be valid.\nThe goal is to develop mathematical models that mitigate the problem of peeking. Another approach is preregistration. That said, there are many valid reasons to evaluate results as data comes in and to stop when a clear conclusion can be drawn."
  },
  {
    "objectID": "Projects/sequential hypothesis testing and safe anytime inference.html#sequential-probability-ratio-test-sprt",
    "href": "Projects/sequential hypothesis testing and safe anytime inference.html#sequential-probability-ratio-test-sprt",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "sequential probability ratio test (SPRT)",
    "text": "sequential probability ratio test (SPRT)\nA little more formal explanation of the SPRT:\nHere, we are only looking at two hypotheses and their corresponding points:\n\n\\(H_0: \\theta = \\theta_0\\), where \\(p(x_i)\\) is the distribution under the null hypothesis.\n\n\\(H_1: \\theta = \\theta_1\\), where \\(q(x_i)\\) is the distribution under the alternative hypothesis.\n\nWe begin with a wealth of one.\nUnder the null hypothesis, the expectation of the likelihood ratio is given by:\n\\[  \n\\mathbb{E}\\left( \\frac{q(X_i)}{p(X_i)} \\right) = \\int \\frac{p(x)}{q(x)} q(x) \\, dx = \\int p(x) \\, dx = 1  \n\\]\nThe wealth can be written as:\n\\[  \nM_t = \\prod_{i=1}^{T} \\frac{q(X_i)}{p(X_i)}  \n\\]\nThis is a submartingale, and by using Ville’s inequality, we obtain a probability guarantee for the entire process.\nUnder the alternative hypothesis, the inverse of the likelihood ratio will be a submartingale, leading to two thresholds: one for rejecting the null hypothesis \\(H_0\\) and one for rejecting \\(H_1\\).\nNotice how simple the theory is here."
  },
  {
    "objectID": "Projects/sequential hypothesis testing and safe anytime inference.html#msprt",
    "href": "Projects/sequential hypothesis testing and safe anytime inference.html#msprt",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "mSPRT",
    "text": "mSPRT\nIn practice, one rarely has a point hypothesis but rather a null and composite hypothesis.\nThe question is often whether there is an effect or not.\nThe composite hypothesis is a combination of multiple hypotheses.\nmSPRT uses a mixture distribution for this:\n\\[\n\\Lambda(X_n) = \\int_{\\theta}\\prod_{i=1}^{n}\\frac{f_{\\theta}(x_i)}{f_{\\theta_0}(x_i)}h(\\theta)d\\theta\n\\]\nThe same stopping threshold is used for \\(B\\), meaning type 1 error control is the same since, under the null, the log-likelihood is a submartingale:\n\\[\n\\mathbb{E_{\\theta_0}}\\left(\\frac{f_{\\theta}(x)}{f_{\\theta_0}(x)}h(\\theta)\\right) = \\int f_{\\theta}(x)d\\theta = 1\n\\]\nThis is described as “hedging your bets.” The idea here is that you are betting on multiple hypotheses.\nIt becomes a little more complicated when considering the composite hypothesis, as there are multiple hypotheses, and only one can be true. It is not a submartingale unless one finds a way to adjust the expectation as the process progresses. I have not found such a method. This means the lower threshold for the sequence is not valid.\nWhat one can do instead is to set a limit on the amount of data one wants to gather and set \\(\\beta\\) to zero. This means losing type 2 error control.\nA second option is two experiment with adjusted \\(\\beta\\), using distributions on both sides of the null.\n- If both are stopped by the \\(B\\) criterion, which is related to \\(\\alpha\\), this would suggest the null is true.\n- If one confirms the null and the other rejects it, this would suggest evidence against the null.\nThus, it is easier to simply negate the type 2 error. In practice, the user chooses an \\(\\alpha\\), the tolerance for type 1 error they are willing to accept, and a maximum number of experiments they want to perform or can afford. Then the experiment runs, and if the process is not stopped, the conclusion is that there was no significance.\nBelow i made a small simulation of how one can do this.\n\n\nCode\nimport numpy as np\nfrom scipy.stats import norm\n\n# SPRT implementation\ndef msprt(l_null, l_composite, N_stopping, alpha, draw_function):\n    draws = []\n    log_likelihood_ratios = []\n    b = np.log(1 / alpha)\n    log_likelihood_ratio = 0\n    decision = \"Null\"\n    for i in range(N_stopping):\n        x = draw_function()\n        draws.append(x)\n        log_likelihood_ratio += np.log(l_composite(x) / l_null(x))\n        log_likelihood_ratios.append(log_likelihood_ratio)\n        if log_likelihood_ratios[i] &gt; b:\n            decision = \"Alternative\"\n            break\n    return {\n        \"confirmed_hypothesis\": decision,\n        \"log_likelihood_sum\": log_likelihood_ratio,\n        \"draws\": draws,\n        \"length\": len(draws),\n    }\n\n# Simulation of coverage\ndef simulate_cover_msprt(l_null, l_composite, N_stopping, alpha, draw_function, N_experiments):\n    cover = []\n    length = []\n    for i in range(N_experiments):\n        sim_ob = msprt(l_null, l_composite, N_stopping, alpha, draw_function)\n        cover.append(sim_ob[\"confirmed_hypothesis\"] == \"Null\")\n        length.append(sim_ob[\"length\"])\n    return {\"cover\": np.mean(cover), \"mean length\": np.mean(length)}\n\n# Likelihood functions\ndef get_likelihood_null(x, mean_null, std_dev_null):\n    return norm.pdf(x, loc=mean_null, scale=std_dev_null)\n\ndef get_likelihood_composit(x, mean_1, std_dev_1, phi_1, mean_2, std_dev_2):\n    return phi_1 * norm.pdf(x, loc=mean_1, scale=std_dev_1) + (1 - phi_1) * norm.pdf(x, loc=mean_2, scale=std_dev_2)\n\n# Parameters\nmean_true = 0\nvariance_true = 1\nstd_dev_true = variance_true ** 0.5\n\n# Simulation\nsim_result = simulate_cover_msprt(\n    l_null=lambda x: get_likelihood_null(x, mean_null=0, std_dev_null=1),\n    l_composite=lambda x: get_likelihood_composit(x, mean_1=-2, std_dev_1=1, phi_1=0.5, mean_2=2, std_dev_2=1),\n    N_stopping=1000,\n    alpha=0.05,\n    draw_function=lambda: np.random.normal(mean_true, std_dev_true),\n    N_experiments=1000\n)\n\n# Results\nprint(\"cover:\", sim_result[\"cover\"])\nprint(\"mean length:\", sim_result[\"mean length\"])\n\n\ncover: 0.981\nmean length: 981.069\n\n\nas can be see the cover is good.\nVille’s inequality can also be used for anytime-valid p-values:\n\\[\np_n = \\min \\left[ 1, \\min \\left( \\tilde{\\Lambda}_t^{-1} : t \\leq n \\right) \\right]\n\\]\nIf one thinks about the equation, it is quite simple. This involves moving a horizontal line down until it hits the highest point of the likelihood ratio trajectory and finding the corresponding \\(\\alpha\\).\nThere are important things to notice here. This way of looking at p-values is very consistent, meaning that if we gather evidence against the null, no matter what data is later gathered, the p-values do not rise again.\nThis stands in contrast to “normal” p-values, which can be inconsistent over time periods.\nA very nice propperty."
  },
  {
    "objectID": "Projects/sequential hypothesis testing and safe anytime inference.html#a-small-point",
    "href": "Projects/sequential hypothesis testing and safe anytime inference.html#a-small-point",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "A Small Point",
    "text": "A Small Point\nThe submartingale property comes under a very specific distribution under the null.\nSo, one is not only testing if \\(\\theta = \\theta_0\\) but also making assumptions about the distribution.\nThe Central Limit Theorem (CLT) is very powerful in the sense that, by taking the mean, many distributions will converge, providing some robustness to the CLT.\nHowever, when looking at every single point and making assumptions about the distribution, that robustness is not present in the same way.\nOne way to combat this is by making batches of data, but this takes away some of the idea of anytime-valid confidence intervals.\nA second approach would be to determine the number of times one is allowed to peek and adjust the \\(\\alpha\\) based on this.\nThese methods suffer from some of the same incentive structure errors as normal p-values. Over-peeking becomes a problem, and people not involved in the analysis cannot see if this has occurred."
  },
  {
    "objectID": "Projects/sequential hypothesis testing and safe anytime inference.html#looking-into-if-the-distribion-mispecified",
    "href": "Projects/sequential hypothesis testing and safe anytime inference.html#looking-into-if-the-distribion-mispecified",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "Looking into if the distribion mispecified",
    "text": "Looking into if the distribion mispecified\nBelow here i have made simulation the true distribuion is a t distribuion mean zero with variance 3 scale 1. The null is normal distribuion with variance 3 and mean zero. The only difference is the and the sape of the distribuionsion. The hyposis test is if the mean is the same.\n\n\nCode\nimport numpy as np\nfrom scipy.stats import norm, t\n\n# SPRT implementation\ndef msprt(l_null, l_composite, N_stopping, alpha, draw_function):\n    draws = []\n    log_likelihood_ratios = []\n    b = np.log(1 / alpha)\n    log_likelihood_ratio = 0\n    decision = \"Null\"\n    for i in range(N_stopping):\n        x = draw_function()\n        draws.append(x)\n        log_likelihood_ratio += np.log(l_composite(x) / l_null(x))\n        log_likelihood_ratios.append(log_likelihood_ratio)\n        if log_likelihood_ratios[i] &gt; b:\n            decision = \"Alternative\"\n            break\n    return {\n        \"confirmed_hypothesis\": decision,\n        \"log_likelihood_sum\": log_likelihood_ratio,\n        \"draws\": draws,\n        \"length\": len(draws),\n    }\n\n# Simulation of coverage\ndef simulate_cover_msprt(l_null, l_composite, N_stopping, alpha, draw_function, N_experiments):\n    cover = []\n    length = []\n    for i in range(N_experiments):\n        sim_ob = msprt(l_null, l_composite, N_stopping, alpha, draw_function)\n        cover.append(sim_ob[\"confirmed_hypothesis\"] == \"Null\")\n        length.append(sim_ob[\"length\"])\n    return {\"cover\": np.mean(cover), \"mean length\": np.mean(length)}\n\n# Likelihood functions\ndef get_likelihood_null(x, mean_null, std_dev_null):\n    return norm.pdf(x, loc=mean_null, scale=std_dev_null)\n\ndef get_likelihood_composit(x, mean_1, std_dev_1, phi_1, mean_2, std_dev_2):\n    return phi_1 * norm.pdf(x, loc=mean_1, scale=std_dev_1) + (1 - phi_1) * norm.pdf(x, loc=mean_2, scale=std_dev_2)\n\n# Parameters\nmean_true = 0\nvariance_true = 1\nstd_dev_true = variance_true ** 0.5\n\n# Simulation\nsim_result = simulate_cover_msprt(\n    l_null=lambda x: get_likelihood_null(x, mean_null=0, std_dev_null=np.sqrt(3)),\n    l_composite=lambda x: get_likelihood_composit(x, mean_1=-5, std_dev_1=np.sqrt(3), phi_1=0.5, mean_2=5, std_dev_2=np.sqrt(3)),\n    N_stopping=1000,\n    alpha=0.05,\n    draw_function = lambda: t.rvs(df=3, loc=mean_true, scale=1),\n    N_experiments=1000\n)\n\n# Results\nprint(\"cover:\", sim_result[\"cover\"])\nprint(\"mean length:\", sim_result[\"mean length\"])\n\n\nC:\\Users\\johan\\AppData\\Local\\Temp\\ipykernel_37716\\857125392.py:14: RuntimeWarning:\n\ninvalid value encountered in scalar divide\n\nC:\\Users\\johan\\AppData\\Local\\Temp\\ipykernel_37716\\857125392.py:14: RuntimeWarning:\n\ndivide by zero encountered in scalar divide\n\n\n\ncover: 0.955\nmean length: 955.44\n\n\nAs can be see in this case one actullay get good cover in this case. The simulated cover is close to the teoretical but if I make the null distribuion wide the cover is much lees acurate.\nbelow I have made a small change to the true distribusion so now the scale parater is 2 meaning a vider shape of the t distribusion. So the mean is the same but variance is diffrent.\n\n\nCode\nimport numpy as np\nfrom scipy.stats import norm, t\n\n# SPRT implementation\ndef msprt(l_null, l_composite, N_stopping, alpha, draw_function):\n    draws = []\n    log_likelihood_ratios = []\n    b = np.log(1 / alpha)\n    log_likelihood_ratio = 0\n    decision = \"Null\"\n    for i in range(N_stopping):\n        x = draw_function()\n        draws.append(x)\n        log_likelihood_ratio += np.log(l_composite(x) / l_null(x))\n        log_likelihood_ratios.append(log_likelihood_ratio)\n        if log_likelihood_ratios[i] &gt; b:\n            decision = \"Alternative\"\n            break\n    return {\n        \"confirmed_hypothesis\": decision,\n        \"log_likelihood_sum\": log_likelihood_ratio,\n        \"draws\": draws,\n        \"length\": len(draws),\n    }\n\n# Simulation of coverage\ndef simulate_cover_msprt(l_null, l_composite, N_stopping, alpha, draw_function, N_experiments):\n    cover = []\n    length = []\n    for i in range(N_experiments):\n        sim_ob = msprt(l_null, l_composite, N_stopping, alpha, draw_function)\n        cover.append(sim_ob[\"confirmed_hypothesis\"] == \"Null\")\n        length.append(sim_ob[\"length\"])\n    return {\"cover\": np.mean(cover), \"mean length\": np.mean(length)}\n\n# Likelihood functions\ndef get_likelihood_null(x, mean_null, std_dev_null):\n    return norm.pdf(x, loc=mean_null, scale=std_dev_null)\n\ndef get_likelihood_composit(x, mean_1, std_dev_1, phi_1, mean_2, std_dev_2):\n    return phi_1 * norm.pdf(x, loc=mean_1, scale=std_dev_1) + (1 - phi_1) * norm.pdf(x, loc=mean_2, scale=std_dev_2)\n\n# Parameters\nmean_true = 0\nvariance_true = 1\nstd_dev_true = variance_true ** 0.5\n\n# Simulation\nsim_result = simulate_cover_msprt(\n    l_null=lambda x: get_likelihood_null(x, mean_null=0, std_dev_null=np.sqrt(3)),\n    l_composite=lambda x: get_likelihood_composit(x, mean_1=-5, std_dev_1=np.sqrt(3), phi_1=0.5, mean_2=5, std_dev_2=np.sqrt(3)),\n    N_stopping=1000,\n    alpha=0.05,\n    draw_function = lambda: t.rvs(df=3, loc=mean_true, scale=2),\n    N_experiments=1000\n)\n\n# Results\nprint(\"cover:\", sim_result[\"cover\"])\nprint(\"mean length:\", sim_result[\"mean length\"])\n\n\nC:\\Users\\johan\\AppData\\Local\\Temp\\ipykernel_37716\\636247826.py:14: RuntimeWarning:\n\ninvalid value encountered in scalar divide\n\nC:\\Users\\johan\\AppData\\Local\\Temp\\ipykernel_37716\\636247826.py:14: RuntimeWarning:\n\ndivide by zero encountered in scalar divide\n\n\n\ncover: 0.584\nmean length: 590.431\n\n\nThe coverage is now changed, as it should be. This is the point: the problem is that one is not only making assumptions about the values of the parameter but also about the distribution itself.\nTo sum up, in both these experiments, the model is mis-specified. However, if one is only interested in the mean, the coverage is quite good in one case and quite bad in the other.\nRemember, these assumptions about the distribution need to be made before the experiment starts. Thus, one likely needs to gather some data to estimate the null distribution.\nThe same can be said about the distributions in the composite hypothesis. If I had to work with this in practice, I would conduct some simulation studies.\nA good use case is convergence rates, which can be described by a Bernoulli distribution. This is particularly useful since the Bernoulli distribution only has one parameter, making it behave nicely in terms of distribution. By this, I mean that it should be less sensitive to being misspecified."
  },
  {
    "objectID": "Projects/sequential hypothesis testing and safe anytime inference.html#msprt-aplications",
    "href": "Projects/sequential hypothesis testing and safe anytime inference.html#msprt-aplications",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "mSPRT aplications",
    "text": "mSPRT aplications\nMost of the implented aplications of mSPRT i found is based on (The article)[https://arxiv.org/pdf/1512.04922]\nFor the type II error control. It seam like that by running two test with leve \\(\\frac{\\alpha}{2}\\) this come from A modified sequential probability ratio test. I have desiced with my self i will look into this, if am gona use this in practis. So for now this is out of the scope of this asigment, but the idear is pretty brilliant. if one line the placement is null– alternetiv –True hyposis. if the true hyposis is very far of it should still work but the effect in the alternetiv hyposis will not be true, so the test becomes, more do one see a effect ore not posetiv effect and negativ effect."
  },
  {
    "objectID": "Projects/sequential hypothesis testing and safe anytime inference.html#a-final-remark",
    "href": "Projects/sequential hypothesis testing and safe anytime inference.html#a-final-remark",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "A Final Remark",
    "text": "A Final Remark\nIf I want \\(\\beta\\) control, I would run two SPRT tests with \\(\\alpha\\) and \\(\\beta\\), and I would plot the distributions.\nBefore starting, I would check if there is a region where the fraction \\(\\frac{p(x)}{q(x)}\\) varies significantly. I would also need to adjust \\(\\beta\\) accordingly.\nHowever, if I only want \\(\\beta\\) control, I could run a single live experiment, set the maximum number of points I can afford to sample, and then stop either when the algorithm terminates or when I run out of budget."
  },
  {
    "objectID": "Projects/sequential hypothesis testing and safe anytime inference.html#simulation",
    "href": "Projects/sequential hypothesis testing and safe anytime inference.html#simulation",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "Simulation",
    "text": "Simulation\nBelow i have made small simulation of the the fair game desciped above and the SPRT. both coins af fair meaning their is \\(P(X=head)0.5\\) for both \\(\\alpha = 0.05\\)\n\n\nCode\nimport numpy as np\n\n# Variables for the experiment\np_coin_1 = 0.5\np_coin_2 = 0.5\n\n# Probability of match and no match\np_match = (1 - p_coin_1) * (1 - p_coin_2) + p_coin_1 * p_coin_2\np_no_match = 1 - p_match\n\n# Hypotheses\nmatch = 0.5  # Null hypothesis\nalternative_hypothesis = 0.7  # Alternative hypothesis\n\n# Tolerances\nalpha = 0.05  # Type I error\nbeta = 0.20  # Type II error\n\n# Thresholds\na = np.log(beta / (1 - alpha))\nb = np.log((1 - beta) / alpha)\n\n# Make button\n\n\ndef ber_llr(h_0, h_1, outcome):\n    \"\"\"\n    Calculate the log-likelihood ratio for a Bernoulli outcome.\n    \n    Args:\n        h_0: Null hypothesis probability\n        h_1: Alternative hypothesis probability\n        outcome: Observed outcome (0 or 1)\n        \n    Returns:\n        Log-likelihood ratio for the given outcome.\n    \"\"\"\n    return np.log((h_1**outcome * (1 - h_1)**(1 - outcome)) /\n                  (h_0**outcome * (1 - h_0)**(1 - outcome)))\n\ndef sprt(p_match, h_0, h_1, a, b):\n    \"\"\"\n    Perform Sequential Probability Ratio Test (SPRT).\n    \n    Args:\n        p_match: Probability of a match\n        h_0: Null hypothesis probability\n        h_1: Alternative hypothesis probability\n        a: Lower threshold (log-scale)\n        b: Upper threshold (log-scale)\n        \n    Returns:\n        Dictionary with test results.\n    \"\"\"\n    Lambda = 0  # Cumulative log-likelihood ratio\n    draws = []  # Store outcomes\n    log_likelihood_sum = [0]  # Track likelihood values\n    confirmed_hypothesis = None  # Store confirmed hypothesis\n\n    while Lambda &gt; a and Lambda &lt; b:\n        outcome = np.random.binomial(1, p_match)\n        draws.append(outcome)\n        Lambda += ber_llr(h_0, h_1, outcome)\n        log_likelihood_sum.append(Lambda)\n    \n    # Determine the final hypothesis\n    if Lambda &gt;= b:\n        confirmed_hypothesis = \"Alternative\"\n    elif Lambda &lt;= a:\n        confirmed_hypothesis = \"Null\"\n\n    return {\n        \"confirmed_hypothesis\": confirmed_hypothesis,\n        \"log_likelihood_sum\": log_likelihood_sum,\n        \"draws\": draws,\n        \"length\": len(draws)\n    }\n\n# Run the SPRT\nresult = sprt(p_match, match, alternative_hypothesis, a, b)\n\n\ndef simulate_cover_sprt(p_match, h_0, h_1, a, b,N=1000):\n    cover=[]\n    length=[]\n    for i in range(N):\n        sim_ob=sprt(p_match, h_0, h_1, a, b)\n        cover.append(sim_ob[\"confirmed_hypothesis\"]==\"Null\")\n        length.append(sim_ob[\"length\"])\n    return {\"cover\":np.mean(cover),\"mean length\":np.mean(length)}\n\nsim_result = simulate_cover_sprt(p_match, match, alternative_hypothesis, a, b,10000)\nprint(\"cover\")\nprint(sim_result[\"cover\"])\nprint(\"mean run length\")\nprint(sim_result[\"mean length\"])\n\n\ncover\n0.9523\nmean run length\n18.1838\n\n\nAs can be see the cover i correct of the hyposis."
  },
  {
    "objectID": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html",
    "href": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "",
    "text": "This project was one of those rabbit-hole endeavors. I wanted to learn about A/B testing, which turned out to be a straightforward hypothesis test with some power calculations.\nInterestingly, the calculations for power analysis were concepts I had already learned during my bachelor’s degree. So, I decided to explore some newer methods instead. That led me to sequential testing—a fascinating field. It’s this intriguing mix of betting strategies, martingales, and a lot of other fun concepts.\nI’m mostly writing this project for myself, so I have something to work on and a way to document my learning process. But if someone happens to stumble across this, they should be aware that it can get a bit technical. I’ve included references to the math so I can easily find it again, and I’ve conducted a few small simulation studies. These simulations help me get a sense of the performance of the methods while also allowing me to bend the assumptions a little to see what happens."
  },
  {
    "objectID": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#formally",
    "href": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#formally",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "Formally",
    "text": "Formally\nLet \\(H_0\\) be the null and let \\(H_1\\) be other hyposis\n\\(\\alpha\\) is the proberbillaty of making a type one error meaning\n\\(P(acepting\\: H_1\\: when\\: H_0\\: is\\: true)\\)\n\\(\\beta\\) is the probillaty of type 2 error\n\\(P(acepting\\: H_0\\: when\\: H_1\\: is\\: true)\\)\nIf the distribtion is nice meaning law of large number insure it conveges to normal distribuion.\n\\[n\\geq 2(\\frac{\\phi^{-1}(1-\\alpha-\\phi^{-1}(1-\\beta)}{\\Delta/\\sigma})^2\\]\n\\(\\phi^{-1}\\) is the inverse cumulative standard normal distribution,\n\\(\\Delta\\) is the abeslut differnece in effect betwwen the null \\(H_0\\) and \\(H_1\\).\n\\(\\sigma\\) is the variance."
  },
  {
    "objectID": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#peeking",
    "href": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#peeking",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "Peeking",
    "text": "Peeking\nPeeking is when a one looks at the data as they come in, using standart hyposis test method, and make a conclusion if they dont see a effect. The problem is that it destrou the coverage gaurenty. So if one look at the data as they flow in and end the experiment and before if their is no significans.\nThis idear of if we have hundret experiement, in \\((1-\\alpha)*100\\) the right hypothesis identified is not valid.\nBelow i have made some small simulations to ilustrate the problem\n\\(A=a_1,\\dots,a_n\\) and \\(B=b_1,\\dots,b_n\\) let \\(a_i\\) and \\(b_i\\) iid and let \\(a_i\\sim\\mathbb{N}(0,1)\\) and \\(b_i\\sim\\mathbb{N}(0,1)\\).\nSince \\(A\\) and \\(B\\) has the same distribuion their is no effect. This is sometime calle a A/A test. Sometimes this is used to test method in a live environment. If a useres test method, they should get the type 1 error cover from this.\nAt each point the simulation will draw with no replacment from A ore B calulat the p values and see if the true mean is in the confiden intervall.\n\n\nCode\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nrandom.seed(42)\n# Parameters\nn = 100  # Number of samples in A and B\nnum_simulations = 1  # Number of simulations\nalpha = 0.05  # Significance level\ntrue_mean = 0  # True mean to check against\nvariance=1 # varince know\na=np.random.normal(true_mean,variance,n) # draw\nb=np.random.normal(true_mean,variance,n) # draw\n\ndef get_sigficanlevet(A:list,B:list,A_variance:float=1,B_variance:float=1):\n    mean_A=np.mean(A)\n    mean_B=np.mean(B)\n    Z = (mean_A - mean_B) / np.sqrt(A_variance / len(A) + B_variance/ len(B))\n    p_value = 2 * (1 - norm.cdf(abs(Z)))\n    return p_value\n\n\ndef get_significant_seq(A: list, B: list, A_variance: float = 1, B_variance: float = 1):\n    # Initialize an empty list to store the significant values\n    sig_seq = []\n\n    # Create copies of A and B for sampling without replacement\n    A_copy = np.copy(A)\n    B_copy = np.copy(B)\n\n    # Initialize vectors for the selected samples\n    A_vector = []\n    B_vector = []\n\n    # Sample the first element from A and B\n    A_vector.append(A_copy[0])\n    A_copy = np.delete(A_copy, 0)\n\n    B_vector.append(B_copy[0])\n    B_copy = np.delete(B_copy, 0)\n\n    # Initial significance level calculation\n    sig_seq.append(get_sigficanlevet(A_vector, B_vector))  # Assuming this function exists\n\n    # Run the loop until both lists are empty\n    while len(A_copy) &gt; 0 or len(B_copy) &gt; 0:\n        # Randomly choose whether to sample from A or B\n        if np.random.rand() &gt; 0.5 and len(A_copy) &gt; 0:\n            A_vector.append(A_copy[0])  # Append the first element from A\n            A_copy = np.delete(A_copy, 0)  # Remove the sampled element from A\n        elif len(B_copy) &gt; 0:\n            B_vector.append(B_copy[0])  # Append the first element from B\n            B_copy = np.delete(B_copy, 0)  # Remove the sampled element from B\n        \n        # Calculate and store the significance level at this step\n        sig_seq.append(get_sigficanlevet(A_vector, B_vector))  # Assuming this function exists\n\n    return sig_seq\n\n\ndef plot_significance_seq(sig_seq):\n    # Plot the significance sequence\n    plt.figure(figsize=(10, 6))\n    plt.plot(sig_seq, marker='o', linestyle='-', color='b', label='Significance Level')\n    \n    # Add horizontal line at y=0.05 for the threshold\n    plt.axhline(y=0.05, color='r', linestyle='-', label='Significance Threshold (0.05)')\n    \n    # Add small green line segments for significance values below 0.05\n    for i, value in enumerate(sig_seq):\n        if value &lt; 0.05:\n            plt.plot([i, i], [0, value], color='green', lw=2)  # Line from 0 to the value\n\n    # Set limits for the y-axis between 0 and 1\n    plt.ylim(0, 1)\n    \n    # Adding labels and title\n    plt.xlabel('Sample Number')\n    plt.ylabel('Significance Level')\n    plt.title('Significance Level Sequence with Threshold at 0.05')\n    \n    # Adding a legend\n    plt.legend()\n\nresult=get_significant_seq(a,b)\nplot_significance_seq(result)\n\n\n\n\n\n\n\n\n\nAs can be see from the plot the p values goes up and down doing the experiment, So if one was to stop the experiement when the p value cross a threshold. The cover is not correct, this means in this case that tyoe 1 error alot higer than one exptes.\nSo let’s look at how the coverage is affected by the peaking. Belove i have made small simulation that corespond to making the plot above a 1000 times and with a p value \\(0.05\\).\nI simulate two draws from the same distribution and check if there is a significant difference between them. If a significant difference is found, I stop the experiment and count it as a success. If no significant difference is found, the experiment continues with a new draw, and the same test is conducted. I repeat this process for 100 draws and 1000 experiments. Finally, I sum the number of experiments that show a significant result and divide this sum by the total number of experiments (1000) to calculate the proportion of significant results.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n\n\ndef get_sigficanlevet(A:list,B:list,A_variance:float=1,B_variance:float=1):\n    mean_A=np.mean(A)\n    mean_B=np.mean(B)\n    Z = (mean_A - mean_B) / np.sqrt(A_variance / len(A) + B_variance/ len(B))\n    p_value = 2 * (1 - norm.cdf(abs(Z)))\n    return p_value\n\n\ndef run_significant_seq(A: list, B: list, A_variance: float = 1, B_variance: float = 1):\n    # Initialize an empty list to store the significant values\n\n    # Create copies of A and B for sampling without replacement\n    A_copy = np.copy(A)\n    B_copy = np.copy(B)\n\n    # Initialize vectors for the selected samples\n    A_vector = []\n    B_vector = []\n\n    # Sample the first element from A and B\n    A_vector.append(A_copy[0])\n    A_copy = np.delete(A_copy, 0)\n\n    B_vector.append(B_copy[0])\n    B_copy = np.delete(B_copy, 0)\n    \n    sig_erro=0\n    # Run the loop until both lists are empty\n    while len(A_copy) &gt; 0 or len(B_copy) &gt; 0:\n        # Randomly choose whether to sample from A or B\n        if np.random.rand() &gt; 0.5 and len(A_copy) &gt; 0:\n            A_vector.append(A_copy[0])  # Append the first element from A\n            A_copy = np.delete(A_copy, 0)  # Remove the sampled element from A\n        elif len(B_copy) &gt; 0:\n            B_vector.append(B_copy[0])  # Append the first element from B\n            B_copy = np.delete(B_copy, 0)  # Remove the sampled element from B\n        \n        # Calculate and store the significance level at this step\n        if(get_sigficanlevet(A_vector, B_vector)&lt;0.05):\n            sig_erro=1\n            break \n    return sig_erro\n\n\n\nn = 100  # Number of samples in A and B\nnum_simulations = 1000  # Number of simulations\nalpha = 0.05  # Significance level\ntrue_mean = 0  # True mean to check against\nvariance=1 # varince know\n\nestimated=[]\nfor i in range(1,num_simulations):\n    a=np.random.normal(true_mean,variance,n) # draw\n    b=np.random.normal(true_mean,variance,n) # draw\n    estimated.append(run_significant_seq(a,b))\n\nprint(\"cover\")\nprint(np.mean(estimated))\n\n\ncover\n0.4124124124124124\n\n\nFor these settings, I usually obtain an estimate of around 0.4, which, to me, seems high. Remember, this should be the Type 1 coverage rate, so around 0.05. In practice, a person would likely not peek at the data every time a new point arrives. This is merely intended to illustrate the problem of peeking.\nIf peeking causes problems, the solution seems obvious: avoid peeking. However, in practice, people often do.\nThe literature on the method introduced below discusses designing systems to account for user behavior. This includes cases where users receive credit for findings, even when peeking leads to results that may not be valid.\nThe goal is to develop mathematical models that mitigate the problem of peeking. Another approach is preregistration. That said, there are many valid reasons to evaluate results as data comes in and to stop when a clear conclusion can be drawn."
  },
  {
    "objectID": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#sequential-probability-ratio-test-sprt",
    "href": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#sequential-probability-ratio-test-sprt",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "sequential probability ratio test (SPRT)",
    "text": "sequential probability ratio test (SPRT)\nA little more formal explanation of the SPRT:\nHere, we are only looking at two hypotheses and their corresponding points:\n\n\\(H_0: \\theta = \\theta_0\\), where \\(p(x_i)\\) is the distribution under the null hypothesis.\n\n\\(H_1: \\theta = \\theta_1\\), where \\(q(x_i)\\) is the distribution under the alternative hypothesis.\n\nWe begin with a wealth of one.\nUnder the null hypothesis, the expectation of the likelihood ratio is given by:\n\\[  \n\\mathbb{E}_{q(x)}( \\frac{q(X_i)}{p(X_i)}) = \\int \\frac{p(x)}{q(x)} q(x) \\, dx = \\int p(x) \\, dx = 1  \n\\]\nThe wealth can be written as:\n\\[  \nM_t = \\prod_{i=1}^{T} \\frac{q(X_i)}{p(X_i)}  \n\\]\nThis is a submartingale, and by using Ville’s inequality, we obtain a probability guarantee for the entire process.\nUnder the alternative hypothesis, the inverse of the likelihood ratio will be a submartingale, leading to two thresholds: one for rejecting the null hypothesis \\(H_0\\) and one for rejecting \\(H_1\\).\nNotice how simple the theory is here."
  },
  {
    "objectID": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#simulation",
    "href": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#simulation",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "Simulation",
    "text": "Simulation\nBelow i have made small simulation of the the fair game desciped above and the SPRT. both coins af fair meaning their is \\(P(X=head)=0.5\\) for both \\(\\alpha = 0.05\\)\n\n\nCode\nimport numpy as np\n\n# Variables for the experiment\np_coin_1 = 0.5\np_coin_2 = 0.5\n\n# Probability of match and no match\np_match = (1 - p_coin_1) * (1 - p_coin_2) + p_coin_1 * p_coin_2\np_no_match = 1 - p_match\n\n# Hypotheses\nmatch = 0.5  # Null hypothesis\nalternative_hypothesis = 0.7  # Alternative hypothesis\n\n# Tolerances\nalpha = 0.05  # Type I error\nbeta = 0.20  # Type II error\n\n# Thresholds\na = np.log(beta / (1 - alpha))\nb = np.log((1 - beta) / alpha)\n\n# Make button\n\n\ndef ber_llr(h_0, h_1, outcome):\n    \"\"\"\n    Calculate the log-likelihood ratio for a Bernoulli outcome.\n    \n    Args:\n        h_0: Null hypothesis probability\n        h_1: Alternative hypothesis probability\n        outcome: Observed outcome (0 or 1)\n        \n    Returns:\n        Log-likelihood ratio for the given outcome.\n    \"\"\"\n    return np.log((h_1**outcome * (1 - h_1)**(1 - outcome)) /\n                  (h_0**outcome * (1 - h_0)**(1 - outcome)))\n\ndef sprt(p_match, h_0, h_1, a, b):\n    \"\"\"\n    Perform Sequential Probability Ratio Test (SPRT).\n    \n    Args:\n        p_match: Probability of a match\n        h_0: Null hypothesis probability\n        h_1: Alternative hypothesis probability\n        a: Lower threshold (log-scale)\n        b: Upper threshold (log-scale)\n        \n    Returns:\n        Dictionary with test results.\n    \"\"\"\n    Lambda = 0  # Cumulative log-likelihood ratio\n    draws = []  # Store outcomes\n    log_likelihood_sum = [0]  # Track likelihood values\n    confirmed_hypothesis = None  # Store confirmed hypothesis\n\n    while Lambda &gt; a and Lambda &lt; b:\n        outcome = np.random.binomial(1, p_match)\n        draws.append(outcome)\n        Lambda += ber_llr(h_0, h_1, outcome)\n        log_likelihood_sum.append(Lambda)\n    \n    # Determine the final hypothesis\n    if Lambda &gt;= b:\n        confirmed_hypothesis = \"Alternative\"\n    elif Lambda &lt;= a:\n        confirmed_hypothesis = \"Null\"\n\n    return {\n        \"confirmed_hypothesis\": confirmed_hypothesis,\n        \"log_likelihood_sum\": log_likelihood_sum,\n        \"draws\": draws,\n        \"length\": len(draws)\n    }\n\n# Run the SPRT\nresult = sprt(p_match, match, alternative_hypothesis, a, b)\n\n\ndef simulate_cover_sprt(p_match, h_0, h_1, a, b,N=1000):\n    cover=[]\n    length=[]\n    for i in range(N):\n        sim_ob=sprt(p_match, h_0, h_1, a, b)\n        cover.append(sim_ob[\"confirmed_hypothesis\"]==\"Null\")\n        length.append(sim_ob[\"length\"])\n    return {\"cover\":np.mean(cover),\"mean length\":np.mean(length)}\n\nsim_result = simulate_cover_sprt(p_match, match, alternative_hypothesis, a, b,10000)\nprint(\"cover\")\nprint(sim_result[\"cover\"])\nprint(\"mean run length\")\nprint(sim_result[\"mean length\"])\n\n\ncover\n0.9528\nmean run length\n18.149\n\n\nAs can be see the cover i correct of the hyposis."
  },
  {
    "objectID": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#msprt",
    "href": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#msprt",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "mSPRT",
    "text": "mSPRT\nIn practice, one rarely has a point hypothesis but rather a null and composite hypothesis.\nThe question is often whether there is an effect or not.\nThe composite hypothesis is a combination of multiple hypotheses.\nmSPRT uses a mixture distribution for this:\n\\[\n\\Lambda(X_n) = \\int_{\\theta}\\prod_{i=1}^{n}\\frac{f_{\\theta}(x_i)}{f_{\\theta_0}(x_i)}h(\\theta)d\\theta\n\\]\nThis will be denoted \\(\\Lambda_t\\)\nThe same stopping threshold is used for \\(B\\), meaning type 1 error control is the same since, under the null, the log-likelihood is a submartingale:\n\\[\n\\mathbb{E_{\\theta_0}}\\left(\\frac{f_{\\theta}(x)}{f_{\\theta_0}(x)}h(\\theta)\\right) = \\int f_{\\theta}(x)d\\theta = 1\n\\]\nThis is described as “hedging your bets.” The idea here is that you are betting on multiple hypotheses.\nIt becomes a little more complicated when considering the composite hypothesis, as there are multiple hypotheses, and only one can be true. It is not a submartingale unless one finds a way to adjust the expectation as the process progresses. I have not found such a method. This means the lower threshold for the sequence is not valid.\nWhat one can do instead is to set a limit on the amount of data one wants to gather and set \\(\\beta\\) to zero. This means losing type 2 error control.\nA second option is two experiment with adjusted \\(\\beta\\), using distributions on both sides of the null.\n- If both are stopped by the \\(B\\) criterion, which is related to \\(\\alpha\\), this would suggest the null is true.\n- If one confirms the null and the other rejects it, this would suggest evidence against the null.\nThus, it is easier to simply negate the type 2 error. In practice, the user chooses an \\(\\alpha\\), the tolerance for type 1 error they are willing to accept, and a maximum number of experiments they want to perform or can afford. Then the experiment runs, and if the process is not stopped, the conclusion is that there was no significance.\nBelow i made a small simulation of how one can do this.\n\n\nCode\nimport numpy as np\nfrom scipy.stats import norm\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\", \n    category=RuntimeWarning, \n    message=\"invalid value encountered in scalar divide\"\n)\n\n# SPRT implementation\ndef msprt(l_null, l_composite, N_stopping, alpha, draw_function):\n    draws = []\n    log_likelihood_ratios = []\n    b = np.log(1 / alpha)\n    log_likelihood_ratio = 0\n    decision = \"Null\"\n    for i in range(N_stopping):\n        x = draw_function()\n        draws.append(x)\n        log_likelihood_ratio += np.log(l_composite(x) / l_null(x))\n        log_likelihood_ratios.append(log_likelihood_ratio)\n        if log_likelihood_ratios[i] &gt; b:\n            decision = \"Alternative\"\n            break\n    return {\n        \"confirmed_hypothesis\": decision,\n        \"log_likelihood_sum\": log_likelihood_ratio,\n        \"draws\": draws,\n        \"length\": len(draws),\n    }\n\n# Simulation of coverage\ndef simulate_cover_msprt(l_null, l_composite, N_stopping, alpha, draw_function, N_experiments):\n    cover = []\n    length = []\n    for i in range(N_experiments):\n        sim_ob = msprt(l_null, l_composite, N_stopping, alpha, draw_function)\n        cover.append(sim_ob[\"confirmed_hypothesis\"] == \"Null\")\n        length.append(sim_ob[\"length\"])\n    return {\"cover\": np.mean(cover), \"mean length\": np.mean(length)}\n\n# Likelihood functions\ndef get_likelihood_null(x, mean_null, std_dev_null):\n    return norm.pdf(x, loc=mean_null, scale=std_dev_null)\n\ndef get_likelihood_composit(x, mean_1, std_dev_1, phi_1, mean_2, std_dev_2):\n    return phi_1 * norm.pdf(x, loc=mean_1, scale=std_dev_1) + (1 - phi_1) * norm.pdf(x, loc=mean_2, scale=std_dev_2)\n\n# Parameters\nmean_true = 0\nvariance_true = 1\nstd_dev_true = variance_true ** 0.5\n\n# Simulation\nsim_result = simulate_cover_msprt(\n    l_null=lambda x: get_likelihood_null(x, mean_null=0, std_dev_null=1),\n    l_composite=lambda x: get_likelihood_composit(x, mean_1=-2, std_dev_1=1, phi_1=0.5, mean_2=2, std_dev_2=1),\n    N_stopping=1000,\n    alpha=0.05,\n    draw_function=lambda: np.random.normal(mean_true, std_dev_true),\n    N_experiments=1000\n)\n\n# Results\nprint(\"cover:\", sim_result[\"cover\"])\nprint(\"mean length:\", sim_result[\"mean length\"])\n\n\ncover: 0.986\nmean length: 986.056\n\n\nas can be see the cover is good.\nVille’s inequality can also be used for anytime-valid p-values:\n\\[\np_n = \\min \\left[ 1, \\min \\left( \\tilde{\\Lambda}_t^{-1} : t \\leq n \\right) \\right]\n\\]\nIf one thinks about the equation, it is quite simple. This involves moving a horizontal line down until it hits the highest point of the likelihood ratio trajectory and finding the corresponding \\(\\alpha\\).\nThere are important things to notice here. This way of looking at p-values is very consistent, meaning that if we gather evidence against the null, no matter what data is later gathered, the p-values do not rise again.\nThis stands in contrast to “normal” p-values, which can be inconsistent over time periods.\nA very nice propperty."
  },
  {
    "objectID": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#a-small-point",
    "href": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#a-small-point",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "A Small Point",
    "text": "A Small Point\nThe submartingale property comes under a very specific distribution under the null.\nSo, one is not only testing if \\(\\theta = \\theta_0\\) but also making assumptions about the distribution.\nThe Central Limit Theorem (CLT) is very powerful in the sense that, by taking the mean, many distributions will converge, providing some robustness to the CLT.\nHowever, when looking at every single point and making assumptions about the distribution, that robustness is not present in the same way.\nOne way to combat this is by making batches of data, but this takes away some of the idea of anytime-valid confidence intervals.\nA second approach would be to determine the number of times one is allowed to peek and adjust the \\(\\alpha\\) based on this.\nThese methods suffer from some of the same incentive structure errors as normal p-values. Over-peeking becomes a problem, and people not involved in the analysis cannot see if this has occurred."
  },
  {
    "objectID": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#looking-into-if-the-distribion-mispecified",
    "href": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#looking-into-if-the-distribion-mispecified",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "Looking into if the distribion mispecified",
    "text": "Looking into if the distribion mispecified\nBelow here i have made simulation the true distribuion is a t distribuion mean zero with variance 3 scale 1. The null is normal distribuion with variance 3 and mean zero. The only difference is the and the sape of the distribuionsion. The hyposis test is if the mean is the same.\n\n\nCode\nimport numpy as np\nfrom scipy.stats import norm, t\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\", \n    category=RuntimeWarning, \n    #message=\"invalid value encountered in scalar divide\"\n)\n\n\n# SPRT implementation\ndef msprt(l_null, l_composite, N_stopping, alpha, draw_function):\n    draws = []\n    log_likelihood_ratios = []\n    b = np.log(1 / alpha)\n    log_likelihood_ratio = 0\n    decision = \"Null\"\n    for i in range(N_stopping):\n        x = draw_function()\n        draws.append(x)\n        log_likelihood_ratio += np.log(l_composite(x) / l_null(x))\n        log_likelihood_ratios.append(log_likelihood_ratio)\n        if log_likelihood_ratios[i] &gt; b:\n            decision = \"Alternative\"\n            break\n    return {\n        \"confirmed_hypothesis\": decision,\n        \"log_likelihood_sum\": log_likelihood_ratio,\n        \"draws\": draws,\n        \"length\": len(draws),\n    }\n\n# Simulation of coverage\ndef simulate_cover_msprt(l_null, l_composite, N_stopping, alpha, draw_function, N_experiments):\n    cover = []\n    length = []\n    for i in range(N_experiments):\n        sim_ob = msprt(l_null, l_composite, N_stopping, alpha, draw_function)\n        cover.append(sim_ob[\"confirmed_hypothesis\"] == \"Null\")\n        length.append(sim_ob[\"length\"])\n    return {\"cover\": np.mean(cover), \"mean length\": np.mean(length)}\n\n# Likelihood functions\ndef get_likelihood_null(x, mean_null, std_dev_null):\n    return norm.pdf(x, loc=mean_null, scale=std_dev_null)\n\ndef get_likelihood_composit(x, mean_1, std_dev_1, phi_1, mean_2, std_dev_2):\n    return phi_1 * norm.pdf(x, loc=mean_1, scale=std_dev_1) + (1 - phi_1) * norm.pdf(x, loc=mean_2, scale=std_dev_2)\n\n# Parameters\nmean_true = 0\nvariance_true = 1\nstd_dev_true = variance_true ** 0.5\n\n# Simulation\nsim_result = simulate_cover_msprt(\n    l_null=lambda x: get_likelihood_null(x, mean_null=0, std_dev_null=np.sqrt(3)),\n    l_composite=lambda x: get_likelihood_composit(x, mean_1=-5, std_dev_1=np.sqrt(3), phi_1=0.5, mean_2=5, std_dev_2=np.sqrt(3)),\n    N_stopping=1000,\n    alpha=0.05,\n    draw_function = lambda: t.rvs(df=3, loc=mean_true, scale=1),\n    N_experiments=1000\n)\n\n# Results\nprint(\"cover:\", sim_result[\"cover\"])\nprint(\"mean length:\", sim_result[\"mean length\"])\n\n\ncover: 0.963\nmean length: 963.067\n\n\nAs can be see in this case one actullay get good cover in this case. The simulated cover is close to the teoretical but if I make the null distribuion wide the cover is much lees acurate.\nbelow I have made a small change to the true distribusion so now the scale parater is 2 meaning a vider shape of the t distribusion. So the mean is the same but variance is diffrent.\n\n\nCode\nimport numpy as np\nfrom scipy.stats import norm, t\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\", \n    category=RuntimeWarning, \n    message=\"divide by zero encountered in scalar divide\"\n)\n\n\n# SPRT implementation\ndef msprt(l_null, l_composite, N_stopping, alpha, draw_function):\n    draws = []\n    log_likelihood_ratios = []\n    b = np.log(1 / alpha)\n    log_likelihood_ratio = 0\n    decision = \"Null\"\n    for i in range(N_stopping):\n        x = draw_function()\n        draws.append(x)\n        log_likelihood_ratio += np.log(l_composite(x) / l_null(x))\n        log_likelihood_ratios.append(log_likelihood_ratio)\n        if log_likelihood_ratios[i] &gt; b:\n            decision = \"Alternative\"\n            break\n    return {\n        \"confirmed_hypothesis\": decision,\n        \"log_likelihood_sum\": log_likelihood_ratio,\n        \"draws\": draws,\n        \"length\": len(draws),\n    }\n\n# Simulation of coverage\ndef simulate_cover_msprt(l_null, l_composite, N_stopping, alpha, draw_function, N_experiments):\n    cover = []\n    length = []\n    for i in range(N_experiments):\n        sim_ob = msprt(l_null, l_composite, N_stopping, alpha, draw_function)\n        cover.append(sim_ob[\"confirmed_hypothesis\"] == \"Null\")\n        length.append(sim_ob[\"length\"])\n    return {\"cover\": np.mean(cover), \"mean length\": np.mean(length)}\n\n# Likelihood functions\ndef get_likelihood_null(x, mean_null, std_dev_null):\n    return norm.pdf(x, loc=mean_null, scale=std_dev_null)\n\ndef get_likelihood_composit(x, mean_1, std_dev_1, phi_1, mean_2, std_dev_2):\n    return phi_1 * norm.pdf(x, loc=mean_1, scale=std_dev_1) + (1 - phi_1) * norm.pdf(x, loc=mean_2, scale=std_dev_2)\n\n# Parameters\nmean_true = 0\nvariance_true = 1\nstd_dev_true = variance_true ** 0.5\n\n# Simulation\nsim_result = simulate_cover_msprt(\n    l_null=lambda x: get_likelihood_null(x, mean_null=0, std_dev_null=np.sqrt(3)),\n    l_composite=lambda x: get_likelihood_composit(x, mean_1=-5, std_dev_1=np.sqrt(3), phi_1=0.5, mean_2=5, std_dev_2=np.sqrt(3)),\n    N_stopping=1000,\n    alpha=0.05,\n    draw_function = lambda: t.rvs(df=3, loc=mean_true, scale=2),\n    N_experiments=1000\n)\n\n# Results\nprint(\"cover:\", sim_result[\"cover\"])\nprint(\"mean length:\", sim_result[\"mean length\"])\n\n\ncover: 0.603\nmean length: 608.506\n\n\nThe coverage is now changed, as it should be. This is the point: the problem is that one is not only making assumptions about the values of the parameter but also about the distribution itself.\nTo sum up, in both these experiments, the model is mis-specified. However, if one is only interested in the mean, the coverage is quite good in one case and quite bad in the other.\nRemember, these assumptions about the distribution need to be made before the experiment starts. Thus, one likely needs to gather some data to estimate the null distribution.\nThe same can be said about the distributions in the composite hypothesis. If I had to work with this in practice, I would conduct some simulation studies.\nA good use case is convergence rates, which can be described by a Bernoulli distribution. This is particularly useful since the Bernoulli distribution only has one parameter, making it behave nicely in terms of distribution. By this, I mean that it should be less sensitive to being misspecified."
  },
  {
    "objectID": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#msprt-aplications",
    "href": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#msprt-aplications",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "mSPRT aplications",
    "text": "mSPRT aplications\nMost of the implented aplications of mSPRT i found is based on The article\nFor the type II error control. It seam like that by running two test with leve \\(\\frac{\\alpha}{2}\\) this come from A modified sequential probability ratio test. I have desiced with my self i will look into this, if am gona use this in practis. So for now this is out of the scope of this asigment, but the idear is pretty brilliant."
  },
  {
    "objectID": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#a-final-remark",
    "href": "Projects/sequential-hypothesis-testing and-safe-anytime-inference.html#a-final-remark",
    "title": "Sequential Hypothesis Testing and Safe Anytime Inference",
    "section": "A Final Remark",
    "text": "A Final Remark\nIf I want \\(\\beta\\) control, I would run two SPRT tests with \\(\\alpha\\) and \\(\\beta\\), and I would plot the distributions.\nBefore starting, I would check if there is a region where the fraction \\(\\frac{p(x)}{q(x)}\\) varies significantly. I would also need to adjust \\(\\beta\\) accordingly.\nThere is also an extension of the method called e-variables or an e-process. It seems to rely on the fact that under the null, one can change the \\(H_1\\) distribution and still have a supermartingale. One can also scale the likelihood, which they use to make a betting strategy. The idea is really interesting. The idea of being able to change the hypothesis midway seems a bit suspicious. I have yet to figure out if the new hypothesis will be penalized by the earlier outcome, which would be problematic.\nThere is an article with an overview here.\nA tiny review on e-values and e-processes by Ruodu Wang."
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "",
    "text": "In this project, I will examine a mixture of K scaled and shifted t-distributions.\nThe scaled and shifted t-distributions have the following PDF. \\[\nf(x|\\mu,\\sigma,\\nu)=\\frac{\\Gamma((\\nu+1)/2)}{\\sqrt{\\pi\\nu\\sigma^2}\\Gamma(\\nu/2)}(1+\\frac{(x-\\mu)^2}{\\nu\\sigma^2})^{-(\\nu+1)/2}\n\\]\n\\(\\mu\\in     \\mathbb{R}\\), \\(\\sigma&gt;0\\) and \\(\\nu&gt;0\\)\nWith the mixed distribution.\n\\[\\sum_{k=1}^{K} \\pi_k f(x|\\mu_k,\\sigma_k,\\nu_k)\\]"
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#posterior-probabilities-expectation-step",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#posterior-probabilities-expectation-step",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "Posterior probabilities (Expectation step)",
    "text": "Posterior probabilities (Expectation step)\nThe posterior probabilities correspond to the probability that an outcome is drawn from a given distribution. \\[\nP(Z=\\pi_k|x_i)=\\frac{P(x_i|Z=\\pi_k)P(Z_i=\\pi_k)}{P(x_i)}=\\frac{\\pi_kf(x_i|\\mu_k,\\sigma_k,\\nu_k)}{\\sum_{k=1}^{K}{\\pi_k}f(x_i|\\mu_k,\\sigma_k,\\nu_k)}=w(x_i)\n\\]"
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#maximization-step-m-step",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#maximization-step-m-step",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "Maximization step M step:",
    "text": "Maximization step M step:\nIn the M step, the coefficients \\(\\{\\pi_k,\\theta_k\\}\\) are estimated, where \\(\\theta_k=\\{\\mu_k,\\sigma_k,\\nu_k\\}\\)"
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#estiamtion-of-theta_kmu_ksigma_knu_k",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#estiamtion-of-theta_kmu_ksigma_knu_k",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "Estiamtion of \\(\\theta_k=\\{\\mu_k,\\sigma_k,\\nu_k\\}\\)",
    "text": "Estiamtion of \\(\\theta_k=\\{\\mu_k,\\sigma_k,\\nu_k\\}\\)\nThe estimation of the parameters \\(\\theta_k={\\mu_k,\\sigma_k,\\nu_k}\\) is done by minimizing the negative log-likelihood.\nAs demonstrated below, the problem is that there is no closed-form solution. This arises from the fact that there is a sum in the denominator of the gradient."
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#finding-the-gradient-of-negativ-log-likelyhood-for-sigma_k",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#finding-the-gradient-of-negativ-log-likelyhood-for-sigma_k",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "Finding the gradient of negativ log likelyhood for \\(\\sigma_k\\)",
    "text": "Finding the gradient of negativ log likelyhood for \\(\\sigma_k\\)\n\\[\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k} =- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_k,\\sigma_k\\nu_k)}\\frac{\\partial f(x_i|\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial f(x_i|\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k}=\\frac{-1}{\\sigma}f(x_i|\\mu_k,\\sigma_k,\\nu_k)+\\frac{(\\nu_k+1)(x-\\mu_k)^2}{\\nu_k\\sigma_k^3}\\frac{\\nu_k\\sigma_k^2}{\\nu_k\\sigma_k^2+(x-\\mu_k)^2}f(x_i|\\mu_k,\\sigma_k,\\nu_k)=\\\\\n\\frac{1}{\\sigma_k}f(x_i|\\mu_k,\\sigma_k,\\nu_k)(-1+\\frac{(v_k+1)(x_i-\\mu_k)^2}{\\nu_k\\sigma_k +(x_i-\\mu_k)^2})\n\\end{aligned}\n\\]\nThis leads to the gradient\n\\[\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)  }{\\partial \\sigma_k} = -\\sum_{i=1}^{N}w(x_i)(-1+\\frac{(\\nu_k+1)(x_i-\\mu_k)^2}{\\nu_k\\sigma_k +(x_i-\\mu_k)^2})\\frac{1}{\\sigma_k}\n\\]\nSince the denominator cannot be split, the expression cannot be simplified.\n\\[\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)}{\\partial \\nu_k}=- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_i,\\sigma_i\\nu_i)} \\frac{\\partial f(x|\\mu_k,\\sigma_k,\\nu_k) }{\\partial \\nu_k}\n\\]\nIt can be useful to know that \\[\n\\begin{aligned}\n\\frac{\\partial \\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}}{\\partial \\nu} = \\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma(\\nu_k/2)}-\\frac{\\Gamma((\\nu_k+1)/2)}{2\\Gamma(\\nu_k/2)^2}\\psi(\\nu_k/2)\\\\ =\\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma((\\nu_k+1)/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}-\\frac{\\psi(\\nu_k/2)}{2\\Gamma(\\nu_k/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}\n\\end{aligned}\n\\]\nGiving the derivative.\n\\[\n\\begin{align*}\n\\frac{\\partial f(x_i|\\mu_k,\\sigma_k^2,\\nu_k) }{\\partial \\nu_k}=-\\frac{1}{2\\nu_k}\\frac{\\Gamma((\\nu_k+1)/2)}{\\sqrt{\\pi\\nu_k\\sigma_k^2}\\Gamma(\\nu_k/2)}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{\\frac{-(\\nu_k+1)}{2}}\\\\\n+\\frac{(x_i-\\mu_k)^2(\\nu_k+1)}{\\nu_k\\sigma^2+(x_i-\\mu_k)^2}\\frac{1}{2\\nu_k}\\frac{\\Gamma((\\nu_k+1)/2)}{\\sqrt{\\pi\\nu_k\\sigma_k^2}\\Gamma(\\nu_k/2)}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{\\frac{-(\\nu_k+1)}{2}-1}\n\\\\\n+\\frac{\\partial \\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}}{\\partial \\nu_k}\\frac{1}{\\sqrt{\\pi\\nu_k\\sigma_k^2}}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k \\sigma_k^2})^{\\frac{-(\\nu_k+1)}{2}}\n\\\\ = \\\\\n-\\frac{1}{2\\nu_k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\\n+\\frac{(x_i-\\mu_k)^2(\\nu_k+1)}{\\nu_k\\sigma_k^2+(x_i-\\mu_k)^2}\\frac{1}{2\\nu_k}f(x|\\mu_k,\\sigma_k^2,\\nu_k)\n\\\\\n\\\\ +\\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma((\\nu_k+1)/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}\\frac{1}{\\sqrt{\\pi\\nu_k\\sigma_k^2}}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{-\\frac{\\nu_k+1}{2}}\\\\\n-\\frac{\\psi(\\nu_k/2)}{2\\Gamma(\\nu_k/2)}\\frac{\\Gamma((\\nu_k+1)/2)}{\\Gamma(\\nu_k/2)}\\frac{1}{\\sqrt{\\pi\\nu_k\\sigma_k^2}}(1+\\frac{(x_i-\\mu_k)^2}{\\nu_k\\sigma_k^2})^{-\\frac{\\nu_k+1}{2}}\\\\\n=\\\\\n-\\frac{1}{2\\nu_k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\\n+\\frac{(x_i-\\mu_k)^2(\\nu_k+1)}{\\nu_k\\sigma^2+(x_i-\\mu_k)^2}\\frac{1}{2\\nu_k}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\\\\\n+\\frac{\\psi((\\nu_k+1)/2)}{2\\Gamma((\\nu_k+1)/2)}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)-\\frac{\\psi(\\nu_k/2)}{2\\Gamma(\\nu_k/2)}f(x_i|\\mu_k,\\sigma_k^2,\\nu_k)\n\\end{align*}\n\\]\nGiving the gradient of the likelihood.\n\\[\n\\begin{aligned}\n\\frac{\\partial l(\\mu_k,\\sigma_k,\\nu_k)}{\\partial \\nu_k}=- \\sum_{i=1}^{N}\\frac{\\pi_k}{\\sum_{k}^{K}\\pi_{k}f(x_i|\\mu_i,\\sigma_i\\nu_i)} \\frac{\\partial f(x_i|\\mu_k,\\sigma_k^2,\\nu_k) }{\\partial \\nu_k}\\\\ =\n-\\sum_{i=1}^{N}\\frac{-1}{2}w(x_i)+\\frac{(\\nu_k+1)}{2}\\frac{(x_i-\\mu_k)^2}{ \\nu_k^2\\sigma_k^2}w(x_i)+\\frac{\\psi((\\nu+1)/2)}{2\\Gamma((\\nu+1)/2)}w(x_i)-\\frac{\\psi(\\nu/2)}{2\\Gamma(\\nu/2)}w(x_i)\\\\\n\\end{aligned}\n\\]\nAll the gradients are computed. The goal is to obtain the maximum likelihood, but there is no closed-form solution. For some special cases, the functions might be approximated nicely. However, the gradients can still be used for gradient descent, which is the method I have chosen."
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#undershooting-the-gradient-out-of-domain.",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#undershooting-the-gradient-out-of-domain.",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "Undershooting the gradient out of domain.",
    "text": "Undershooting the gradient out of domain.\nIf gradient descent undershoots and sets \\(\\nu_i&lt;0\\) ore \\(\\sigma_i&lt;0\\), the density is not defined. This is handled by setting a minimum value for the parameters \\(\\nu_i\\) and \\(\\sigma_i\\). This means that the minimum value for these parameters is not zero but a value close to zero.\nUndershooting can also cause problems when using built-in optimization tools. A pro tip is that if you encounter NaN values as output during optimization, it’s a good idea to start debugging with this issue in mind. In my experience, there is not always a built-in check for these cases."
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#gradient-explotion",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#gradient-explotion",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "Gradient explotion",
    "text": "Gradient explotion\nThis does not fix the problem of gradient explosion. What happens is that the gradient decent may take too large a step in either direction. To make matters worse, the sum in the expression for the gradients can cause the gradients to grow as the sample size increases, which can lead to gradient explosion.\nTo combat this, I have tried different strategies. Below are the two method i tried described."
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#normalising-the-gradient",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#normalising-the-gradient",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "Normalising the gradient",
    "text": "Normalising the gradient\nWhen analyzing the gradient, it’s important to note that the sum is not divided by the size of \\(X\\)(denoted as \\(N\\)). This means that the absolute value of the gradient grows with \\(N\\). If gradient descent is used with a fixed learning rate \\(\\alpha\\), problems may arise where the gradient either undershoots or overshoots. In an implementation setting, this issue is very apparent. However, in real-world problems where the true parameters are unknown, it is impossible to know if the estimated values are accurate.\nThere are two methods to address this issue: one is to check the norm of the gradient to see if you are near a local optimum, and the other is to increase the number of iterations. I have chosen to normalize the gradient by dividing by \\(N\\). This corresponds to choosing \\(\\alpha\\) based on the size of \\(X\\)."
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#gradient-clipping",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#gradient-clipping",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "Gradient clipping",
    "text": "Gradient clipping\nGradient clipping works by setting a maximum value for the norm of the gradient. Specifically, if \\(|\\nabla l(X,\\pi,\\theta)|&gt;c\\) then the gradient is scaled to \\(\\nabla l=c*\\frac{\\nabla l}{|\\nabla l|}\\). This allows you to set a maximum value for the norm of the gradient step.\nSince this method is based on the norm of the entire gradient, it would be beneficial to vary the clipping based on the type of parameter \\(\\{\\mu,\\sigma,\\nu\\}\\)., especially for \\(\\mu\\). I have implemented a version where the clipping is based on different parameters.\nIn testing, it seems that gradient clipping is easy to use and works better. Therefore, in the implementation, gradient clipping is used.\nBelow is a summary of the gradient descent approach #Gradient decent. Gradient descent is used in the M step to optimize the parameters given the weights.\nThere is no closed-form solution for many of the gradients. To make matters worse, the sum in the expression for the gradients can cause the gradients to grow as the sample size increases, potentially leading to exploding gradients. If gradient descent undershoots and sets \\(\\nu_i&lt;0\\) or \\(\\sigma_i&lt;0\\), the density is not defined. I have addressed the issue of undershooting by setting a minimum value for the parameters \\(\\nu_i\\) and \\(\\sigma_i\\). This means that the minimum is not zero but a value close to zero.\nHowever, this does not solve the problem of overshooting gradients. The solution to this issue is normalization. One strategy is to normalize the gradient by the sample size N. Another solution is to use gradient clipping. Gradient clipping works by setting a maximum value for the norm of the gradient. Specifically, if \\(|\\nabla l|&gt;c\\), then the gradient is set to \\(\\nabla l=c*\\frac{\\nabla l}{|\\nabla l|}\\). This allows you to set a maximum value for the norm of the gradient step. Since this method is based on the norm of the entire gradient, it would be beneficial to vary the clipping based on the type of parameter \\(\\{\\mu,\\sigma,\\nu\\}\\), especially for \\(\\mu\\).\nI have implemented a version where the clipping is based on different parameter types. Specifically, if the gradient matrix column corresponding to a given parameter is normalized by its corresponding vector and scaled by a given constant for that parameter. If this explanation is confusing, refer to the function Gradient_clipping_vec in the Rcpp file, which is fairly self-explanatory.\nAdditionally, I have included a check to see if a steep step leads to smaller values in the objective function. If not, the gradient is scaled again by \\(\\alpha\\).\n\nImplentatsion notes for gradient decent\nSince the EM algorithm is an iterative process and R handles loops poorly, Rcpp is used to run C++ code for faster implementation. This is a common approach in many libraries."
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#combining-it-all-to-one-em-function.",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#combining-it-all-to-one-em-function.",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "Combining it all to one EM function.",
    "text": "Combining it all to one EM function.\nSince the EM algorithm is a two-step optimization process, it is not strictly necessary to find the smallest value in the M step. The critical aspect is that both steps lead to a decrease in the negative likelihood.\nIn terms of implementation, this affects the settings used for running gradient descent. For example, setting a maximum number of iterations too high can result in very long runtimes.\nI have not found any definitive guidelines for tuning these parameters in general settings, so I have made it possible for the end user to adjust them as needed."
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#initialization",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#initialization",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "Initialization",
    "text": "Initialization\nThe EM algorithm requires starting parameters, and if they are too far off, it can affect the runtime. I have chosen to use the hard clustering technique, k-means++, which provides a set of partitions. In each of these partitions, the scaled and shifted t-distributions are fitted and used as the starting parameters.\nK-means++ works similarly to k-means, with the primary difference being in the initialization process. The goal of k-means is to minimize the objective function:\n\\(\\underset{\\mu}{\\arg\\min}\\sum_{k=1}^{K}\\sum_{i=1}^{N}|x_i-\\mu_k|^2\\) K-means++ improves on this by initially selecting one center randomly and then iteratively assigning points to the nearest center and updating the centers. It can be shown that this process will decrease the objective function, making it a greedy algorithm."
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#gridsearch-initialization",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#gridsearch-initialization",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "GridSearch Initialization",
    "text": "GridSearch Initialization\nIn most cases, the method of using K-means partitions to estimate \\(\\mu\\) and using the proportions to estimate \\(\\pi\\) seems reasonable. So, I have implemented a version where GridSearch is used for the parameters \\(\\sigma\\) and \\(\\nu\\), which is what caused problems before. By doing this, one gets the advantage of the simplicity and relative speed of using K-means while obtaining usable starting parameters for \\(\\sigma\\) and \\(\\nu\\)."
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#test-with-second-initialization",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#test-with-second-initialization",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "Test with second initialization",
    "text": "Test with second initialization\n\n\nCode\nset.seed(3123)\ndraws=Draw_mixture_componet(Mu=c(-1,4,10))\n\nmodel&lt;-EM_Mix_T_Dist(draws,K=3,Start_method_optim = F)\n\n\nPlot_mix_t_distribution(draws,model$Pi,model$Mu,model$Sigma,model$Nu,bins=50)\n\n\n\n\n\n\n\n\n\nThe method based on the grid search is computational expensiv but can give good result."
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#why-the-initialization-fails.",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#why-the-initialization-fails.",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "Why the initialization fails.",
    "text": "Why the initialization fails.\nIf the starting values are too extreme, such as \\(\\nu_i=50000\\), the gradient cannot be computed properly due to issues with the gamma function or its derivative. As it stands, the initialization can produce such problematic values if the data from k-means is not well-behaved. If the distribution is not well-separated, the hard clustering technique will not capture enough of the distribution in the decreasing part as the function moves out. This can lead to excessively high values of \\(\\nu\\)."
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#model-selection",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#model-selection",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "Model selection",
    "text": "Model selection\nFurther improvements should be made to the initialization process, as this would enable automatic model selection based on AIC or BIC."
  },
  {
    "objectID": "Projects/Mixture of K scaled and shifted t-distributions.html#confidence-bands",
    "href": "Projects/Mixture of K scaled and shifted t-distributions.html#confidence-bands",
    "title": "Mixture of K scaled and shifted t-distributions",
    "section": "confidence bands",
    "text": "confidence bands\nImplementing confidence bands using a parametric bootstrap approach would be beneficial. Their is small complication since \\(\\pi_i\\) label ambiguity. For faster implementation, the bootstrap process could utilize a “hot start,” meaning setting the starting values for the parameters to the estimated values.\n\nInitialization\nThe initialization could run in parallel for both the grid search and the optimization method. It seems like R won’t import functions defined in a separate Rcpp file to the cluster. There are two options: define the necessary functions and dependent functions directly in R via the RcppFunction, or do the parallelization in Rcpp. Doing the parallelization directly in Rcpp would be better.\n\n\nGradient decent.\nIt would be nice if the gradient descent could use more iterations in the later steps. This is fairly easy to implement, but for an end-user and for me, it’s hard to set a good standard setting. Potentially, some of the same effect can be achieved by increasing the number of iterations since if the weights don’t change, it would almost be the same.\nFor now the project is closed."
  },
  {
    "objectID": "Projects/Running least squares.html",
    "href": "Projects/Running least squares.html",
    "title": "Update rules for least squares",
    "section": "",
    "text": "Introduction\nIn this project, I will explore fitting a running linear model as well as running predictions.\nDuring my bachelor’s studies, I focused on the prediction of time series. The model I used was fitted using least squares. I worked with FFP3, which is implemented in R.\nAt that time, I used the Forecast package but ended up implementing the estimator from the ground up. As I have improved my programming and mathematical skills, I thought it would be fun to revisit this problem and see if I could develop a better solution.\nRob how wrote FFP3, has a blog their i sais their is trick to speeding op the calculation of rolling predition for AR process, and I think I figured it out.\nLest squres can be written as\n\\[\\underset{\\beta}{argmin} ||Y-X\\beta||^2\\]\nSince this is a linear function the problem above i convex with one uniq closed form solution\n\\[\\hat{\\beta}=(X^TX)^{-1}X^TY\\]\n\n\nRunning Prediction\nWhen using a running model, a simple solution is to add a new data point and fit the model again. This approach works but is not very efficient.\nInstead, let’s consider whether we can update the \\(\\beta\\) estimates directly.\nOne option is the Recursive least squares filter\nLastly, let’s explore how this can be used.\nLet \\(x[i]\\) represent the i-th index of the vector.\nFor \\(XX^T\\) a rank-1 update can be expressed as:\n\\[(X_{new}X_{new}^T)=(X_{old}^TX_{old})+x_{new}^Tx_{new}\\]\nThis becomes clear when the product is written out explicitly.\nLook at \\((X^TX)[j,g]=x_j^Tx_g\\) So \\(\\sum_{i=1}^{n}x_{j}[i]x_{g}[i]\\)\nNext, consider a given entry in the inner product. \\((X_{old}X_{old}^T)_{[j,g]}=x_j^Tx_g\\) So \\(\\sum_{i=1}^{n}x_{j}[i]x_{g}[i]\\)\nNow look at.\n\\((X_{new}X_{new}^T)[j,g]=(\\sum_{i=1}^{n}x_{j}[i]x_{g}[i])+x_{new}[j]x_{new}[g]\\)\nuse Sherman–Morrison formula\nFor the update of \\((X^TX)\\)\n\\[(X_{new}^TX_{new})^{-1}=(X_{old}^TX_{old}+x_{new}^Tx_{new})^{-1}=(X_{old}^TX_{old})^{-1}-\\frac{(X_{old}^TX_{old})^{-1}x_{new}^Tx_{new}(X_{old}^TX_{old})^{-1}}{1+x_{new}(X_{old}^TX_{old})^{-1}x_{new}^T}\\]\nLet’s look at the nex part of the expresion\nFor \\(B=X^TY\\) the udpate rule is \\[B=(X_{new}^TY_{new})=(X_{old}^TY_{old})+x_{new}*y_{new}\\]\nnote here that \\(y_{new}\\) act as a scale\n\n\nUse Cases\nRolling prediction can be used for the evaluation of time series.\nHowever, these techniques can also be used for the quick implementation of a sliding window estimator or to update a linear model as new data arrives. Additionally, this method provides a fast approach for leave-one-out evaluation of predictive strength.\nI will create one implementations, where I use this for rolling prediction.\nThe method could be used for making a implements of a sliding window.\nFor the sliding window, it is necessary not only to add the new row but also to remove the old one.\n\n\nCode\nimport numpy as np\nimport unittest\n\ndef get_rank_updated_matrix(A: list, v: list, u: list):\n    return A + v.T @ u\ndef test_get_rank_updated_matrix():\n    A_test=np.array([[1,0],[0,1]])\n    v_test=np.array([[1,1]])\n    u_test=np.array([[1,1]])\n    assert (np.all(get_rank_updated_matrix(A_test,v_test,u_test)==np.array([[2, 1], [1, 2]])))\n    return\ntest_get_rank_updated_matrix()\n\n\ndef test_get_inverse_updated_matrix_remove_row():\n    X_T_inner_X_inverse_test=np.array([[1,1],[1,1]])\n    x_test=np.array([[1,1]])\n    assert np.all(get_inverse_updated_matrix(X_T_inner_X_inverse_test,x_test)==np.array([[1,1],[1,1]])+4/5)\n    return\n\ndef get_inverse_updated_matrix_remove_row(X_T_inner_X_inverse: list, x: list):\n    return X_T_inner_X_inverse + X_T_inner_X_inverse @ (x.T @ x) @ X_T_inner_X_inverse / (1 + x @ X_T_inner_X_inverse @ x.T)\n\n\ndef get_inverse_updated_matrix(X_T_inner_X_inverse: list, x: list):\n    return X_T_inner_X_inverse - X_T_inner_X_inverse @ (x.T @ x) @ X_T_inner_X_inverse / (1 + x @ X_T_inner_X_inverse @ x.T)\n\ndef test_get_inverse_updated_matrix():\n    X_T_inner_X_inverse_test=np.array([[1,1],[1,1]])+4/5#np.array([[1,1],[1,1]])\n    x_test=np.array([[1,1]])\n    assert np.all(get_inverse_updated_matrix_remove_row(X_T_inner_X_inverse_test,x_test)==np.array([[1,1],[1,1]]))\n    return\n\n\ndef get_least_sqrd_closed_form_parameter(X: list, Y: list):\n    return np.linalg.inv(X.T @ X) @ X.T @ Y\n\ndef test_get_least_sqrd_closed_form_parameter():\n    X_test=np.array([[1,0],[0,1]])\n    Y_test=np.array([[1],[1]])\n    assert np.all((get_least_sqrd_closed_form_parameter(X_test,Y_test))==[[1],[1]])\n    return\ntest_get_least_sqrd_closed_form_parameter()\n\ndef get_least_sqrd_A_B_parameter(A: list, B: list):\n    return A @ B\n\ndef test_get_least_sqrd_A_B_parameter():\n    A_test=np.array([[1,0],[0,1]])\n    B_test=np.array([[1,0],[0,1]])\n    assert np.all(get_least_sqrd_A_B_parameter(A_test,B_test)==[[1,0],[0,1]])\n    return\ntest_get_least_sqrd_A_B_parameter()\n\ndef get_matrix_update_B(B: list, x: list, y:float):\n    return B + x.T *y\n\ndef test_get_matrix_update_B():\n    B_test=np.array([[1,1]])\n    x_test=np.array([[1,1]])\n    y_test=2.0\n    assert np.all(get_matrix_update_B(B_test,x_test,y_test)==np.array([[3.0,3.0]]))\n    return\ntest_get_matrix_update_B()\n\ndef get_matrix_update_B_remove_row(B: list, x: list, y:float):\n    return B - x.T *y\n\ndef get_matrix_update_B_remove_row():\n    B_test=np.array([[1,1]])\n    x_test=np.array([[1,1]])\n    y_test=1.0\n    assert np.all(get_matrix_update_B_remove_row(B_test,x_test,y_test)==np.array([[0.0,0.0]]))\n    return\ntest_get_matrix_update_B()\n\ndef get_prediction(A, B, X):\n    return X@get_least_sqrd_A_B_parameter(A, B)\n\ndef test_get_prediction():\n    x_test = np.array([[1, 0], [1, 1]])\n    y_test = np.array([[1], [2]])\n    A_test = np.linalg.inv(x_test.T @ x_test)\n    B_test = x_test.T @ y_test\n    assert np.all((get_prediction(A_test, B_test, np.array([[1, 2]])))==[[3]])\ntest_get_prediction()\n\n\n\n\nCode\ndef rolling_prediction(x: list, y: list, starting_length: int):\n    # Ensure x is 2D array with shape (n_samples, n_features)\n    x_array = np.array(x).reshape(-1, 1) if len(np.array(x).shape) == 1 else np.array(x)\n    y_array = np.array(y).reshape(-1, 1) if len(np.array(y).shape) == 1 else np.array(x)\n    \n    length = len(x_array)\n    prediction = np.zeros(length - starting_length)\n    los_score = np.zeros(length - starting_length)\n\n    # Initialize A and B matrices\n    initial_x = x_array[:starting_length]\n    initial_y = y_array[:starting_length]\n    \n    A = np.linalg.inv(initial_x.T @ initial_x)\n    B = initial_x.T @ initial_y\n\n    # Initial prediction\n    next_x = x_array[starting_length].reshape(1, -1)  \n    prediction[0] = get_prediction(A, B, next_x)\n    los_score[0] = y_array[starting_length] - prediction[0]\n    \n    # Rolling update for predictions and loss scores\n    \n    ind = starting_length + 1\n    current_x = x_array[ind].reshape(1, -1)  \n    for i in range(length - starting_length - 1): \n        ind = starting_length + i\n        current_x = x_array[ind].reshape(1, -1) \n        # Update matrices\n        A = get_inverse_updated_matrix(A, current_x)\n        B = get_matrix_update_B(B, current_x, float(y_array[ind]))\n        # Make prediction for next point\n        next_x = x_array[ind + 1].reshape(1, -1)  \n        prediction[i + 1] = get_prediction(A, B, next_x)\n        los_score[i + 1] = y_array[ind + 1] - prediction[i + 1]\n    return {\"prediction\": prediction, \"los_score\": los_score}\n\n\n\n\nTesting the Application\nSince the underlying model is not changing, this provides a straightforward way to test the model. The residuals should closely resemble the errors.\nHowever, they will not be identical, as this would require the coefficients to be perfectly accurate.\n\n\nCode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#Dianostic plot \nimport matplotlib.pyplot as plt\n\nstart_val=500\nx = np.random.rand(1000)  \nerror=np.random.normal(size=1000)  \ny = 2*x + error\n\n# Run the prediction\nresult = rolling_prediction(x, y, start_val)\n\n\nplt.hist(result['los_score'],alpha=0.5)\n\nplt.hist(error[(start_val):],alpha=0.5)\nplt.legend([\"Residuals\", \"True error\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWhen the Application Fails\nThe reader should note that I have not included an intercept in the data-generating process. Below, I have conducted the same test, but this time I have added a constant to the expression.\n\n\nCode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#Dianostic plot \nimport matplotlib.pyplot as plt\n\nstart_val=500\nx = np.random.rand(1000)  \nerror=np.random.normal(size=1000)  \ny =10+ 2*x + error\n\n# Run the prediction\nresult = rolling_prediction(x, y, start_val)\n\n\nplt.hist(result['los_score'],alpha=0.5)\n\nplt.hist(error[(start_val):],alpha=0.5)\nplt.legend([\"Residuals\", \"True error\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nAs can be see now the aplication fails.\nThis can be fixed by adding a column of ones to the model matrix. I would also need to update all the functions to accommodate this change. Without adjusting for this, the dimensions will not match when adding the point to the inverse.\nI will stop the project here. This was intended to be a small NumPy project. If I were to create functions that allow specifying a model, I might develop it more properly and let the user specify the model through an expression, similar to how linear models are specified in R. Since all model specifications in linear models are based on modifying the model matrix, this could provide flexibility."
  }
]