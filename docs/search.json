[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Purpose",
    "section": "",
    "text": "This website serves as a platform to showcase the projects I’ve worked on. My hope is that they will contribute to achieving a Pareto optimal solution, in the sense that they may be of use to others and is free for me to share."
  },
  {
    "objectID": "index.html#languages",
    "href": "index.html#languages",
    "title": "Purpose",
    "section": "Languages",
    "text": "Languages\nI am native Danish, so it is possible that some of the content will be in Danish.\nThis also means that the English may not always be flawless"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here are some of my projects:\n\nBee a Spelling Genius\nBee a Spelling Genius\nThis is a live application made with the Shiny for Python version.\nThe intention was to create a small project and learn about reactive programming, as well as how to develop small applications and dashboards. By using Shiny for Python, I could run it directly on my GitHub page, making it easier to share with others.\n\n\nMixture of K scaled and shifted t distributions\nMixture of K scaled and shifted t distributions\nThe implementation was created using a combination of R and C++ via the Rcpp library. The report was written in RMarkdown.\nThis project is based on an assignment from one of my classes at KU. I enjoyed it because it had a nice mixture of theoretical calculations, numerical methods, and coding. When I first worked on it, I encountered many problems due to the steep gradient. It was rewarding to revisit it and learn how to handle these challenges. I also got to use C++ and see how it improved computation time.\nThis was one of those projects where the final product was very clean, but the process involved a lot of troubleshooting and learning about program interactions, which was a lot of fun.\nFor example, I used C++ overloaded functions, but due to R not allowing functions with the same name, I ran into issues. When I tested my R implementation with a set output, it worked, and when I tested my C++ implementation, it also worked. It’s easy to understand once you figure it out, but it was challenging until then.\nA small note: this project is a bit technical. If you’ve stumbled upon it and want some intuition about what’s going on, I recommend starting by looking at Gaussian mixtures. Since Gaussian mixtures have closed-form solutions, the expressions are much simpler, which makes understanding the intuition easier.\nFor t-distributions, there is no closed form—or at least I couldn’t derive one—so I had to use numerical methods to compensate. This is why the project ends up focusing heavily on implementing fast code using C++. It also involves addressing numerical issues such as unstable gradients and explaining why the function is not convex. If it were convex, I would have used Newton’s method.\nThis challenge was also what made the project interesting to me. Anyone can fit a Gaussian mixture, which is why it’s so widely used. However, tackling numerical methods and figuring out how to implement solutions when the expressions don’t cooperate is a central skill I wanted to develop.\n\n\nVP-search trees for cgMLST sequences\nVP-search trees for cgMLST sequences\nThis is a project I completed for Statens Serum Institut. The goal was to develop a theoretical approach for more efficient searches in their database for cgMLST analyses. I ended up creating an adaptation of VP-search trees based on Hamming distance. Due to the significant number of missing values in the sequences, I had to modify the method and prove some minor bounds. The underlying problem was how to optimize repeated in-range searches, which are, by nature, very similar to KNN.\nI implemented this in Python, making it usable for their system.\nThe department had a database with the data but conducted data processing in Python. Therefore, it might be more efficient to write code that worked closer to the database. Consequently, the report structure became a bit unconventional.\nI wrote extensive documentation for all functions and implemented everything from scratch to ensure it could be adapted to different programming languages where Python libraries might not be flexible.\nI also conducted speed testing on a smaller, published dataset to showcase how the implementation could be evaluated.\n\n\nPrediction of House Prices\nPrediction of House Prices\nThis is a small project I did because I wanted to learn XG-Boost, Lasso and work on a practical prediction project. Kaggle has an open dataset where one can practice. It turns out that their version of advanced regression techniques boils down to a crazy amount of data cleaning and feature engineering, which is common in real-world problems, but not necessarily needed for a practice case. This is a common practice project, and various versions can be found online. I did some visualizations a bit differently, as well as hierarchical clustering to reduce variables.\nThe feature engineering and cleaning process is extensive but could be improved. Since this was not the main goal of my project. I achieved a score of 0.13755, which was considered a rather good score before people started inflating the scores by exploiting the dataset, which has been used multiple times, and feeding it into the validation set.\nI made this notebook using R and created a small interface to interact with the plot. Otherwise, the notebook would have too many blocks of variables.\n\n\nA Toy Model for Market Dynamics: Building a Agent-Based Frameworks\nA Toy Model for Market Dynamics: Building a Agent-Based Frameworks\nThis project explores the foundational concepts of agent-based simulations through a simplified market model. Using Python, it simulates shoppers navigating a fictional representation of Strøget, Copenhagen’s shopping district. Each shopper is modeled as an agent with specific preferences, budget constraints, and behaviors, while shops act as agents offering products in distinct categories.\nThe project emphasizes structural design choices, such as implementing interactions (e.g., buying decisions) and tracking aggregated metrics like total sales. It also addresses technical challenges, including managing dynamic effects and considering potential parallelization issues, though these aspects have not yet been fully implemented.\nThe implementation is written in Python and includes unit tests to verify the functionality of key components. Primarily intended as a learning exercise, this model aims to refine object-oriented programming skills and simulate agent interactions. It serves as a foundation for exploring more complex systems and optimizing efficiency in future iterations."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m interested in the topics of economics, machine learning(prediction), statistics(inference), and coding."
  },
  {
    "objectID": "about.html#my-background",
    "href": "about.html#my-background",
    "title": "About",
    "section": "My background",
    "text": "My background\nMaster of Science (statistics)\nBachelor math economics\nBoth from University of Copenhagen(KU)"
  },
  {
    "objectID": "Bee_a_Spelling_Guinness.html",
    "href": "Bee_a_Spelling_Guinness.html",
    "title": "Projects",
    "section": "",
    "text": "Here are some of my projects:\n\nMixture of K scaled and shifted t distributions\n\nMixture of K scaled and shifted t distributions\n\nThe implementation is made using a combination of R and C++ via the Rcpp library.\n\n\nVP-search trees for cgMLST sequences\n\nVP-search trees for cgMLST sequences\n\nThe implementation is made in python and the report is written in jupyter notebooks."
  },
  {
    "objectID": "docs/projects.html",
    "href": "docs/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here are some of my projects:\n\nMixture of K scaled and shifted t distributions\n\nMixture of K scaled and shifted t distributions\n\nThe implementation is made using a combination of R and C++ via the Rcpp library.\n\n\nVP-search trees for cgMLST sequences\n\nVP-search trees for cgMLST sequences\n\nThe implementation is made in python and the report is written in jupyter notebooks."
  },
  {
    "objectID": "aplications.html",
    "href": "aplications.html",
    "title": "Applications",
    "section": "",
    "text": "For now, this site contains only one application built with Shiny. I may extend it in the future if I create more."
  },
  {
    "objectID": "aplications.html#about",
    "href": "aplications.html#about",
    "title": "Applications",
    "section": "About",
    "text": "About\nThis is a interactive app made to, help/improve player’s of New York times Spelling Bee.\nIn the spelling bee, the player is given seven letters. One letter is marked in yellow.\nThe goal is to make as many words as possible from the seven letters. Every word should contain the yellow letter.\nScoring\n\nA word of 4 letters earns one point.\nWords longer than 4 letters earn additional points (the exact amount is to be determined).\n\nThe dictionary used is provided Here under an open license.\nThe Spelling Bee uses a list of curated words. The dictionary is more like a reference for how words are spelled. For example, “AAAA” might appear in the dictionary but is unlikely to be included in the Spelling Bee.\nThere may also be words that are offensive or outdated. While they are correctly spelled, they probably won’t appear in the Spelling Bee."
  },
  {
    "objectID": "aplications.html#version-with-the-code",
    "href": "aplications.html#version-with-the-code",
    "title": "Applications",
    "section": "Version with the code",
    "text": "Version with the code\nHere is a version where the code can be seen\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n#| components: [editor, viewer]\n\nfrom shiny import *\n# For plot\nimport matplotlib.pyplot as plt\n# Get all words in the english diconary\nimport numpy as np\nimport requests\n# URL of the text file on GitHub\nurl = \"https://raw.githubusercontent.com/dwyl/english-words/refs/heads/master/words.txt\"\n\n# Fetch the file content\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    # Read the content of the file\n    file_content = response.text\n    # Split the content into a list of words\n    words_list = file_content.splitlines()\n    print(words_list)\nelse:\n    print(\"Failed to retrieve the file:\", response.status_code)\n\ndef check_word_canidat_early_stopping(\n        word:str,\n        yellow_chr:chr,\n        chr_list:list):\n    \"\"\"\n    This function return True ore False.\n    To be valid candidat a get true retrun, the input word\n    need to contain the yellow chr, and can not contain other chr that yellow and chr from the list of charater list\n    \n    Args:\n        word (str): The word that need to be ckecked.\n        yellow_chr (chr): a chr that all word must contain\n        chr_list (list): a list of chr that the word can be contain in.\n    Returns:\n        bool: True ore False based on if the word forfiels the conditions\n   \n    \"\"\"\n    lover_yellow =yellow_chr.lower()\n    return_obj=False\n    is_word_combination_of_chr_list=False\n    is_yellow_chr_contained= False\n    lower_chars = [char.lower() for char in chr_list]\n    \n    for letter in word:\n        is_letter_contained= letter.lower() in lower_chars\n        if (is_letter_contained==False):\n            is_word_combination_of_chr_list=False\n            break\n        else:\n            is_word_combination_of_chr_list=True\n        if (is_yellow_chr_contained==False):\n            is_yellow_chr_contained=(lover_yellow==letter.lower())\n    \n    if(is_word_combination_of_chr_list==True and is_yellow_chr_contained==True):\n        return_obj =True\n    return return_obj\n\n\n\ndef get_word_candidates(\n        word_list:list,\n        yellow_chr: chr,\n        chr_list:list,\n        min_word_length:int = 4):\n    \"\"\"\n    This returns list containg alle candidat words, for the spelling bee game.\n\n    All word contain the yellow letter, and consist of combination yello and letter from the letter list.\n    All words is longer ore equal to min_word_length\n    \n    Args:\n        word_list (list): a list with string containg words to serach though\n        yellow_chr (chr): all word in return should contain this letter\n        chr_list (list): a list of chr that can be conatin a word in return\n    Returns:\n        list: a list of strings. all word should be combination of yellow chr and letter from the list.\n        \n    \"\"\"\n    return_objet = []\n    index_list=[] #this list contais the index of wich words is potensiel words in the index list.\n    allowed_chartes= chr_list + [yellow_chr]\n    # len word is not calculated but store in the type string so it fast to check if len of the word is les than min first\n    for i in range(len(word_list)):\n        word_for_examination=word_list[i]\n        if(len(word_for_examination)&gt;=min_word_length):\n            if(check_word_canidat_early_stopping(word_for_examination,yellow_chr,allowed_chartes)==True):\n                return_objet.append(word_for_examination)\n                index_list.append(i)\n    return(return_objet)\n\n\n# Generat plots\n\ndef get_len_word_list(words_list:int):\n    list_len=[]\n    for word in words_list: list_len.append(len(word))\n    return list_len\n\n\n\n\n# Define the app UI\napp_ui = ui.page_fluid(\n    ui.input_selectize(\"yellow_chr\", \"Choose one yellow letter\", \n                       [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"], \n                       multiple=False),\n    ui.output_text_verbatim(\"yellow_chr_output\"),\n    ui.input_selectize(\"grey_chr\", \"Choose six grey letters\", \n                       [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"], \n                       multiple=True),\n    ui.output_text_verbatim(\"grey_chr_output\"),\n    ui.output_text_verbatim(\"len_data_txt\"),\n    ui.output_plot(\"plot\"),\n    ui.output_ui(\"output_content\"),\n    ui.input_switch(\"switch\", \"Show word results\", False),  \n    ui.output_text_verbatim(\"word_list_output\")\n)\n\n# Define the app server logic\ndef server(input, output, session):\n    @reactive.Calc\n    def dataset():\n        # This function returns a dataset when 6 grey letters are selected\n        if len(input.grey_chr()) == 6:\n            return get_word_candidates(words_list, input.yellow_chr(),list(input.grey_chr()))\n        else:\n            return []\n    @reactive.Calc\n    def len_data():\n        len_data=get_len_word_list(dataset())\n        return len_data\n        \n    @output\n    @render.text\n    def yellow_chr_output():\n        return f\"The chosen yellow letter is {input.yellow_chr()}\"\n    @output\n    @render.text\n    def grey_chr_output():\n        if len(input.grey_chr()) != 6:\n            return f\"You need to choose 6 grey letters; currently, you have chosen {len(input.grey_chr())}.\"\n        else:\n            return \"You have chosen 6 grey letters.\"\n    @output\n    @render.text\n    def len_data_txt():\n        ret_ob=\"The data has not bin computed\"\n        if(len(input.grey_chr())==6):\n            ret_ob=f\"There are {len(dataset())} possible words with 4 or more letters\"\n        return ret_ob\n    @output\n    @render.plot(alt=\"A histogram of dataset values\")\n    def plot():    \n        fig, ax = plt.subplots()\n        if len(input.grey_chr()) == 6:\n            unique_values, counts = np.unique(len_data(), return_counts=True)\n            ax.bar(unique_values, counts, color='skyblue', edgecolor='black')\n            ax.set_title(\"Histogram of Word Candidates\")\n            ax.set_xlabel(\"Length of word\")\n            ax.set_ylabel(\"Count\")\n        else:\n            ax.text(0.5, 0.5, \"Please select 6 grey letters\", horizontalalignment='center', verticalalignment='center')\n        return fig\n    @output\n    @render.text\n    def word_list_output():\n        ret_ob=\"\"\n        if(input.switch()==True):\n            ret_ob=\"\\n\".join(dataset())\n        return ret_ob\n        \n    \n\napp = App(app_ui, server)"
  },
  {
    "objectID": "aplications.html#made-with-shiny-for-python-shiny-live",
    "href": "aplications.html#made-with-shiny-for-python-shiny-live",
    "title": "Applications",
    "section": "Made with Shiny for Python (Shiny Live)",
    "text": "Made with Shiny for Python (Shiny Live)\nThis application is created using Shiny for Python (Shiny Live). There is also an R version available.\nWhat makes the Shiny Live version so appealing is that it allows embedding Python code into WebAssembly. This application runs Python directly from your personal computer, with the dictionary downloaded from a GitHub link.\nThe library support is somewhat limited.\nFor small projects that should be accessible to the public, I like Shiny Live because it doesn’t require hosting a server for the backend.\nEstially the backend runs on you pc.\nTheir is som drawback, every aplication need the data to load in seperatly. Alle code and data is exeable for the user, documentasion is spars. It hard to debug, since it reaktiv programing."
  },
  {
    "objectID": "applications.html",
    "href": "applications.html",
    "title": "Applications",
    "section": "",
    "text": "For now, this site contains only one application built with Shiny. I may extend it in the future if I create more."
  },
  {
    "objectID": "applications.html#about",
    "href": "applications.html#about",
    "title": "Applications",
    "section": "About",
    "text": "About\nThis is a interactive app made to, help/improve player’s of New York times Spelling Bee.\nIn the spelling bee, the player is given seven letters. One letter is marked in yellow.\nThe goal is to make as many words as possible from the seven letters. Every word should contain the yellow letter.\nScoring\n\nA word of 4 letters earns one point.\nWords longer than 4 letters earn additional points (the exact amount is to be determined).\n\nThe dictionary used is provided Here under an open license.\nThe Spelling Bee uses a list of curated words. The dictionary is more like a reference for how words are spelled. For example, “AAAA” might appear in the dictionary but is unlikely to be included in the Spelling Bee.\nThere may also be words that are offensive or outdated. While they are correctly spelled, they probably won’t appear in the Spelling Bee.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n\n\nfrom shiny import *\n# For plot\nimport matplotlib.pyplot as plt\n# Get all words in the english diconary\nimport numpy as np\nimport requests\n# URL of the text file on GitHub\nurl = \"https://raw.githubusercontent.com/dwyl/english-words/refs/heads/master/words.txt\"\n\n# Fetch the file content\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    # Read the content of the file\n    file_content = response.text\n    # Split the content into a list of words\n    words_list = file_content.splitlines()\n    print(words_list)\nelse:\n    print(\"Failed to retrieve the file:\", response.status_code)\n\ndef check_word_canidat_early_stopping(\n        word:str,\n        yellow_chr:chr,\n        chr_list:list):\n    \"\"\"\n    This function returns True or False.\n    To be a valid candidate for a True return, the input word \n    must contain the yellow character and cannot contain any characters \n    other than the yellow character and characters from the provided character list.\n    Args:\n        word (str): The word to be checked.\n        yellow_chr (str): A character that all valid words must contain.\n        chr_list (list): A list of characters that the word is allowed to contain.\n    Returns:\n        bool: True or False depending on whether the word fulfills the conditions.\n\n    \"\"\"\n    lover_yellow =yellow_chr.lower()\n    return_obj=False\n    is_word_combination_of_chr_list=False\n    is_yellow_chr_contained= False\n    lower_chars = [char.lower() for char in chr_list]\n    \n    for letter in word:\n        is_letter_contained= letter.lower() in lower_chars\n        if (is_letter_contained==False):\n            is_word_combination_of_chr_list=False\n            break\n        else:\n            is_word_combination_of_chr_list=True\n        if (is_yellow_chr_contained==False):\n            is_yellow_chr_contained=(lover_yellow==letter.lower())\n    \n    if(is_word_combination_of_chr_list==True and is_yellow_chr_contained==True):\n        return_obj =True\n    return return_obj\n\n\n\ndef get_word_candidates(\n        word_list:list,\n        yellow_chr: chr,\n        chr_list:list,\n        min_word_length:int = 4):\n    \"\"\"\n    This returns list containg alle candidat words, for the spelling bee game.\n\n    All word contain the yellow letter, and consist of combination yello and letter from the letter list.\n    All words is longer ore equal to min_word_length\n    \n    Args:\n        word_list (list): a list with string containg words to serach though\n        yellow_chr (chr): all word in return should contain this letter\n        chr_list (list): a list of chr that can be conatin a word in return\n    Returns:\n        list: a list of strings. all word should be combination of yellow chr and letter from the list.\n        \n    \"\"\"\n    return_objet = []\n    index_list=[] #this list contais the index of wich words is potensiel words in the index list.\n    allowed_chartes= chr_list + [yellow_chr]\n    # len word is not calculated but store in the type string so it fast to check if len of the word is les than min first\n    for i in range(len(word_list)):\n        word_for_examination=word_list[i]\n        if(len(word_for_examination)&gt;=min_word_length):\n            if(check_word_canidat_early_stopping(word_for_examination,yellow_chr,allowed_chartes)==True):\n                return_objet.append(word_for_examination)\n                index_list.append(i)\n    return(return_objet)\n\n\n# Generat plots\n\ndef get_len_word_list(words_list:int):\n    list_len=[]\n    for word in words_list: list_len.append(len(word))\n    return list_len\n\n\n\n\n# Define the app UI\napp_ui = ui.page_fluid(\n    ui.input_selectize(\"yellow_chr\", \"Choose one yellow letter\", \n                       [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"], \n                       multiple=False),\n    ui.output_text_verbatim(\"yellow_chr_output\"),\n    ui.input_selectize(\"grey_chr\", \"Choose six grey letters\", \n                       [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"], \n                       multiple=True),\n    ui.output_text_verbatim(\"grey_chr_output\"),\n    ui.output_text_verbatim(\"len_data_txt\"),\n    ui.output_plot(\"plot\"),\n    ui.output_ui(\"output_content\"),\n    ui.input_switch(\"switch\", \"Show word results\", False),  \n    ui.output_text_verbatim(\"word_list_output\")\n)\n\n# Define the app server logic\ndef server(input, output, session):\n    @reactive.Calc\n    def dataset():\n        # This function returns a dataset when 6 grey letters are selected\n        if len(input.grey_chr()) == 6:\n            return get_word_candidates(words_list, input.yellow_chr(),list(input.grey_chr()))\n        else:\n            return []\n    @reactive.Calc\n    def len_data():\n        len_data=get_len_word_list(dataset())\n        return len_data\n        \n    @output\n    @render.text\n    def yellow_chr_output():\n        return f\"The chosen yellow letter is {input.yellow_chr()}\"\n    @output\n    @render.text\n    def grey_chr_output():\n        if len(input.grey_chr()) != 6:\n            return f\"You need to choose 6 grey letters; currently, you have chosen {len(input.grey_chr())}.\"\n        else:\n            return \"You have chosen 6 grey letters.\"\n    @output\n    @render.text\n    def len_data_txt():\n        ret_ob=\"The data has not bin computed\"\n        if(len(input.grey_chr())==6):\n            ret_ob=f\"There are {len(dataset())} possible words with 4 or more letters\"\n        return ret_ob\n    @output\n    @render.plot(alt=\"A histogram of dataset values\")\n    def plot():    \n        fig, ax = plt.subplots()\n        if len(input.grey_chr()) == 6:\n            unique_values, counts = np.unique(len_data(), return_counts=True)\n            ax.bar(unique_values, counts, color='skyblue', edgecolor='black')\n            ax.set_title(\"Histogram of Word Candidates\")\n            ax.set_xlabel(\"Length of word\")\n            ax.set_ylabel(\"Count\")\n        else:\n            ax.text(0.5, 0.5, \"Please select 6 grey letters\", horizontalalignment='center', verticalalignment='center')\n        return fig\n    @output\n    @render.text\n    def word_list_output():\n        ret_ob=\"\"\n        if(input.switch()==True):\n            ret_ob=\"\\n\".join(dataset())\n        return ret_ob\n        \n    \n\napp = App(app_ui, server)"
  },
  {
    "objectID": "applications.html#version-with-the-code",
    "href": "applications.html#version-with-the-code",
    "title": "Applications",
    "section": "Version with the code",
    "text": "Version with the code\nHere is a version where the code can be seen\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n#| components: [editor, viewer]\n\nfrom shiny import *\n# For plot\nimport matplotlib.pyplot as plt\n# Get all words in the english diconary\nimport numpy as np\nimport requests\n# URL of the text file on GitHub\nurl = \"https://raw.githubusercontent.com/dwyl/english-words/refs/heads/master/words.txt\"\n\n# Fetch the file content\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    # Read the content of the file\n    file_content = response.text\n    # Split the content into a list of words\n    words_list = file_content.splitlines()\n    print(words_list)\nelse:\n    print(\"Failed to retrieve the file:\", response.status_code)\n\ndef check_word_canidat_early_stopping(\n        word:str,\n        yellow_chr:chr,\n        chr_list:list):\n    \"\"\"\n    This function returns True or False.\n    To be a valid candidate for a True return, the input word \n    must contain the yellow character and cannot contain any characters \n    other than the yellow character and characters from the provided character list.\n    Args:\n        word (str): The word to be checked.\n        yellow_chr (str): A character that all valid words must contain.\n        chr_list (list): A list of characters that the word is allowed to contain.\n    Returns:\n        bool: True or False depending on whether the word fulfills the conditions.\n\n    \"\"\"\n    lover_yellow =yellow_chr.lower()\n    return_obj=False\n    is_word_combination_of_chr_list=False\n    is_yellow_chr_contained= False\n    lower_chars = [char.lower() for char in chr_list]\n    \n    for letter in word:\n        is_letter_contained= letter.lower() in lower_chars\n        if (is_letter_contained==False):\n            is_word_combination_of_chr_list=False\n            break\n        else:\n            is_word_combination_of_chr_list=True\n        if (is_yellow_chr_contained==False):\n            is_yellow_chr_contained=(lover_yellow==letter.lower())\n    \n    if(is_word_combination_of_chr_list==True and is_yellow_chr_contained==True):\n        return_obj =True\n    return return_obj\n\n\n\ndef get_word_candidates(\n        word_list:list,\n        yellow_chr: chr,\n        chr_list:list,\n        min_word_length:int = 4):\n    \"\"\"\n    This returns list containg alle candidat words, for the spelling bee game.\n\n    All word contain the yellow letter, and consist of combination yellow and letter from the letter list.\n    All words is longer ore equal to min_word_length\n    \n    Args:\n        word_list (list): a list with string containg words to serach though\n        yellow_chr (chr): all word in return should contain this letter\n        chr_list (list): a list of chr that can be conatin a word in return\n    Returns:\n        list: a list of strings. all word should be combination of yellow chr and letter from the list.\n        \n    \"\"\"\n    return_objet = []\n    index_list=[] #this list contais the index of wich words is potensiel words in the index list.\n    allowed_chartes= chr_list + [yellow_chr]\n    # len word is not calculated but store in the type string so it fast to check if len of the word is les than min first\n    for i in range(len(word_list)):\n        word_for_examination=word_list[i]\n        if(len(word_for_examination)&gt;=min_word_length):\n            if(check_word_canidat_early_stopping(word_for_examination,yellow_chr,allowed_chartes)==True):\n                return_objet.append(word_for_examination)\n                index_list.append(i)\n    return(return_objet)\n\n\n# Generat plots\n\ndef get_len_word_list(words_list:int):\n    list_len=[]\n    for word in words_list: list_len.append(len(word))\n    return list_len\n\n\n\n\n# Define the app UI\napp_ui = ui.page_fluid(\n    ui.input_selectize(\"yellow_chr\", \"Choose one yellow letter\", \n                       [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"], \n                       multiple=False),\n    ui.output_text_verbatim(\"yellow_chr_output\"),\n    ui.input_selectize(\"grey_chr\", \"Choose six grey letters\", \n                       [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"], \n                       multiple=True),\n    ui.output_text_verbatim(\"grey_chr_output\"),\n    ui.output_text_verbatim(\"len_data_txt\"),\n    ui.output_plot(\"plot\"),\n    ui.output_ui(\"output_content\"),\n    ui.input_switch(\"switch\", \"Show word results\", False),  \n    ui.output_text_verbatim(\"word_list_output\")\n)\n\n# Define the app server logic\ndef server(input, output, session):\n    @reactive.Calc\n    def dataset():\n        # This function returns a dataset when 6 grey letters are selected\n        if len(input.grey_chr()) == 6:\n            return get_word_candidates(words_list, input.yellow_chr(),list(input.grey_chr()))\n        else:\n            return []\n    @reactive.Calc\n    def len_data():\n        len_data=get_len_word_list(dataset())\n        return len_data\n        \n    @output\n    @render.text\n    def yellow_chr_output():\n        return f\"The chosen yellow letter is {input.yellow_chr()}\"\n    @output\n    @render.text\n    def grey_chr_output():\n        if len(input.grey_chr()) != 6:\n            return f\"You need to choose 6 grey letters; currently, you have chosen {len(input.grey_chr())}.\"\n        else:\n            return \"You have chosen 6 grey letters.\"\n    @output\n    @render.text\n    def len_data_txt():\n        ret_ob=\"The data has not bin computed\"\n        if(len(input.grey_chr())==6):\n            ret_ob=f\"There are {len(dataset())} possible words with 4 or more letters\"\n        return ret_ob\n    @output\n    @render.plot(alt=\"A histogram of dataset values\")\n    def plot():    \n        fig, ax = plt.subplots()\n        if len(input.grey_chr()) == 6:\n            unique_values, counts = np.unique(len_data(), return_counts=True)\n            ax.bar(unique_values, counts, color='skyblue', edgecolor='black')\n            ax.set_title(\"Histogram of Word Candidates\")\n            ax.set_xlabel(\"Length of word\")\n            ax.set_ylabel(\"Count\")\n        else:\n            ax.text(0.5, 0.5, \"Please select 6 grey letters\", horizontalalignment='center', verticalalignment='center')\n        return fig\n    @output\n    @render.text\n    def word_list_output():\n        ret_ob=\"\"\n        if(input.switch()==True):\n            ret_ob=\"\\n\".join(dataset())\n        return ret_ob\n        \n    \n\napp = App(app_ui, server)"
  },
  {
    "objectID": "applications.html#made-with-shiny-for-python-shiny-live",
    "href": "applications.html#made-with-shiny-for-python-shiny-live",
    "title": "Applications",
    "section": "Made with Shiny for Python (Shiny Live)",
    "text": "Made with Shiny for Python (Shiny Live)\nThis application is created using Shiny for Python (Shiny Live). There is also an R version available.\nWhat makes the Shiny Live version so appealing is that it allows embedding Python code into WebAssembly. This application runs Python directly from your personal computer, with the dictionary downloaded from a GitHub link.\nThe library support is somewhat limited.\nFor small projects that should be accessible to the public, I like Shiny Live because it doesn’t require hosting a server for the backend.\nEstially the backend runs on you pc.\nTheir is som drawback, every aplication need the data to load in seperatly. Alle code and data is exeable for the user, documentasion is spars. It hard to debug, since it reaktiv programing.\nBut overall, being able to create applications without needing to host a server is pretty cool."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "",
    "text": "I am applying for a job where the primary task involves working on and implementing agent-based simulations. It has been a while since I last worked with object-oriented coding, so I thought I would brush up on my basic skills by creating a small project based on a simple market simulation.\nMy background is primarily in mathematics, economics, and statistics. At the computer science department at KU, there don’t seem to be many courses that focus on agent-based simulation approaches. However, I recall attending a talk organized by the group “Kritiske Politter” a long time ago. The speaker was a professor from the physics department who, as a hobby, created simulations of markets. His main critique of traditional economics was the reliance on steady-state assumptions, which are quite common in the field. He argued that natural systems often exhibit fluctuations, even when they are in balance.\n\n\nTo give an example: If one simulates an island with rabbits and foxes, there will be fluctuations in the populations. When there are many rabbits, the fox population increases, which then leads to a decline in rabbits. This, in turn, causes a decline in foxes, allowing the rabbit population to grow again, and the cycle continues. In this case, a perfect equilibrium is unrealistic; instead, the equilibrium is a range in which the number of foxes and rabbits alternates over time.\nI find this field fascinating. One of my primary concerns is not just creating rational agents but making them realistic. In my economics courses, a lot of different techniques and models were presented to answer central questions. The argument often was that if these models capture the essence of some problem, they are useful. To some extent, I agree with this argument. However, I disagree with making assumptions solely for the purpose of solving the steady state of a system. This is why the agent-based approach is so interesting to me—it resembles the way physicists would approach a problem.\n\n\n\nIn this small project, I will attempt to create a basic framework for simulating agents. When I was first introduced to object-oriented programming, we worked on something similar to the rabbit-and-fox scenario.\nThe Code Will Be Written in Python Since the company appears to use Python, I will carry out the simulation using this language.\nIf I were building an extensive simulation framework for agents, I might consider using C++ due to its performance advantages. The key idea behind agent-based simulations is the interaction between agents, which makes it an iterative process. While Python may not be the fastest for such loops, parallelized programming could improve performance.\nAlthough I have experience with parallelization in R, I imagine Python offers more sophisticated methods. For instance, dividing agents into clusters that interact and processing these simultaneously could be an interesting approach. Learning and implementing this would be a valuable experience, although it exceeds my current skill level. In R, parallelization often involves copying the entire dataset to avoid writing conflicts; it would be exciting to explore more efficient solutions.\n\n\n\nIn summary, agent-based approaches are incredibly interesting and would be enjoyable to work on. I believe this problem provides a great opportunity to refresh my object-oriented programming skills. These simulations rely on Monte Carlo methods as their underlying mathematical approach, which makes them challenging to test."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#rabbits-and-foxes",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#rabbits-and-foxes",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "",
    "text": "To give an example: If one simulates an island with rabbits and foxes, there will be fluctuations in the populations. When there are many rabbits, the fox population increases, which then leads to a decline in rabbits. This, in turn, causes a decline in foxes, allowing the rabbit population to grow again, and the cycle continues. In this case, a perfect equilibrium is unrealistic; instead, the equilibrium is a range in which the number of foxes and rabbits alternates over time.\nI find this field fascinating. One of my primary concerns is not just creating rational agents but making them realistic. In my economics courses, a lot of different techniques and models were presented to answer central questions. The argument often was that if these models capture the essence of some problem, they are useful. To some extent, I agree with this argument. However, I disagree with making assumptions solely for the purpose of solving the steady state of a system. This is why the agent-based approach is so interesting to me—it resembles the way physicists would approach a problem."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#this-project",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#this-project",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "",
    "text": "In this small project, I will attempt to create a basic framework for simulating agents. When I was first introduced to object-oriented programming, we worked on something similar to the rabbit-and-fox scenario.\nThe Code Will Be Written in Python Since the company appears to use Python, I will carry out the simulation using this language.\nIf I were building an extensive simulation framework for agents, I might consider using C++ due to its performance advantages. The key idea behind agent-based simulations is the interaction between agents, which makes it an iterative process. While Python may not be the fastest for such loops, parallelized programming could improve performance.\nAlthough I have experience with parallelization in R, I imagine Python offers more sophisticated methods. For instance, dividing agents into clusters that interact and processing these simultaneously could be an interesting approach. Learning and implementing this would be a valuable experience, although it exceeds my current skill level. In R, parallelization often involves copying the entire dataset to avoid writing conflicts; it would be exciting to explore more efficient solutions."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#summary",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#summary",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "",
    "text": "In summary, agent-based approaches are incredibly interesting and would be enjoyable to work on. I believe this problem provides a great opportunity to refresh my object-oriented programming skills. These simulations rely on Monte Carlo methods as their underlying mathematical approach, which makes them challenging to test."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#the-environment-strøget",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#the-environment-strøget",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "The Environment (Strøget)",
    "text": "The Environment (Strøget)\nSince the company is located in Copenhagen, I thought it would be fun to use the city center as the topic of this exercise.\nIf we limit Strøget to the area between “Storkespringvandet” and the entrance near City Hall, we can imagine it as a long corridor with shops on both sides. To simplify, we’ll assume there’s one shop at each location along this corridor.\nThus, the environment can be represented as a long chain of shops. Agents can start from either end and pass through the corridor."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#the-shopper",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#the-shopper",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "The Shopper",
    "text": "The Shopper\nFrom personal experience, it seems that nobody goes to Strøget to buy necessities—it’s more about purchasing nice-to-have items and doing light shopping. I’ll assume people arrive at Strøget with a certain amount of money (cash) they want to spend.\nI created some preference classes, such as Food, Clothing, and Books. By making these preferences into classes, I can tailor purchase patterns for each category.\nFood: This is consumed only once, meaning if someone buys food, they won’t purchase it again. Cash: The shopper’s cash balance decreases with each purchase. Position: Tracks where the shopper is located in the chain of shops. Presence: Indicates whether the shopper is still on the street. When a shopper encounters a shop that matches their preferences, they will buy something with a given probability—but only if they can afford it."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#the-shop-agents",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#the-shop-agents",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "The Shop Agents",
    "text": "The Shop Agents\nShop agents will offer products at a given price in specific categories. To keep things simple, each shop offers one product in one category.\nSimulations The simulation is managed via the Street class.\nTo simplify, shops are ordered by their placement along the street. I haven’t implemented checks for errors in the ordering since this is just a toy example.\nThe simulation progresses with a tick function, allowing the system to update step by step.\nOverall Overall, I like this construction. Instead of storing positions in a variable, I could have moved the object itself or used pointers to save computational costs related to copying."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#classes",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#classes",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "Classes",
    "text": "Classes\nBelow, my classes and the central construction can be found.\n\n\nCode\nimport random\n\nclass Shop:\n    def __init__(self, placement: int, category: str, price: int):\n        \"\"\"\n        Represents a shop in the market.\n        :param placement: Position of the shop along the market.\n        :param category: The type of items sold by the shop (e.g., 'Food', 'Books').\n        :param price: The price of the items sold.\n        \"\"\"\n        self.placement = placement\n        self.category = category\n        self.price = price\n        self.units_sold=0\n    \n    def sold_unit(self):\n        self.units_sold +=1\n\n    def turnover(self):\n        return self.units_sold*self.price\n\n\"\"\"The belov class represent prefference a cosumer could have\"\"\"\nclass food:\n    def __init__(self,type,buy_prop,have_eaten=False,units_bought=0):\n        self.type=type\n        self.buy_prop=buy_prop\n        self.have_eaten=have_eaten\n        self.units_bought=units_bought\n    def will_buy(self):\n        return_obj=False\n        if(self.have_eaten==False):\n            return_obj= random.random()&lt; self.buy_prop\n            if(return_obj==True):\n                self.have_eaten=True\n                self.units_bought+=1\n        return (return_obj)\n\nclass cloth:\n    def __init__(self,type,buy_prop,units_bought=0):\n        self.type=type\n        self.buy_prop=buy_prop\n        self.units_bought=units_bought\n    def will_buy(self): \n        return_obj=random.random()&lt; self.buy_prop\n        if return_obj== True: self.units_bought+=1\n        return return_obj\n\nclass book:\n    def __init__(self,type,buy_prop,units_bought=0):\n        self.type=type\n        self.buy_prop=buy_prop\n        self.units_bought=units_bought\n    def will_buy(self):\n        return_obj=random.random()&lt; self.buy_prop\n        if return_obj== True: self.units_bought+=1\n        return return_obj\n\n\nclass Shopper:\n    def __init__(self,placement: int,direction,cash: float,preferences: list):\n        \"\"\"\n        Represents a shopper in the market.\n        :param placement: Current position of the shopper.\n        :param direction: Direction of movement (-1 for left, 1 for right).\n        :param cash: Amount of money the shopper has.\n        :param gender: Gender of the shopper (used for preferences).\n        :param preferences: Dictionary of categories and buy probabilities.\n        \"\"\"\n        self.placement = placement\n        self.direction = direction\n        self.cash = cash\n        self.preferences = preferences  \n        self.has_eaten = False\n        self.has_eaten_dessert = False\n        self.shoper_pressent_at_market=True\n        \n    def update_position(self):\n        \"\"\"This function update the postion of the shopper and updates if their are shoper pressent at market\"\"\"\n        self.placement=self.placement+self.direction\n        return\n    \n    def visit_shop(self, shop: Shop):\n        \"\"\"\n        Interact with a shop and decide whether to buy.\n        :param shop: The shop to interact with.\n        \"\"\"\n        # find if shope is in the prefrence\n        did_buy=False\n        for index in range(len(self.preferences)):\n            if(self.preferences[index].type==shop.category):\n                if(shop.price&lt;self.cash):\n                    did_buy=self.preferences[index].will_buy()\n            if (did_buy==True):\n                self.cash=self.cash-shop.price\n                shop.sold_unit()\n        self.update_position()\n        return did_buy\n        \n    \nclass shopping_street:\n    def __init__(self,list_of_shops,list_of_shopers):\n        # List of shoops should be ordere by placement\n        self.list_of_shops=list_of_shops\n        self.list_of_shopers=list_of_shopers\n        self.min=list_of_shops[0].placement\n        self.max=list_of_shops[(len(list_of_shops)-1)].placement\n\n    def run_tick_shopper(self,index):\n        \"\"\"This function will run one iteration of the simulation for a given agent\"\"\"\n        if (self.list_of_shopers[index].shoper_pressent_at_market)==True:\n            self.list_of_shopers[index].visit_shop(self.list_of_shops[index])\n        if(self.min&lt;(self.list_of_shopers[index].placement) or self.list_of_shopers[index].placement&lt;(self.max)):\n            self.list_of_shopers[index].shoper_pressent_at_market=False\n    def run_tick(self):\n        \"\"\"This function will run one iteration of the simulation for all agents\"\"\"\n        for ind in range(len(self.list_of_shopers)):\n            self.run_tick_shopper(ind)\n    def get_unit(self):\n        unit_sold=0\n        for ind in range(len(self.list_of_shops)):\n            unit_sold +=self.list_of_shops[ind].units_sold\n        return(unit_sold)\n    def get_total_turnover(self):\n        turnover=0\n        for ind in range(len(self.list_of_shops)):\n            turnover +=self.list_of_shops[ind].turnover()\n        return(turnover)"
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#unit-testing",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#unit-testing",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "Unit Testing",
    "text": "Unit Testing\nBelow, my Unit test, can be seen\n\n\nCode\n# unit test\nimport  unittest\n\ndef test_Shop_class():\n    test_Shop = Shop(2,\"men cloth\",100)\n    test_Shop.sold_unit()\n    assert(test_Shop.units_sold==1)\n    assert(test_Shop.units_sold*test_Shop.price==test_Shop.turnover())\n    \n    return\ntest_Shop_class()\n\ndef test_cloth_class():\n    test_cloth = cloth(type=\"men cloth\", buy_prop=1)\n    assert test_cloth.will_buy() == True\n    assert test_cloth.units_bought==1\n    return\ntest_cloth_class()\ndef test_book_class():\n    test_book = book(type=\"book store\", buy_prop=1)\n    assert test_book.will_buy() == True\n    assert test_book.units_bought==1\n    return\ntest_book_class()\ndef test_food_class():\n    test_food=food(type=\"Fastfood\",buy_prop=1)\n    assert(test_food.have_eaten)==False\n    # Should buy since buy prop = 1 \n    assert(test_food.will_buy()==True)\n    #\n    assert(test_food.units_bought==1)\n    # have eaten = True \n    assert(test_food.have_eaten)==True\n    #should not buy since have eaten\n    assert(test_food.will_buy()==False)\n    return \ntest_food_class()\n    \ndef test_Shopper_class():\n    test_Shop = Shop(2,\"men cloth\",100)\n    test_food=food(type=\"Fastfood\",buy_prop=1)\n    test_cloth = cloth(type=\"men cloth\", buy_prop=1)\n    test_Shopper=Shopper(placement=2,direction=-1,cash=1000,preferences=[test_food,test_cloth])\n    assert test_Shopper.placement==2\n    # The visit function should change the cash and update the units sold if the shop is in the preferences\n    assert(test_Shopper.visit_shop(test_Shop))\n    assert test_Shopper.placement==1\n    assert test_Shopper.preferences[1].units_bought==1\n    assert test_Shopper.cash==1000-test_Shop.price\n    assert test_Shopper.placement==1\n    # Test of shope not in preferences\n    test_Shop_2=Shop(1,\"book\",100)\n    assert test_Shopper.visit_shop(test_Shop_2) ==False\n    # since\n    assert test_Shopper.cash==1000-test_Shop.price\n    assert test_Shopper.placement==0\n    return \n    \ntest_Shopper_class()\n\ndef test_shopping_street_class():\n    test_Shop_1 = Shop(2,\"men cloth\",100)\n    test_shop_2=Shop(1,\"book\",50) \n    test_food=food(type=\"Fastfood\",buy_prop=1)\n    test_cloth = cloth(type=\"men cloth\", buy_prop=1)\n    test_Shopper_1=Shopper(placement=2,direction=-1,cash=1000,preferences=[test_food,test_cloth])\n    test_Shopper_2=Shopper(placement=1,direction=1,cash=1000,preferences=[test_food,test_cloth])\n    test_shopping_street=shopping_street(list_of_shops=[test_Shop_1,test_shop_2],list_of_shopers=[test_Shopper_1,test_Shopper_2])\n    \n    # No book should be hold sold \n    # The mens cloth should be sold\n    assert test_shopping_street.list_of_shopers[0].placement==2\n    assert test_shopping_street.list_of_shopers[1].placement==1\n    assert test_shopping_street.list_of_shops[0].units_sold==0\n    assert test_shopping_street.list_of_shops[1].units_sold==0\n    test_shopping_street.run_tick()\n    assert test_shopping_street.list_of_shopers[0].placement==1\n    assert test_shopping_street.list_of_shopers[1].placement==2\n    assert test_shopping_street.list_of_shops[0].units_sold==1\n    assert test_shopping_street.list_of_shops[1].units_sold==0\n    test_shopping_street.run_tick()\n    test_shopping_street.run_tick()\n    assert test_shopping_street.list_of_shopers[0].placement==0\n    assert test_shopping_street.list_of_shopers[1].placement==3\n    assert test_shopping_street.list_of_shops[0].units_sold==2\n    assert test_shopping_street.list_of_shops[1].units_sold==0\n    test_shopping_street.run_tick()\n    assert test_shopping_street.list_of_shopers[0].placement==0\n    assert test_shopping_street.list_of_shopers[1].placement==3\n    assert test_shopping_street.list_of_shops[0].units_sold==2\n    assert test_shopping_street.list_of_shops[1].units_sold==0\n    assert test_shopping_street.get_total_turnover()==2*100\n    assert test_shopping_street.get_unit()\n\n    return\ntest_shopping_street_class()"
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#simulation",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#simulation",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "Simulation",
    "text": "Simulation\nIn this code, I implemented a small simulation to validate the framework. I plotted the units sold and the turnover. Since this is a Monte Carlo simulation, there is no theoretical distribution for comparison\n\n\nCode\nimport matplotlib as plt\n\ndef simulate_street():\n    shops = [\n        Shop(placement=0, category=\"food\", price=50),\n        Shop(placement=1, category=\"cloth\", price=200),\n        Shop(placement=2, category=\"book\", price=150),\n        Shop(placement=3, category=\"food\", price=100),\n        Shop(placement=4, category=\"cloth\", price=300),\n        Shop(placement=5, category=\"book\", price=250)\n        ]\n    ticks=len(shops)\n    preferences_1 = [\n        food(type=\"food\", buy_prop=0.8),\n        cloth(type=\"cloth\", buy_prop=0.5),\n        book(type=\"book\", buy_prop=0.6)\n        ]\n    preferences_2 = [\n        food(type=\"food\", buy_prop=0.6),\n        cloth(type=\"cloth\", buy_prop=0.3),\n        book(type=\"book\", buy_prop=0.9)\n        ]\n    shoppers = [\n        Shopper(placement=0, direction=1, cash=500, preferences=random.choice([preferences_1, preferences_2])),\n        Shopper(placement=5, direction=-1, cash=800, preferences=random.choice([preferences_1, preferences_2])),\n        Shopper(placement=2, direction=1, cash=300, preferences=random.choice([preferences_1, preferences_2]))\n        ]\n    # Initialize the shopping street\n    street = shopping_street(list_of_shops=shops, list_of_shopers=shoppers)\n    for _ in range(ticks):\n        street.run_tick()\n\n    # Return a dictionary with the results\n    return {\n        \"Total_units\": street.get_unit(),\n        \"Total_turnover\": street.get_total_turnover()\n    }\n\n\n#plot distibution of sale and and units sold\nTotal_units=[]\nTotal_turnover=[]\nfor i in range(100):\n    sim_obj=simulate_street()\n    Total_units.append(sim_obj[\"Total_units\"])\n    Total_turnover.append(sim_obj[\"Total_turnover\"])\n\n\n\n\nimport matplotlib.pyplot as plt\n\n\n\n# Create two subplots\nfig, axes = plt.subplots(2, 1, figsize=(8, 10))  # 2 rows, 1 column\n\n# Histogram for List A\naxes[0].hist(Total_units, bins=5, color='blue', alpha=0.7)\naxes[0].set_title('Histogram of Total units')\naxes[0].set_xlabel('Value')\naxes[0].set_ylabel('Frequency')\n\n# Histogram for List B\naxes[1].hist(Total_turnover, bins=5, color='orange', alpha=0.7)\naxes[1].set_title('Histogram of Total turnover')\naxes[1].set_xlabel('Value')\naxes[1].set_ylabel('Frequency')\n\n# Adjust layout\nplt.tight_layout()\n\n# Show plots\nplt.show()"
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#downstream-effects",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#downstream-effects",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "Downstream Effects",
    "text": "Downstream Effects\nI believe downstream effects would be simpler to implement. To track these efficiently, I would use a directed graph or a reactive graph (as described in Mastering Shiny). The reactive graph approach could be more efficient for managing dependencies and dynamic changes in the system."
  },
  {
    "objectID": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#parallelization-challenges",
    "href": "Projects/A Toy Model for Market Dynamics Building a Agent-Based Frameworks.html#parallelization-challenges",
    "title": "A Toy Model for Market Dynamics: Building a Agent-Based Frameworks",
    "section": "Parallelization Challenges",
    "text": "Parallelization Challenges\nA minor technical detail: as the code is written now, parallelizing the simulation would be challenging. Although the shoppers do not interact directly, they interact with the same shops. This creates a potential issue when writing to shared variables, such as the units sold in a shop. If multiple shoppers update this variable simultaneously, errors could occur.\nI am particularly interested in learning how to address these kinds of problems effectively. For example, using locks or other concurrency control methods might help, but exploring more sophisticated solutions could be valuable."
  },
  {
    "objectID": "Projects/House-prices-project.html",
    "href": "Projects/House-prices-project.html",
    "title": "Housing pricing project",
    "section": "",
    "text": "#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(purrr)\n\n\ndata_before &lt;- read.csv(\"https://github.com/Missing-almost-everywhere/Missing-almost-everywhere.io/blob/main/Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/data_before_celan.csv\", stringsAsFactors = FALSE)\ndata_before&lt;-as.data.frame(data_before)\n# Define UI\nui &lt;- fluidPage(\n  titlePanel(\"Dynamic Plotting with Shiny\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"variable\", \"Select Variable:\", \n                  choices = names(data_before))\n    ),\n    mainPanel(\n      plotOutput(\"dynamicPlot\")\n    )\n  )\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n  \n  output$dynamicPlot &lt;- renderPlot({\n    col &lt;- input$variable\n    \n    if (is.numeric(df[[col]])) {\n      # For numeric variables, create a histogram\n      n_unique &lt;- length(unique(data_before[[col]]))\n      ggplot(data_before, aes_string(x = col)) + \n        geom_histogram(bins = min(n_unique, 30), fill = 'skyblue', color = 'black', alpha = 0.7) + \n        labs(title = paste('Histogram of', col), x = col, y = 'Frequency') + \n        theme_minimal()\n    } else {\n      # For non-numeric variables, create a bar plot\n      ggplot(data_before, aes_string(x = col)) + \n        geom_bar(fill = 'orange', color = 'black', alpha = 0.7) + \n        labs(title = paste('Bar Plot of', col), x = col, y = 'Count') + \n        theme_minimal()\n    }\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html",
    "title": "House Pricing Project",
    "section": "",
    "text": "This is practics case based on some data from kaggle\nI mostly made this to play around with some data and learn new methods. When working with data, like many others, I like to use either R Markdown or Jupyter Notebook. I also enjoy writing small notes and comments so that if I need to revisit the project later, I can easily see what I did. This approach is really helpful in terms of reproducibility. Unfortunately, people often share notebooks without adding any text or comments.\nWhile I’ve written out some comments here, I don’t go too deep. However, I do show parts of my workflow. The code is written in R, and I consider it what I call “one-time code.” It’s not meant to be read by others, so there aren’t a lot of comments. It’s not optimized either—I’ve simply chosen the fastest code I could think of or find to solve a problem. I ende op using Chat-gbt for some code aswell then correcting the mistakes. The methology and the models chosen is all me."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#comments-on-available-data",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#comments-on-available-data",
    "title": "House Pricing Project",
    "section": "Comments on Available Data",
    "text": "Comments on Available Data\nAll the houses are located in the Ames city area. There is information on the month and year sold. (In both the training and test sets, the years have the same values, meaning there’s no need to estimate values forward in time. This is a bit unusual—what is the purpose of the model in such a case?) Most of the data is categorical or discrete in nature. Some of the data has a natural order or rank. When examining the available data, it’s worth noting that there is no information on earlier sale prices—why is this missing? Information on “time on market” is also missing."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#data-cleaning-and-exploration",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#data-cleaning-and-exploration",
    "title": "House Pricing Project",
    "section": "Data cleaning and exploration",
    "text": "Data cleaning and exploration\n\n\nCode\nlibrary(readr)\ndf=as.data.frame(read.csv(\"train.csv\"))\n\n#split &lt;- sample(1:nrow(df), size = 0.8 * nrow(df))\n\n# Create training and testing sets\n#df &lt;- df[split, ]  # 80% of the data\n#df_test_for_choice &lt;- df[-split, ]  # other 20%\n\n#\ndf_test_preformence=as.data.frame(read.csv(\"test.csv\"))"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#recoding-na",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#recoding-na",
    "title": "House Pricing Project",
    "section": "Recoding NA",
    "text": "Recoding NA\nIn the description, I noticed that some of the variables use NA as a category.\nVaribels where NA is a catgori is listed below.\n\nAlley\nBsmtQual\nBsmtCond\nBsmtExposure\nBsmtFinType1\nBsmtFinType2\nFireplaceQu\nGarageType\nGarageFinish\nGarageQual\nGarageCond\nPoolQC\nFence\nMiscFeature\n\nThese variables need to be recoded, as the values are not missing—they are simply in the wrong format.\n\n\nCode\n#change \nchang_to_df&lt;-function(DF){\n  DF$Alley&lt;-DF$Alley[is.na(DF$Alley)] &lt;- \"No alley\"\n  DF$BsmtQual[is.na(DF$BsmtQual)]&lt;-\"No Basement\"\n  DF$BsmtCond[is.na(DF$BsmtCond)]&lt;-\"No Basement\"\n  DF$BsmtExposure[is.na(DF$BsmtExposure)]&lt;-\"No Basement\"\n  DF$BsmtFinType1[is.na(DF$BsmtFinType1)]&lt;- \"No Basement\"\n  DF$BsmtFinType2[is.na(DF$BsmtFinType2)]&lt;- \"No Basement\"\n  DF$FireplaceQu[is.na(DF$FireplaceQu)]&lt;-\"No Fireplace\"\n  DF$GarageType[is.na(DF$GarageType)]&lt;-\"No Garage\"\n  DF$GarageFinish[is.na(DF$GarageFinish)]&lt;-\"No Garage\"\n  DF$GarageQual[is.na(DF$GarageQual)]&lt;-\"No Garage\"\n  DF$GarageCond[is.na(DF$GarageCond)]&lt;-\"No Garage\"\n  DF$GarageYrBlt[is.na(DF$GarageYrBlt)]&lt;-0 # all values that is NA corespond wither other showing that their is no garage.\n  DF$PoolQC[is.na(DF$PoolQC)]&lt;-\"No Pool\"\n  DF$Fence[is.na(DF$Fence)]&lt;-\"No Fence\"\n  DF$MiscFeature[is.na(DF$MiscFeature)]&lt;-\"None\"\n  return(DF)\n}\n\ndf&lt;-chang_to_df(df)\n#df_test_for_choice&lt;-chang_to_df(df_test_for_choice)\ndf_test_preformence&lt;-chang_to_df(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#examining-missing-values",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#examining-missing-values",
    "title": "House Pricing Project",
    "section": "Examining Missing Values",
    "text": "Examining Missing Values\nAfter recoding cases where NA does not represent missing values, we will now analyze the actual missing values.\nBelow is a plot showing the missing values. I had to split the plot into two parts; otherwise, the variable names would not be readable.\n\n\nCode\nlibrary(naniar)\nvis_miss(df[,1:40])\n\n\n\n\n\n\n\n\n\nCode\nvis_miss(df[,40:81])\n\n\n\n\n\n\n\n\n\nLotFrontage missing values look weird.\nThe definition is Linear feet of street connected to property. Below i have printed uniqe entreances in LotFrontage.\n\n\nCode\ntable((df$LotFrontage))\n\n\n\n 21  24  30  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48 \n 23  19   6   5   1  10   9   6   5   1   1  12   6   4  12   9   3   1   5   6 \n 49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68 \n  4  57  15  14  10   6  17   5  12   7  13 143   8   9  17  19  44  15  12  19 \n 69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88 \n 11  70  12  17  18  15  53  11   9  25  17  69   6  12   5   9  40  10   5  10 \n 89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 \n  6  23   6  10   8   6   7   8   2   8   3  16   2   4   3   3   6   1   7   3 \n109 110 111 112 114 115 116 118 120 121 122 124 128 129 130 134 137 138 140 141 \n  2   6   1   1   2   2   2   2   7   2   2   2   1   2   3   2   1   1   1   1 \n144 149 150 152 153 160 168 174 182 313 \n  1   1   1   1   1   1   1   2   1   2 \n\n\nFrom the definition, it could refer to farms, but there are no farms in the dataset.\nIf we plot the house types against the number of missing values, we see that most of them come from single-family detached houses.\n\n\nCode\nlibrary(dplyr)\n# Test farm theory\ntest_LotFrontage &lt;- df$LotFrontage\ntest_LotFrontage[is.na(test_LotFrontage)] &lt;- 0\ntest_BldgType &lt;- factor(df$BldgType)\n\n\n\n# Create a data frame\ndata &lt;- data.frame(test_BldgType, test_LotFrontage)\n\nzero_counts &lt;- data %&gt;%\n  filter(test_LotFrontage == 0) %&gt;%\n  count(test_BldgType)\n\n# Create the bar plot with category labels at the bottom\nbarplot(zero_counts$n, \n        names.arg = zero_counts$test_BldgType,  # Use the correct column for labels\n        main = \"Number of Zeros by Category\",\n        xlab = \"Categories\",\n        ylab = \"Count of Zeros\",\n        col = \"lightblue\",\n        las = 2)  # Rotate labels to vertical\n\n\n\n\n\n\n\n\n\nIt seems like most of the missing values are from single-family detached houses, so it’s unrealistic to assume there is no street connected to the property. This means setting the values to zero is likely a bad option.\nOptions for imputation:\nLinear regression based on other variables. k-nearest neighbors. I could also disregard this variable since I doubt it has high predictive power.\nIt seems like an interactive linear model, incorporating the interaction between LotArea and LotShape, would provide good imputation (this also makes sense conceptually).\n\n\nCode\nImputasion_model &lt;- lm(LotFrontage ~ LotArea * LotShape, df)\n\n# Plot fitted values vs. residuals\nplot(Imputasion_model$fitted.values, Imputasion_model$residuals,\n     xlab = \"Fitted Values\",\n     ylab = \"Residuals\",\n     main = \"Residuals plot (LotFrontage ~ LotArea * LotShape)\")\n\n# Add a horizontal line at the mean of the residuals (which should be zero)\nabline(h = mean(Imputasion_model$residuals), col = \"red\", lwd = 2, lty = 2)\nmean_value &lt;- mean(Imputasion_model$residuals)\ntext(x = max(Imputasion_model$fitted.values), \n     y = mean_value, \n     labels = \"Mean\", \n     col = \"red\")\n\n\n\n\n\n\n\n\n\nMissing values for LotFrontage will be imputed using the linear model described above.\nThis also means that if LotFrontage is used in the model, the imputation will need to be performed on the test set as well. I have internally debated whether I should include the test set when building the imputation model—essentially, creating a model based on both the training and test sets for this purpose.\nSince I’m a little new to Kaggle, if the test set contains covariates and I only need to upload the fitted values, I would proceed with this approach. Otherwise, I would not. For now, I will just use the model based only on the training set.\n\n\nCode\n# imputatsion\nImputasion_model &lt;- lm(LotFrontage ~ LotArea * LotShape, df)\n\nInput_value&lt;-function(DF){\n  for (i in 1:length(DF$LotFrontage)){\n    if(is.na(DF$LotFrontage[i])){\n      DF$LotFrontage[i]&lt;-predicted_value &lt;- predict(Imputasion_model, newdata = list(\n        LotArea = DF$LotArea[i],\n        LotShape = DF$LotShape[i] ))\n    }\n  }\n  return(DF)\n  }\n\n#input df for all input values\ndf&lt;-Input_value(df)\n#df_test_for_choice&lt;-Input_value(df_test_for_choice)\ndf_test_preformence&lt;-Input_value(df_test_preformence)\n\n\nThere are still 1% of missing values in MasVnrArea and MasVnrType. I will discard the last row containing these missing values."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#recoding-of-variables",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#recoding-of-variables",
    "title": "House Pricing Project",
    "section": "Recoding of Variables",
    "text": "Recoding of Variables\nSome of these variables have a ranking, which makes them well-suited for recoding. By recoding them as numerical values, the ranking becomes more obvious.\nNote: This GPT is best used alongside the editGPT Browser extension.\n\n\nClick to expand/collapse varibel recoeding\n\nTo give a exampel of the recoding. PoolQC: Pool quality\n   Ex   Excellent         (4)\n   Gd   Good              (3)\n   TA   Average/Typical   (2)   (TA is never obsevered in pool varibel for traning set)\n   Fa   Fair              (1)\n   NA   No Pool           (0)\ncan be codes as 0-4\nBelowe here i have made list of how the varibels is code. I have made is so no pressent cagories NA is code to 0. Meaning 0 is only a value if the item is not pressent. and otherwise it starts from 1.\nLotShape: General shape of property Reg Regular (4) IR1 Slightly irregular (3) IR2 Moderately Irregular (2) IR3 Irregular (1)\nLandContour: Flatness of the property\n   Lvl  Near Flat/Level                                                     (4)\n   Bnk  Banked - Quick and significant rise from street grade to building   (3)\n   HLS  Hillside - Significant slope from side to side                      (2)\n   Low  Depression                                                          (1)\nLandSlope: Slope of property\n   Gtl  Gentle slope    (3)\n   Mod  Moderate Slope  (2)\n   Sev  Severe Slope    (1)\nExterQual: Evaluates the quality of the material on the exterior\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\n   \nExterCond: Evaluates the present condition of the material on the exterior\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\n   \nBsmtQual: Evaluates the height of the basement\n   Ex   Excellent (100+ inches)   (5)\n   Gd   Good (90-99 inches)       (4)\n   TA   Typical (80-89 inches)    (3)\n   Fa   Fair (70-79 inches)       (2)\n   Po   Poor (&lt;70 inches          (1)\n   NA   No Basement               (0)\nBsmtCond: Evaluates the general condition of the basement\n   Ex   Excellent                                     (5)\n   Gd   Good                                          (4)\n   TA   Typical - slight dampness allowed             (3)\n   Fa   Fair - dampness or some cracking or settling  (2)\n   Po   Poor - Severe cracking, settling, or wetness  (1)\n   NA   No Basement                                   (0)\nBsmtExposure: Refers to walkout or garden level walls\n   Gd   Good Exposure                                                               (4)\n   Av   Average Exposure (split levels or foyers typically score average or above)  (3) \n   Mn   Mimimum Exposure                                                            (2)\n   No   No Exposure                                                                 (1)\n   NA   No Basement                                                                 (0)\nBsmtFinType1: Rating of basement finished area\n   GLQ  Good Living Quarters            (6)\n   ALQ  Average Living Quarters         (5)\n   BLQ  Below Average Living Quarters     (4)\n   Rec  Average Rec Room                (3)\n   LwQ  Low Quality                     (2)\n   Unf  Unfinshed                       (1)\n   NA   No Basement                       (0)\nBsmtFinType2: Rating of basement finished area (if multiple types)\n   GLQ  Good Living Quarters            (6)\n   ALQ  Average Living Quarters         (5)\n   BLQ  Below Average Living Quarters     (4)\n   Rec  Average Rec Room                (3)\n   LwQ  Low Quality                     (2)\n   Unf  Unfinshed                       (1)\n   NA   No Basement                       (0)\nHeatingQC: Heating quality and condition\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\nKitchenQual: Kitchen quality\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\nFunctional: Home functionality (Assume typical unless deductions are warranted)\n   Typ  Typical Functionality   (7)\n   Min1 Minor Deductions 1      (6)\n   Min2 Minor Deductions 2      (5)\n   Mod  Moderate Deductions     (4)\n   Maj1 Major Deductions 1      (3)\n   Maj2 Major Deductions 2      (2)\n   Sev  Severely Damaged        (1)\n   Sal  Salvage only            (0)\n   \nFireplaceQu: Fireplace quality\n   Ex   Excellent - Exceptional Masonry Fireplace                                               (5)\n   Gd   Good - Masonry Fireplace in main level                                                  (4)\n   TA   Average - Prefabricated Fireplace in main living area or Masonry Fireplace in basement  (3)\n   Fa   Fair - Prefabricated Fireplace in basement                                              (2)\n   Po   Poor - Ben Franklin Stove                                                               (1)\n   NA   No Fireplace                                                                            (0)\nGarageFinish: Interior finish of the garage\n   Fin  Finished        (3)\n   RFn  Rough Finished  (2) \n   Unf  Unfinished      (1)\n   NA   No Garage         (0)\n   \nGarageQual: Garage quality\n   Ex   Excellent         (5)\n   Gd   Good              (4)\n   TA   Typical/Average   (3)\n   Fa   Fair              (2)\n   Po   Poor              (1)\n   NA   No Garage         (0)\nGarageCond: Garage condition\n   Ex   Excellent         (5)\n   Gd   Good              (4)\n   TA   Typical/Average   (3)\n   Fa   Fair              (2)\n   Po   Poor              (1)\n   NA   No Garage         (0)\nPoolQC: Pool quality\n   Ex   Excellent         (4)\n   Gd   Good              (3)\n   TA   Average/Typical   (2)   (TA is never obsevered in pool varibel for traning set)\n   Fa   Fair              (1)\n   NA   No Pool           (0)\n   \nFence: Fence quality\n   GdPrv    Good Privacy    (4)\n   MnPrv    Minimum Privacy (3)\n   GdWo Good Wood         (2)\n   MnWw Minimum Wood/Wire (1)\n   NA   No Fence            (0)\nTheir properly also some rank to other varibels, varibels like building matrials must have a ranking in terms of price. But i dont have any idear about whese.\n\n\n\nCode\n# Reencoding can be don via match.\n\n# recoding\nrecoding&lt;-function(DF){\n  DF$LotShape&lt;-match(DF$LotShape,c(\"IR3\",\"IR2\",\"IR1\",\"Reg\"))\n  #DF$LandContour&lt;-match(DF$LandContour,c(\"Low\",\"HLS\",\"Bnk\",\"Lvl\"))\n  #DF$LandSlope&lt;-match(DF$LandSlope,c(\"Sev\",\"Mod\",\"Gtl\"))\n  DF$ExterQual&lt;-match(DF$ExterQual,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$ExterCond&lt;-match(DF$ExterCond,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$BsmtQual&lt;-match(DF$BsmtQual,c(\"No Basement\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$BsmtCond&lt;-match(DF$BsmtCond,c(\"No Basement\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$BsmtExposure&lt;-match(DF$BsmtExposure,c(\"No Basement\",\"No\",\"Mn\",\"Av\",\"Gd\"))-1\n  DF$BsmtFinType1&lt;-match(DF$BsmtFinType1,c(\"No Basement\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"))-1\n  DF$BsmtFinType2&lt;-match(DF$BsmtFinType2,c(\"No Basement\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"))-1\n  DF$HeatingQC&lt;-match(DF$HeatingQ,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$KitchenQual&lt;-match(DF$KitchenQual,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$Functional&lt;-match(DF$Functional,c(\"Sal\",\"Sev\",\"Maj2\",\"Maj1\",\"Mod\",\"Min2\",\"Min1\",\"Typ\"))-1\n  DF$FireplaceQu&lt;-match(DF$FireplaceQu,c(\"No Fireplace\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$GarageFinish&lt;-match(DF$GarageFinish,c(\"No Garage\",\"Unf\",\"RFn\",\"Fin\"))-1\n  DF$GarageQual&lt;-match(DF$GarageQual,c(\"No Garage\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$GarageCond&lt;-match(DF$GarageCond,c(\"No Garage\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$PoolQC&lt;-match(DF$PoolQC, c(\"No Pool\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  #DF$Fence&lt;-match(DF$Fence,c(\"No Fence\",\"MnWw\",\"GdWo\",\"MnPrv\",\"GdPrv\"))-1\n  return(DF)\n}\n\ndf&lt;-recoding(df)\n#df_test_for_choice&lt;-recoding(df_test_for_choice)\ndf_test_preformence&lt;-recoding(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#feature-transformations",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#feature-transformations",
    "title": "House Pricing Project",
    "section": "Feature transformations",
    "text": "Feature transformations\nIn this section i will list the varibels i have change and what the changes was.\n\n\nClick to expand/collapse varibel Changes\n\nLooking at the description of the data the varibesl with lowe count, is\n\n40: 1-STORY W/FINISHED ATTIC ALL AGES\n45: 1-1/2 STORY - UNFINISHED ALL AGES\n75: 2-1/2 STORY ALL AGES\n85: SPLIT FOYER\n185: PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n\nHere is what i will combine them to.\n40 will be combined with 50 - 50: 1-1/2 STORY FINISHED ALL AGES Making 1-1/2 STORY FINISHED ALL AGES\n85 will be combined with 80 - 80: SPLIT OR MULTI-LEVEL Making Split\n185 will be combined with 160 - 160: 2-STORY PUD - 1946 & NEWER Making PUD - MULTILEVEL\nThis only leaves 45: UNFINISHED ALL AGES\nwith a vary small number of obsevations, in this section we looking at the data but this kind of asumption will affect models choice, what is the value of unfinished house, alle ages. A unfinish house can either be a cheap way to get new house ore a extra expens since it proberly should be removed. Both of these would proberly have lower value than the 1-1/2 STORY FINISHED ALL AGES. So if i inclued them i can drag down the estimat for this groups, and this can be unproporsional if the model is fitted with least sqaur. If i dont comined it with a varibel want to use a interaction effect in linear model I will get model wiht a lot NA, where this the combinations is not represented, i can get around this by tackling overide the model, so basically get to make geuss in whose case, that could be the average house price, something more cleaver, so for now i will leave it in. If i have to do the overird i will tell, this is better than a zero score. In the total linear model this is not a problem, so i may be the case this not problem.\n\nYearBuilt Som of this varibel is allready pressent in MSSubClass\nGarageCars I have think about, it would give nice distribution if it was seplified belove ore equalt 2 cars with true and false. but i think their is more infomation in o zeror so the the varibel vill be zero, one, two, above 2\n\nLooking at this it seam like it would be a god idear to cagaorise/colabse some of these varibel. This is to try to simplify the information. The problem that can arise is that with some of these varibels, they have such ueven distribution that, one can end up fitting a combination to uniqe house. this could be good fine but, in some case by colsaping one can better overall predictiv results. Later I will proberly reduce the dimension based on either spearman ore kendall tau corelation so one would want to ensure that if their is rank in the data is preserved. To give a exampel number of fireplace can be reduces to Yes or No. Yes would stille be higer than No.\n\nfireplace siplified to binary yes and NO\nPoolArea will be change to yes no their is not alot of information -pool yes No\n\nFor these i could it could potensiel be a good idear to eihter make fullbath total Halfbath total. ore colabs them in to yes no\n\nFullbath\nHalfbath\n\nbsmtFullBath\nBsmtHalfBath\nFenche will be change to True ore FAlSE\nSaleCondition will be normal True ore False\nLandContour will be change to level ore not. I will stil count this haveing a order, meaning level wich is equal to one is prefered\nLandSlope will be change to ground level ore not.\n\nFoundation will be combined by combin all other than “PConc” ore “CBlock” to “other”\nLotConfig all other than Corner Inside will be combined\nRoofStyle will be “Gable” or “other”\nThe one data point in MiscFeature that is tenis court will go under other\nIn Condition1 the following RRNe Within 200’ of East-West Railroad RRAe Adjacent to East-West Railroad RRNn Within 200’ of North-South Railroad RRAn Adjacent to North-South Railroad\nWill be combied to (Near Railroad)\nIn the Exterior1st and Exterior2nd. Their is some catagories with single obsevations in, I will put them in the closet cagori.\n\nI have rapt all the change in a function so it esay to aplly to the test dataset aswell\n\n\nCode\nmodify_df &lt;- function(DF){\n  DF$MSSubClass[DF$MSSubClass == 40] &lt;- 50\n  DF$MSSubClass[DF$MSSubClass == 85] &lt;- 80\n  DF$MSSubClass[DF$MSSubClass == 185] &lt;- 160\n  DF$have_Garage&lt;-DF$GarageCars&gt;0\n  DF$GarageCars&lt;-NULL\n  # is Fireplace pressent\n  DF$Fireplaces_present&lt;-DF$Fireplaces&gt;0\n  DF$Fireplaces&lt;-NULL\n  DF$FireplaceQu&lt;-NULL\n  # is pool pressent\n  DF$pool_present&lt;-DF$PoolArea&gt;0\n  DF$PoolArea&lt;-NULL\n  # Full bath pressent above ground\n  DF$FullBath_total&lt;-DF$BsmtFullBath+DF$FullBath\n  DF$BsmtFullBath&lt;-NULL\n  DF$FullBath&lt;-NULL\n  DF$HalfBath_total&lt;-DF$HalfBath+DF$BsmtHalfBath\n  DF$HalfBath&lt;-NULL\n  DF$BsmtHalfBath&lt;-NULL\n  DF$Alley&lt;-NULL\n  DF$Fence&lt;-DF$Fence!=\"No Fence\"\n  DF$SaleCondition&lt;-DF$SaleCondition==\"Normal\"\n  DF$LandContour&lt;-DF$LandContour==\"Lvl\"\n  DF$LandSlope&lt;-DF$LandSlope==\"Gtl\"\n  DF$Foundation[!(DF$Foundation %in% c(\"PConc\", \"CBlock\"))] &lt;- \"Other\"\n  DF$LotConfig[!(DF$LotConfig %in% c(\"Corner\",\"Inside\"))]&lt;-\"Other\"\n  DF$RoofStyle&lt;-DF$RoofStyle!=\"Gable\"\n  DF$MiscFeature[DF$MiscFeature==\"TenC\"]&lt;-\"Othr\"\n  DF$Condition1[DF$Condition1 %in% c(\"RRNe\",\"RRAe\",\"RRNn\",\"RRAn\")]&lt;-\"NR\"\n  DF$Exterior1st[DF$Exterior1st==\"AsphShn\"]&lt;-\"AsbShng\"\n  DF$Exterior1st[DF$Exterior1st==\"CBlock\"]&lt;-\"CemntBd\"\n  DF$Exterior1st[DF$Exterior1st==\"ImStucc\"]&lt;-\"Stucco\"\n  DF$Exterior2nd[DF$Exterior2nd==\"CBlock\"]&lt;-\"CemntBd\"\n  DF$Exterior2nd[DF$Exterior2nd==\"Other\"]&lt;-\"plywood\"\n  return(DF)\n}\n\ndf=modify_df(df)\n#df_test_for_choice&lt;-modify_df(df_test_for_choice)\ndf_test_preformence&lt;-modify_df(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#remove-varibels",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#remove-varibels",
    "title": "House Pricing Project",
    "section": "Remove varibels",
    "text": "Remove varibels\nIn this section, I will list the variables I plan to remove or disregard. The reason for their removal is that their distributions are very skewed compared to other variables, which I deem similar in the information they provide.\nThe main reason for disregarding these variables is that I do not believe they have high predictive strength, and I need to move forward with the data cleaning process. Also, I’m not getting paid for this.\n\n\nClick to expand/collapse varibel removed\n\n\nX3SsnPorch no real information will be removed (removed)\nScreenPorch no information aviable will be removed\nAlley contain no information and will be removed\nstreet contain not information and will be removed\nUtilities contain not information and will be removed\nCondition2 contain not information and will be removed\nRoofMatl contain not information and will be removed will be removed\nHeating contain not information and will be removed will be removed\nCentralAir contain not information and will be removed will be removed\nPoolQC contain not information and will be removed will be removed\nMiscFeature contain not information and will be removed will be removed\nGarageQual\nGarageCond\nGarageType\nElectrical\n\n\n\n\nCode\nremove_varibels&lt;-function(DF){\n  DF$X3SsnPorch&lt;-NULL\n  DF$ScreenPorch&lt;-NULL\n  DF$Alley&lt;-NULL\n  DF$Street&lt;-NULL\n  DF$Utilities&lt;-NULL\n  DF$Condition2&lt;-NULL\n  DF$RoofMatl&lt;-NULL\n  DF$Heating&lt;-NULL\n  DF$CentralAir&lt;-NULL\n  DF$PoolQC&lt;-NULL\n  DF$SaleType&lt;-NULL\n  DF$GarageQual&lt;-NULL\n  DF$GarageCond&lt;-NULL\n  DF$GarageType&lt;-NULL\n  DF$Electrical&lt;-NULL\n  return(DF)\n}\n\ndf=remove_varibels(df)\n#df_test_for_choice&lt;-remove_varibels(df_test_for_choice)\ndf_test_preformence&lt;-remove_varibels(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#variables-after-cleanup",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#variables-after-cleanup",
    "title": "House Pricing Project",
    "section": "Variables After Cleanup",
    "text": "Variables After Cleanup\nBelow, I have plotted the variables remaining after the cleanup.\n\n\nCode\nwrite.csv(df, \"data_after_clean.csv\", row.names = FALSE)\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(purrr)\n\nfor (col in names(df)) {\n  if (is.numeric(df[[col]])) {\n    # Numeriske variabler - Lav histogram\n    n_unique &lt;- length(unique(df[[col]]))\n    p &lt;- ggplot(df, aes_string(x = col)) + \n         geom_histogram(bins = min(n_unique, 30), fill = 'skyblue', color = 'black', alpha = 0.7) + \n         labs(title = paste('Histogram of', col), x = col, y = 'Frequency') + \n         theme_minimal()\n  } else {\n    # Ikke-numeriske variabler - Lav bar plot\n    p &lt;- ggplot(df, aes_string(x = col)) + \n         geom_bar(fill = 'orange', color = 'black', alpha = 0.7) + \n         labs(title = paste('Bar Plot of', col), x = col, y = 'Count') + \n         theme_minimal()\n  }\n  print(p)  # Vis plottet\n}\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(ggplot2)\n\ndata_before &lt;- read.csv(\"https://raw.githubusercontent.com/Missing-almost-everywhere/Missing-almost-everywhere.io/main/Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/data_after_clean.csv\")\n\nui &lt;- fluidPage(\n  titlePanel(\"House Prices Data Visualization\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"variable\", \"Select Variable:\", choices = names(data_before))\n    ),\n    mainPanel(\n      plotOutput(\"dynamicPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$dynamicPlot &lt;- renderPlot({\n    col &lt;- input$variable\n    \n    if (is.numeric(data_before[[col]])) {\n      ggplot(data_before, aes(x = .data[[col]])) + \n        geom_histogram(bins = 30, fill = 'skyblue', color = 'black', alpha = 0.7) +\n        labs(title = paste('Histogram of', col), x = col, y = 'Frequency') +\n        theme_minimal()\n    } else {\n      ggplot(data_before, aes(x = .data[[col]])) + \n        geom_bar(fill = 'orange', color = 'black', alpha = 0.7) +\n        labs(title = paste('Bar Plot of', col), x = col, y = 'Count') +\n        theme_minimal()\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#looking-into-cross-correlation",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#looking-into-cross-correlation",
    "title": "House Pricing Project",
    "section": "Looking Into Cross-Correlation",
    "text": "Looking Into Cross-Correlation\nBelow, I have plotted the correlation matrix for the variables. Since many of these variables are categorical but have a ranking, I used Spearman correlation.\n\n\nCode\ndf=na.omit(df)\nlibrary(corrplot)\n\n\ncorrplot 0.95 loaded\n\n\nCode\nlibrary(Hmisc)\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nCode\nnumeric_df &lt;- df[sapply(na.omit(df), is.numeric)]\n\n# Compute Spearman correlation matrix\nspearman_corr &lt;- cor(numeric_df, method = \"spearman\")\n\n# Compute p-values (optional)\nlibrary(Hmisc)\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n# Plot the correlation matrix\ncorrplot(spearman_corr, method = \"color\",tl.col = \"black\", tl.srt = 60, insig = \"blank\",tl.cex = 0.5,title = \"Spearman Correlation Plot\")\n\n\n\n\n\n\n\n\n\nNotes on the Plot Overall, the internal correlation is a lot smaller than I thought, which is good. This should make it easier to find good predictors.\nThe third variable from the bottom is the sales price. From this, one can see which variables could be potential predictors. It seems like OverallQual could be a really good predictor.\nHowever, correlation only captures linear effects, so one should be cautious about relying solely on correlation for feature selection. Imagine there was a feature with a sinusoidal curve that perfectly described the data—it would have a correlation of zero."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#reducing-variables-based-on-multicollinearity",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#reducing-variables-based-on-multicollinearity",
    "title": "House Pricing Project",
    "section": "Reducing Variables Based on Multicollinearity",
    "text": "Reducing Variables Based on Multicollinearity\nIn this section, I will reduce some variables, as multicollinearity can pose challenges for certain models. While some partitioning-based models are less sensitive to multicollinearity, linear models can be significantly impacted. In a simple linear model with two inputs, the variance of a parameter can be expressed as:\n\\[\\text{Var}(\\beta_1) = \\frac{\\sigma^2}{1 - \\text{cor}(x_1, x_2)}\\]\nThis means that as the correlation between two variables increases, the variance of the estimate explodes. Consequently, certain models are sensitive to multicollinearity.\nOne approach to addressing this is to remove variables with a correlation above a certain threshold. However, determining the optimal threshold is not straightforward. For a specific model, simulations can be conducted to estimate a suitable value. In this case, I want to perform some reduction before applying any models. I have chosen a cutoff point of (|0.7|). If there is a high correlation between two variables, I will remove the one with the weaker correlation to the log of the sale price.\nSince the variables are not numeric but do have a ranking, PCA is not an option. However, Spearman correlation can still be used.\nFor variable reduction, hierarchical clustering can be applied using the absolute value of the Spearman correlations as a distance measure. Single-linkage clustering can then be employed, with a slight reformulation of the problem. Hierarchical clustering requires a distance metric, and for this purpose, the distance between two variables based on correlation can be defined as:\n\\(d(x_1, x_2) = 1 - |\\text{cor}(x_1, x_2)|\\)\nGiven the cutoff of (|0.7|) for correlation, the corresponding distance after reformulation would be:\n\\(1 - 0.7 = 0.3\\)\n\n\nCode\n# The code here is not optimized but it was fast to write\nnumeric_df &lt;- df[sapply(na.omit(df), is.numeric)]\n\nnumeric_saleprice&lt;-numeric_df$SalePrice\nnumeric_df$SalePrice&lt;-NULL # remove from dataframe\n\nlibrary(Hmisc)\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n\ndist_matrix &lt;- as.dist(1-abs(spearman_corr))\n\nhc &lt;- hclust(dist_matrix, method = \"single\")\n\n\nclusters &lt;- cutree(hc, h = 0.3) #\n\n# Visualize dendrogram\nplot(hc, main = \"Hierarchical Clustering Dendrogram\")\nabline(h = 0.3, col = \"red\") # Add cutoff line\n\n\n\n\n\n\n\n\n\nCode\n# reduce varibels\nnumeric_saleprice_corr &lt;- rcorr(as.matrix(numeric_df), numeric_saleprice, type = \"spearman\")#$r[, \"numeric_saleprice\"]\n\n\n# Step 2: Identify clusters with multiple variables\nduplicates &lt;- table(clusters)[table(clusters) &gt; 1]\nclusters_with_duplicates &lt;- names(duplicates)\n\n\nfor (cluster in clusters_with_duplicates) {\n  clusters_with_duplicates\n  n_cluster=length(names(clusters[clusters==cluster]))\n  spear_var=rep(NA,n_cluster)\n  names_cluster=names(clusters[clusters==cluster])\n  for (i in 1:n_cluster){\n    names(clusters[clusters==cluster])[i]\n    spear_var[i]=rcorr(as.matrix(numeric_df[names(clusters[clusters==cluster])[i]]),log(numeric_saleprice),type=\"spearman\")$r[1,2]\n  }\n  index_of_max &lt;- which.max(spear_var)\n  names_cluster[-index_of_max]\n  #numeric_df[names_cluster[-index_of_max]]&lt;-NULL\n  df[names_cluster[-index_of_max]]&lt;-NULL\n  #df_test_for_choice[names_cluster[-index_of_max]]&lt;-NULL\n  df_test_preformence[names_cluster[-index_of_max]]&lt;-NULL\n}\n\n\nWith the clusters found, for each cluster, the feature with the highest Spearman correlation to the log of the sale price will be chosen.\nAs a result, there was a reduction from 61 to 41 features.\n\n\nCode\nnumeric_df &lt;- df[sapply(na.omit(df), is.numeric)]\n\nnumeric_saleprice&lt;-numeric_df$SalePrice\nnumeric_df$SalePrice&lt;-NULL # remove from dataframe\n\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n\ndist_matrix &lt;- as.dist(1-abs(spearman_corr))\n\nhc &lt;- hclust(dist_matrix, method = \"single\")\n\n\nclusters &lt;- cutree(hc, h = 0.3) #\n\n# Visualize dendrogram\nplot(hc, main = \"Hierarchical Clustering Dendrogram\")\nabline(h = 0.3, col = \"red\") # Add cutoff line\n\n\n\n\n\n\n\n\n\nAs can be seen in the new dendrogram, the dataframe has been reduced to meet the requirements."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#overview-of-variables-with-the-highest-correlation-to-log-of-sale-price",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#overview-of-variables-with-the-highest-correlation-to-log-of-sale-price",
    "title": "House Pricing Project",
    "section": "Overview of Variables with the Highest Correlation to Log of Sale Price",
    "text": "Overview of Variables with the Highest Correlation to Log of Sale Price\nBelow, I have ordered the variables based on the absolute value of their Spearman correlation.\n\n\nCode\n# Required libraries\nlibrary(corrplot)\nlibrary(Hmisc)\n\n# Subset the dataframe to numeric columns\nnumeric_df$log_saleprice&lt;-log(df$SalePrice)\nnumeric_df$SalePrice&lt;-NULL\n# Ensure the target variable is present\ntarget_variable &lt;- \"log_saleprice\"\nif (!target_variable %in% colnames(numeric_df)) {\n  stop(\"Target variable not found in numeric dataframe.\")\n}\n\n# Compute Spearman correlation matrix and p-values\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n# Extract correlation with the target variable\ntarget_corr &lt;- spearman_corr[, target_variable, drop = FALSE]\ntarget_corr &lt;- as.data.frame(target_corr)\ncolnames(target_corr) &lt;- \"Spearman_Correlation\"\n\n# Add a column indicating if the correlation could be computed\ntarget_corr$Computed &lt;- !is.na(target_corr$Spearman_Correlation)\n\n# Sort variables by Spearman correlation in descending order\nsorted_corr &lt;- target_corr[order(abs(target_corr$Spearman_Correlation), decreasing = TRUE), ]\n\n# Display sorted variables with their correlation and computed status\nprint(sorted_corr[1:10,])\n\n\n               Spearman_Correlation Computed\nlog_saleprice             1.0000000     TRUE\nOverallQual               0.8087095     TRUE\nGrLivArea                 0.7304531     TRUE\nBsmtQual                  0.6760372     TRUE\nGarageArea                0.6477811     TRUE\nFullBath_total            0.6371933     TRUE\nGarageFinish              0.6323531     TRUE\nTotalBsmtSF               0.6023901     TRUE\nYearRemodAdd              0.5688537     TRUE\nHeatingQC                 0.4906598     TRUE\n\n\nLook at the Year sold the collation is really low. the spand of the years it relly lowe, this would idicate that the price do not raise a lot over time in this area. I would have expted some kind of increase to compensate for inflation.\nBelow i have recoded the Time as months since 2006, 1 being january. I also plottet logsale price as agianst the month\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nnumeric_df$time&lt;-NA\nfor (i in 1:length(df$MoSold)){\n  numeric_df$time[i]&lt;-(numeric_df$YrSold[i]-2006)*12+numeric_df$MoSold[i]\n}\n\nyearly_means &lt;- numeric_df %&gt;%\n  group_by(time) %&gt;%\n  summarise(mean_value = mean(log_saleprice))\n\n# Plot residuals against time\nggplot(data=numeric_df[!is.na(numeric_df$log_saleprice),], aes(x=numeric_df$time, y=log_saleprice)) +\n  geom_point(alpha = 0.6, color = \"black\") +  # Individual data points\n  geom_line(data = yearly_means, aes(x = time, y = mean_value, color = \"Mean Sold Price\"), \n            size = 1, show.legend = TRUE) +\n  geom_point(data = yearly_means, aes(x = (time), y = mean_value),\n             color = \"red\", size = 2) +\n  labs(\n    x = \"Month\",\n    y = \"Value\",\n    title = \"Monthly Data with Mean Overlay\"\n  ) +\n  theme_minimal()# caluculate mean \n\n\n\n\n\n\n\n\n\nCode\nmonthly_means &lt;- numeric_df %&gt;%\n  group_by(MoSold) %&gt;%\n  summarise(mean_value = mean(log_saleprice))\n\n\n\nggplot(data=numeric_df[!is.na(numeric_df$log_saleprice),], aes(x=numeric_df$MoSold, y=log_saleprice)) +\n  geom_point(alpha = 0.6, color = \"black\") +  # Individual data points\n  geom_line(data = monthly_means, aes(x = MoSold, y = mean_value, color = \"Mean Sold Price\"), \n            size = 1, show.legend = TRUE) +\n  geom_point(data = monthly_means, aes(x = (MoSold), y = mean_value),\n             color = \"red\", size = 2) +\n  labs(\n    x = \"Month\",\n    y = \"Value\",\n    title = \"Monthly Data with Mean Overlay\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOverall their dont seam to be any big conettiction between pice and time. From the moth plot it clear if ther varinace of the price is dependen on the time ore if the salevolum changes letting to less spread."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#overallqual",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#overallqual",
    "title": "House Pricing Project",
    "section": "OverallQual",
    "text": "OverallQual\nOverallQual has high corelation with saleprice.\n\n\nCode\nggplot(data=df[!is.na(df$SalePrice),], aes(x=factor(OverallQual), y=log(SalePrice)))+\n        geom_point()+ labs(title=\"OverallQual\")\n\n\n\n\n\n\n\n\n\nlog price seam linear related to overall quality."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#ground-lvving-area",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#ground-lvving-area",
    "title": "House Pricing Project",
    "section": "Ground lvving area",
    "text": "Ground lvving area\nGround living area.\n\n\nCode\nggplot(data=df[!is.na(df$SalePrice),], aes(x=factor(GrLivArea), y=log(SalePrice)))+\n        geom_point()+ labs(title=\"GrLivArea\")"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#looking-at-some-no-numeric-varibels",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#looking-at-some-no-numeric-varibels",
    "title": "House Pricing Project",
    "section": "Looking at some no numeric varibels",
    "text": "Looking at some no numeric varibels\nSome of these varibels do not have natrual ordering, wich mean computing spearman corelation is not posibel. In that case the best option (I know) is to plot the target varibels against the cagories in the varibels, theirs differenct forms, such boxbolot ore violin plot. I have chosen violin plot since it hase more resempelens with a historgram, wich i prefere over boxplot.\n\n\nCode\nwrite.csv(df, \"data_for_final_model.csv\", row.names = FALSE)\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nnon_numeric_cols &lt;- df %&gt;%\n  select_if(~!is.numeric(.)) %&gt;%\n  colnames()\n\nfor (col in non_numeric_cols) {\n  p &lt;- ggplot(df, aes_string(x = col, y = \"log(SalePrice)\")) +  \n    geom_violin() +\n    stat_summary(fun = \"mean\", geom = \"point\", color = \"red\", size = 3) +  # Tilføj mean som rødt punkt\n    labs(title = paste(\"Violin plot for\", col, \"vs log(SalePrice)\")) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  \n  print(p)  # Udskriv violin plot for hver iteration\n}\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata_final &lt;- read.csv(\"https://raw.githubusercontent.com/Missing-almost-everywhere/Missing-almost-everywhere.io/main/Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/data_for_final_model.csv\")\n\nui &lt;- fluidPage(\n  titlePanel(\"None Numerical varibels\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"variable\", \"Select Variable:\", \n                  choices = names(data_final))\n    ),\n    mainPanel(\n      plotOutput(\"dynamicPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$dynamicPlot &lt;- renderPlot({\n    col &lt;- input$variable\n    \n    if (is.numeric(data_final[[col]])) {\n      if(col != \"SalePrice\") {\n        ggplot(data_final, aes(x = .data[[col]], y = log(SalePrice))) +\n          geom_point(alpha = 0.5) +\n          labs(title = paste(col, \"vs log(SalePrice)\")) +\n          theme_minimal()\n      } else {\n        ggplot(data_final, aes(x = .data[[col]])) +\n          geom_histogram(bins = 30, fill = 'skyblue', color = 'black', alpha = 0.7) +\n          labs(title = paste('Histogram of', col)) +\n          theme_minimal()\n      }\n    } else {\n      ggplot(data_final, aes(x = .data[[col]], y = log(SalePrice))) +\n        geom_violin(fill = 'skyblue', alpha = 0.7) +\n        stat_summary(fun = mean, geom = \"point\", color = \"red\", size = 3) +\n        labs(title = paste(col, \"vs log(SalePrice)\")) +\n        theme_minimal() +\n        theme(axis.text.x = element_text(angle = 45, hjust = 1))\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\nSo what I am looking for is something their it seam like the means i sale price is different between the factors. Neighborhood, seam like a good varibel to include, in many intances on can see that the their is not even a overlab in the observed price.\nThe roofing seam like it could have potesial"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#base-line-mocel",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#base-line-mocel",
    "title": "House Pricing Project",
    "section": "Base line mocel",
    "text": "Base line mocel\nAfter Plotting different models and looking the residual i found the below model. Where the saleprice pr sqft of living area is combination the Neighborhood the type of dwelling (MSSubClass)\nwich could be good candidate if one look the residuals being drawn from af t distribusion. So more heavy tale distribution,that a normal distribution. In many ways this model seam resanbull from a intuition point of view.\n\n\nCode\n# Plot original data\n\nmodel_baseline &lt;-lm((SalePrice/GrLivArea) ~ TotalBsmtSF*MSSubClass:Neighborhood:GrLivArea , data = df) # lm((SalePrice / GrLivArea) ~ TotalBsmtSF+MSSubClass+Neighborhood,data=df)\n\n\nplot(df$SalePrice,log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea))\n\n\n\n\n\n\n\n\n\nCode\nhist(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea),breaks=50)\n\n\n\n\n\n\n\n\n\nCode\n# The histogram almost look like a t distribution\nlibrary(MASS)\nfitdistr(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea), \"t\",start = list(m=mean(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea)),s=sd(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea)), df=2),lower=c(-1, 0.001,1))\n\n\n        m              s              df     \n  -0.009049068    0.156394619    4.526172860 \n ( 0.004804025) ( 0.005221797) ( 0.565497406)\n\n\nCode\nlibrary(Dowd)\nTQQPlot(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea),df=4.5 )"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#a-linear-interaction-model.",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#a-linear-interaction-model.",
    "title": "House Pricing Project",
    "section": "A linear interaction model.",
    "text": "A linear interaction model.\nBelove I have fitted a interaction model where log saleprice is a interactiv funtion between The type of house, the neighborhood and the size of the dewelling+ sum effect from the size of the basement. It seam reasonbell and is somewat how i would estimate a type of house for the overall price.\n\n\nCode\nModel_second_baseline&lt;- lm(log(SalePrice)~TotalBsmtSF+MSSubClass:GrLivArea:Neighborhood,data=df)\n\n\nlibrary(Dowd)\nTQQPlot(Model_second_baseline$residuals,df=4.5 )\n\n\n\n\n\n\n\n\n\nCode\n# Define the custom metric for CV evaluation\nlog_diff_metric_model_baseline &lt;- function(data, label) {\n  groups=unique(label)\n  groups_score=rep(NA,length(groups))\n  \n  for (i in 1:length(groups)){\n    groupe_for_test=groups[i]\n    partion_vector_train &lt;- which(label!= groupe_for_test)\n    partion_vector_test &lt;- which(label== groupe_for_test)\n    train_data &lt;- data[partion_vector_train, ] \n    test_data &lt;- data[partion_vector_test, ] \n    # fit model \n    model_baseline &lt;-lm(log(SalePrice)~TotalBsmtSF+MSSubClass:GrLivArea:Neighborhood,data=train_data)\n    # get predition \n    predicted_SalePrice &lt;- predict(model_baseline, newdata = test_data) \n    # get log diff\n    log_diff &lt;- log(test_data$SalePrice) - (predicted_SalePrice)\n    # save results\n    groups_score[i]&lt;-sum(log_diff**2)\n  }\n  \n  return(sum(groups_score)/length(data$SalePrice))\n}\n\n# make labels\n\nn_cv=50\n\nlabels=sample(rep(seq(1,n_cv),floor(length(df$Id)/n_cv)))\nif(length(df$Id)-floor(length(df$Id)/n_cv)*n_cv!=0){\n  labels&lt;-c(labels,seq(1:(length(df$Id)-floor(length(df$Id)/n_cv)*n_cv)))\n}\nlabels=sample(labels)\n\n\nprint(\"RMSE of log prices\")\n\n\n[1] \"RMSE of log prices\"\n\n\nCode\nprint(log_diff_metric_model_baseline(df,labels))\n\n\n[1] 0.06399003\n\n\nOver all the predicative stregth look good the reisudall of the model looks more simullaro to t distribuin than a normal."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#lasso",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#lasso",
    "title": "House Pricing Project",
    "section": "Lasso",
    "text": "Lasso\nIn this chunck of code i fit lasso model for the dataframe, by asuming a linear model over all the vearibels in the datafram penlized by the size of beta. The Lasso solves the following\n\\[\\underset{\\beta}{argmin} ||log(Y)-\\beta X||_{2}^2+\\lambda||\\beta||_1\\].\n“It penalizes the size of the coefficients (beta). Since the penalty is based on the \\(L_1\\) -norm, it tends to set some coefficients to zero. This encourages sparse solutions, where only the most relevant features are retained.\n\n\nCode\nlibrary(glmnet)\n\n# Prepare the data\ndf_lasso &lt;- df\nlog_saleprice &lt;- log(df_lasso$SalePrice)\ndf_lasso$SalePrice &lt;- NULL\ndf_lasso$log_saleprice &lt;- log_saleprice  # Add log_saleprice to df_lasso\ndf_lasso_id&lt;-df_lasso$Id\ndf_lasso$Id&lt;-NULL\n# Remove rows with NA values\ndf_lasso &lt;- na.omit(df_lasso)\n\n# Split data into folds\nk &lt;- 10\nfolds &lt;- sample(1:k, size = nrow(df_lasso), replace = TRUE)\n\n# Initialize a vector to store MSEs for each fold\ncv_mse &lt;- numeric(k)\nbest_lambda_vector &lt;- rep(NA, k)\n\nfor (i in 1:k) {\n  # Split data into training and test sets\n  train_indices &lt;- which(folds != i)\n  test_indices &lt;- which(folds == i)\n  \n  train_data &lt;- df_lasso[train_indices, ]\n  test_data &lt;- df_lasso[test_indices, ]\n  \n  # Create design matrices\n  X_train &lt;- model.matrix(log_saleprice ~ . , data = train_data)\n  X_test &lt;- model.matrix(log_saleprice ~ . , data = test_data)\n  \n  # Fix column mismatch between X_train and X_test\n  common_columns &lt;- intersect(colnames(X_train), colnames(X_test))\n  X_test &lt;- X_test[, common_columns, drop = FALSE]\n  X_train &lt;- X_train[, common_columns, drop = FALSE]\n  \n  # Fit LASSO model with cross-validation to choose lambda\n  lasso_model &lt;- cv.glmnet(X_train, train_data$log_saleprice, alpha = 1, family = \"gaussian\")\n  best_lambda &lt;- lasso_model$lambda.min\n  best_lambda_vector[i] &lt;- best_lambda\n  \n  # Predict on test data\n  predictions &lt;- predict(lasso_model, s = best_lambda, newx = X_test)\n  \n  # Compute MSE for this fold\n  cv_mse[i] &lt;- mean((test_data$log_saleprice - predictions)^2)\n}\n\n# Average MSE across all folds\nmean_cv_mse &lt;- mean(cv_mse)\ncat(\"Mean Cross-Validated MSE:\", mean_cv_mse, \"\\n\")\n\n\nMean Cross-Validated MSE: 0.02131863 \n\n\nCode\n# Fit final LASSO model with the average best lambda\nmean_best_lambda &lt;- mean(best_lambda_vector, na.rm = TRUE)  # Calculate mean lambda\nX_full &lt;- model.matrix(log_saleprice ~ . , data = df_lasso)\nfinal_lasso_model &lt;- glmnet(X_full, log_saleprice, alpha = 1, lambda = mean_best_lambda, family = \"gaussian\")\n\n# Print final model coefficients\nprint(mean_cv_mse)\n\n\n[1] 0.02131863\n\n\nAs observed, the predictive strength of the model is better than the baseline model, but not by a lot in terms of predictive performance."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#xgboost",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#xgboost",
    "title": "House Pricing Project",
    "section": "XGBoost",
    "text": "XGBoost\nXGBoost is an ensemble learning method that uses repeated decision trees, where each tree is fitted on the residuals of the previous one. Essentially, it’s a sophisticated method of partitioning the dataset.\nThis approach has gained significant popularity, so I decided to give it a try. Since XGBoost relies on tree structures for partitioning, the data must have an inherent ordering. To include categorical variables, I applied one-hot encoding to transform them into a suitable format.\n\n\nCode\nlibrary(xgboost)\nlibrary(data.table)\n\n# Prepare the dataset\ndf_xgb &lt;- df\ntarget &lt;- log(df_xgb$SalePrice)  # Log-transform the target variable\ndf_xgb$SalePrice &lt;- NULL         # Remove the target variable from features\ndf_xgb_Id &lt;- df_xgb$Id\ndf_xgb$Id &lt;- NULL\n\n# One-hot encode categorical variables\ndf_xgb &lt;- as.data.table(df_xgb)\ndf_xgb &lt;- model.matrix(~ . - 1, data = df_xgb)  # Perform one-hot encoding and remove intercept\n\n# Create DMatrix\nddata &lt;- xgb.DMatrix(data = df_xgb, label = target)\n\n# Set parameters for the XGBoost model\nparams &lt;- list(\n  objective = \"reg:squarederror\",  # Regression task\n  eta = 0.1,                       # Learning rate\n  max_depth = 6,                   # Maximum depth of trees\n  subsample = 0.8,                 # Subsampling ratio\n  colsample_bytree = 0.8           # Feature subsampling ratio\n)\n\n# Perform 10-fold cross-validation\nset.seed(123)  # For reproducibility\ncv_results &lt;- xgb.cv(\n  params = params,\n  data = ddata,\n  nrounds = 100,                    # Number of boosting rounds\n  nfold = 10,                       # Number of folds for cross-validation\n  metrics = \"rmse\",                 # Evaluation metric\n  early_stopping_rounds = 10,       # Stop early if no improvement\n  print_every_n = 10,               # Print progress every 10 rounds\n  verbose = TRUE\n)\n\n\n[1] train-rmse:10.379960+0.002592   test-rmse:10.379961+0.026888 \nMultiple eval metrics are present. Will use test_rmse for early stopping.\nWill train until test_rmse hasn't improved in 10 rounds.\n\n[11]    train-rmse:3.637180+0.001203    test-rmse:3.637569+0.017285 \n[21]    train-rmse:1.286535+0.000669    test-rmse:1.287011+0.011197 \n[31]    train-rmse:0.469571+0.000640    test-rmse:0.476945+0.007929 \n[41]    train-rmse:0.190574+0.000775    test-rmse:0.215138+0.012590 \n[51]    train-rmse:0.102444+0.000572    test-rmse:0.147398+0.018300 \n[61]    train-rmse:0.075914+0.000609    test-rmse:0.132825+0.019453 \n[71]    train-rmse:0.065099+0.000603    test-rmse:0.129387+0.019381 \n[81]    train-rmse:0.057990+0.001078    test-rmse:0.128064+0.019680 \n[91]    train-rmse:0.052257+0.001140    test-rmse:0.127166+0.019725 \n[100]   train-rmse:0.047739+0.001064    test-rmse:0.126684+0.019586 \n\n\nCode\n# Extract the best number of boosting rounds\nbest_nrounds &lt;- cv_results$best_iteration\ncat(\"Best number of rounds:\", best_nrounds, \"\\n\")\n\n\nBest number of rounds: 98 \n\n\nCode\n# Train final model using the best number of rounds\nfinal_model &lt;- xgb.train(\n  params = params,\n  data = ddata,\n  nrounds = best_nrounds\n)\n\n# Evaluate final model on entire dataset (or split as desired)\npredictions &lt;- predict(final_model, ddata)\noverall_rmse &lt;- sqrt(mean((target - predictions)^2))\ncat(\"RMSE on entire dataset:\", overall_rmse, \"\\n\")\n\n\nRMSE on entire dataset: 0.05299007"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#prepration-of-test-set.",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/Housing pricing project.html#prepration-of-test-set.",
    "title": "House Pricing Project",
    "section": "Prepration of test set.",
    "text": "Prepration of test set.\nIn this section, I analyze the features in the test set. There are some missing values, which I address using the K-Nearest Neighbors (KNN) algorithm. This method is particularly effective for handling missing values in categorical variables.\n\n\nCode\nprint(\"df_test_preformence\")\n\n\n[1] \"df_test_preformence\"\n\n\nCode\ntable(df_test_preformence$MSSubClass)\n\n\n\n 20  30  45  50  60  70  75  80  90 120 150 160 180 190 \n543  70   6 145 276  68   7  88  57  95   1  65   7  31 \n\n\nCode\nprint(\"df\")\n\n\n[1] \"df\"\n\n\nCode\ntable(df$MSSubClass)\n\n\n\n 20  30  45  50  60  70  75  80  90 120 160 180 190 \n532  67  12 148 296  60  16  78  52  86  63  10  30 \n\n\n\n\nCode\ndf_test_preformence$MSSubClass[df_test_preformence$MSSubClass==150]=160\n\n\nmissing values\n\n\nCode\nvis_miss(df_test_preformence)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(VIM)\n\n\nWarning: package 'VIM' was built under R version 4.4.2\n\n\nLoading required package: colorspace\n\n\nLoading required package: grid\n\n\nVIM is ready to use.\n\n\nSuggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues\n\n\n\nAttaching package: 'VIM'\n\n\nThe following object is masked from 'package:bootstrap':\n\n    diabetes\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\n\nCode\n# Impute missing values using KNN\nimputed_data &lt;- kNN(df_test_preformence, k = 5)\n\n# the imputed_data ad a new colum to say what vaules is imputed i am remvoving them.\ndf_test_preformence&lt;-imputed_data[,1:dim(df_test_preformence)[2]]\n\n# Check the updated dataframe\ndim(df_test_preformence)\n\n\n[1] 1459   54\n\n\nTheir is one Plywood in test set\n\n\nCode\nsetdiff(unique(df$Exterior2nd),unique(df_test_preformence$Exterior2nd))\n\n\n[1] \"plywood\"\n\n\nCode\ntable(df_test_preformence$Exterior2nd)\n\n\n\nAsbShng AsphShn Brk Cmn BrkFace CemntBd CmentBd HdBoard ImStucc MetalSd Plywood \n     18       1      15      22       2      66     199       5     233     128 \n  Stone  Stucco VinylSd Wd Sdng Wd Shng \n      1      21     511     194      43 \n\n\nCode\ntable(df$Exterior2nd)\n\n\n\nAsbShng AsphShn Brk Cmn BrkFace CemntBd CmentBd HdBoard ImStucc MetalSd plywood \n     20       3       7      25       1      58     207      10     213       1 \nPlywood   Stone  Stucco VinylSd Wd Sdng Wd Shng \n    142       4      26     499     196      38 \n\n\n\n\nCode\nX_full &lt;- model.matrix(log_saleprice ~ . , data = df_lasso)\nfinal_lasso_model &lt;- glmnet(X_full, df_lasso$log_saleprice, alpha = 1, lambda = mean_best_lambda, family = \"gaussian\")\n\ndf_lasso_evalatio &lt;- df_test_preformence\ndf_lasso_evalatio_id&lt;-df_lasso_evalatio$Id\ndf_lasso_evalatio$Id&lt;-NULL\n# Create design matrix for prediction from test data\nX_predict &lt;- model.matrix(~., data = df_lasso_evalatio)\n\n# Identify any missing columns between X_full and the prediction data (X_predict)\nmissing_cols &lt;- setdiff(colnames(X_full), colnames(X_predict))\n\n# Add missing columns to X_predict with zero values (align columns)\nX_predict &lt;- cbind(X_predict, matrix(0, nrow = nrow(X_predict), ncol = length(missing_cols),\n                                     dimnames = list(NULL, missing_cols)))\n\n# Ensure column order matches X_full\nX_predict &lt;- X_predict[, colnames(X_full)]\n\n# Predict log_saleprice for the test dataset\npredictions &lt;- predict(final_lasso_model, newx = X_predict)\n\n# Convert log predictions to SalePrice scale (exp of log predictions)\nSalePrice_pred &lt;- exp(c(predictions))\n\n# Create a data frame with Id and SalePrice\nprediction_df &lt;- data.frame(Id = df_lasso_evalatio_id, SalePrice = SalePrice_pred)\n\n# Write predictions to a CSV file\n#write.csv(prediction_df, \"sale_prices.csv\", row.names = FALSE)\n\n# Check the length of SalePrice predictions\nlength(prediction_df$SalePrice)\n\n\n[1] 1459"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html",
    "title": "House Pricing Project",
    "section": "",
    "text": "This is practics case based on some data from kaggle\nI mostly made this to play around with some data and learn new methods. When working with data, like many others, I like to use either R Markdown or Jupyter Notebook. I also enjoy writing small notes and comments so that if I need to revisit the project later, I can easily see what I did. This approach is really helpful in terms of reproducibility. Unfortunately, people often share notebooks without adding any text or comments.\nWhile I’ve written out some comments here, I don’t go too deep. However, I do show parts of my workflow. The code is written in R, and I consider it what I call “one-time code.” It’s not meant to be read by others, so there aren’t a lot of comments. It’s not optimized either—I’ve simply chosen the fastest code I could think of or find to solve a problem. I ende op using Chat-gbt for some code aswell then correcting the mistakes. The methology and the models chosen is all me."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#comments-on-available-data",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#comments-on-available-data",
    "title": "House Pricing Project",
    "section": "Comments on Available Data",
    "text": "Comments on Available Data\nAll the houses are located in the Ames city area. There is information on the month and year sold. (In both the training and test sets, the years have the same values, meaning there’s no need to estimate values forward in time. This is a bit unusual—what is the purpose of the model in such a case?) Most of the data is categorical or discrete in nature. Some of the data has a natural order or rank. When examining the available data, it’s worth noting that there is no information on earlier sale prices—why is this missing? Information on “time on market” is also missing."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#data-cleaning-and-exploration",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#data-cleaning-and-exploration",
    "title": "House Pricing Project",
    "section": "Data cleaning and exploration",
    "text": "Data cleaning and exploration\n\n\nCode\nlibrary(readr)\ndf=as.data.frame(read.csv(\"train.csv\"))\n\n#split &lt;- sample(1:nrow(df), size = 0.8 * nrow(df))\n\n# Create training and testing sets\n#df &lt;- df[split, ]  # 80% of the data\n#df_test_for_choice &lt;- df[-split, ]  # other 20%\n\n#\ndf_test_preformence=as.data.frame(read.csv(\"test.csv\"))"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#recoding-na",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#recoding-na",
    "title": "House Pricing Project",
    "section": "Recoding NA",
    "text": "Recoding NA\nIn the description, I noticed that some of the variables use NA as a category.\nVaribels where NA is a catgori is listed below.\n\nAlley\nBsmtQual\nBsmtCond\nBsmtExposure\nBsmtFinType1\nBsmtFinType2\nFireplaceQu\nGarageType\nGarageFinish\nGarageQual\nGarageCond\nPoolQC\nFence\nMiscFeature\n\nThese variables need to be recoded, as the values are not missing—they are simply in the wrong format.\n\n\nCode\n#change \nchang_to_df&lt;-function(DF){\n  DF$Alley&lt;-DF$Alley[is.na(DF$Alley)] &lt;- \"No alley\"\n  DF$BsmtQual[is.na(DF$BsmtQual)]&lt;-\"No Basement\"\n  DF$BsmtCond[is.na(DF$BsmtCond)]&lt;-\"No Basement\"\n  DF$BsmtExposure[is.na(DF$BsmtExposure)]&lt;-\"No Basement\"\n  DF$BsmtFinType1[is.na(DF$BsmtFinType1)]&lt;- \"No Basement\"\n  DF$BsmtFinType2[is.na(DF$BsmtFinType2)]&lt;- \"No Basement\"\n  DF$FireplaceQu[is.na(DF$FireplaceQu)]&lt;-\"No Fireplace\"\n  DF$GarageType[is.na(DF$GarageType)]&lt;-\"No Garage\"\n  DF$GarageFinish[is.na(DF$GarageFinish)]&lt;-\"No Garage\"\n  DF$GarageQual[is.na(DF$GarageQual)]&lt;-\"No Garage\"\n  DF$GarageCond[is.na(DF$GarageCond)]&lt;-\"No Garage\"\n  DF$GarageYrBlt[is.na(DF$GarageYrBlt)]&lt;-0 # all values that is NA corespond wither other showing that their is no garage.\n  DF$PoolQC[is.na(DF$PoolQC)]&lt;-\"No Pool\"\n  DF$Fence[is.na(DF$Fence)]&lt;-\"No Fence\"\n  DF$MiscFeature[is.na(DF$MiscFeature)]&lt;-\"None\"\n  return(DF)\n}\n\ndf&lt;-chang_to_df(df)\n#df_test_for_choice&lt;-chang_to_df(df_test_for_choice)\ndf_test_preformence&lt;-chang_to_df(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#examining-missing-values",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#examining-missing-values",
    "title": "House Pricing Project",
    "section": "Examining Missing Values",
    "text": "Examining Missing Values\nAfter recoding cases where NA does not represent missing values, we will now analyze the actual missing values.\nBelow is a plot showing the missing values. I had to split the plot into two parts; otherwise, the variable names would not be readable.\n\n\nCode\nlibrary(naniar)\nvis_miss(df[,1:40])\n\n\n\n\n\n\n\n\n\nCode\nvis_miss(df[,40:81])\n\n\n\n\n\n\n\n\n\nLotFrontage missing values look weird.\nThe definition is Linear feet of street connected to property. Below i have printed uniqe entreances in LotFrontage.\n\n\nCode\ntable((df$LotFrontage))\n\n\n\n 21  24  30  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48 \n 23  19   6   5   1  10   9   6   5   1   1  12   6   4  12   9   3   1   5   6 \n 49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68 \n  4  57  15  14  10   6  17   5  12   7  13 143   8   9  17  19  44  15  12  19 \n 69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88 \n 11  70  12  17  18  15  53  11   9  25  17  69   6  12   5   9  40  10   5  10 \n 89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 \n  6  23   6  10   8   6   7   8   2   8   3  16   2   4   3   3   6   1   7   3 \n109 110 111 112 114 115 116 118 120 121 122 124 128 129 130 134 137 138 140 141 \n  2   6   1   1   2   2   2   2   7   2   2   2   1   2   3   2   1   1   1   1 \n144 149 150 152 153 160 168 174 182 313 \n  1   1   1   1   1   1   1   2   1   2 \n\n\nFrom the definition, it could refer to farms, but there are no farms in the dataset.\nIf we plot the house types against the number of missing values, we see that most of them come from single-family detached houses.\n\n\nCode\nlibrary(dplyr)\n# Test farm theory\ntest_LotFrontage &lt;- df$LotFrontage\ntest_LotFrontage[is.na(test_LotFrontage)] &lt;- 0\ntest_BldgType &lt;- factor(df$BldgType)\n\n\n\n# Create a data frame\ndata &lt;- data.frame(test_BldgType, test_LotFrontage)\n\nzero_counts &lt;- data %&gt;%\n  filter(test_LotFrontage == 0) %&gt;%\n  count(test_BldgType)\n\n# Create the bar plot with category labels at the bottom\nbarplot(zero_counts$n, \n        names.arg = zero_counts$test_BldgType,  # Use the correct column for labels\n        main = \"Number of Zeros by Category\",\n        xlab = \"Categories\",\n        ylab = \"Count of Zeros\",\n        col = \"lightblue\",\n        las = 2)  # Rotate labels to vertical\n\n\n\n\n\n\n\n\n\nIt seems like most of the missing values are from single-family detached houses, so it’s unrealistic to assume there is no street connected to the property. This means setting the values to zero is likely a bad option.\nOptions for imputation:\nLinear regression based on other variables. k-nearest neighbors. I could also disregard this variable since I doubt it has high predictive power.\nIt seems like an interactive linear model, incorporating the interaction between LotArea and LotShape, would provide good imputation (this also makes sense conceptually).\n\n\nCode\nImputasion_model &lt;- lm(LotFrontage ~ LotArea * LotShape, df)\n\n# Plot fitted values vs. residuals\nplot(Imputasion_model$fitted.values, Imputasion_model$residuals,\n     xlab = \"Fitted Values\",\n     ylab = \"Residuals\",\n     main = \"Residuals plot (LotFrontage ~ LotArea * LotShape)\")\n\n# Add a horizontal line at the mean of the residuals (which should be zero)\nabline(h = mean(Imputasion_model$residuals), col = \"red\", lwd = 2, lty = 2)\nmean_value &lt;- mean(Imputasion_model$residuals)\ntext(x = max(Imputasion_model$fitted.values), \n     y = mean_value, \n     labels = \"Mean\", \n     col = \"red\")\n\n\n\n\n\n\n\n\n\nMissing values for LotFrontage will be imputed using the linear model described above.\nThis also means that if LotFrontage is used in the model, the imputation will need to be performed on the test set as well. I have internally debated whether I should include the test set when building the imputation model—essentially, creating a model based on both the training and test sets for this purpose.\nSince I’m a little new to Kaggle, if the test set contains covariates and I only need to upload the fitted values, I would proceed with this approach. Otherwise, I would not. For now, I will just use the model based only on the training set.\n\n\nCode\n# imputatsion\nImputasion_model &lt;- lm(LotFrontage ~ LotArea * LotShape, df)\n\nInput_value&lt;-function(DF){\n  for (i in 1:length(DF$LotFrontage)){\n    if(is.na(DF$LotFrontage[i])){\n      DF$LotFrontage[i]&lt;-predicted_value &lt;- predict(Imputasion_model, newdata = list(\n        LotArea = DF$LotArea[i],\n        LotShape = DF$LotShape[i] ))\n    }\n  }\n  return(DF)\n  }\n\n#input df for all input values\ndf&lt;-Input_value(df)\n#df_test_for_choice&lt;-Input_value(df_test_for_choice)\ndf_test_preformence&lt;-Input_value(df_test_preformence)\n\n\nThere are still 1% of missing values in MasVnrArea and MasVnrType. I will discard the last row containing these missing values."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#recoding-of-variables",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#recoding-of-variables",
    "title": "House Pricing Project",
    "section": "Recoding of Variables",
    "text": "Recoding of Variables\nSome of these variables have a ranking, which makes them well-suited for recoding. By recoding them as numerical values, the ranking becomes more obvious.\nNote: This GPT is best used alongside the editGPT Browser extension.\n\n\nClick to expand/collapse varibel recoeding\n\nTo give a exampel of the recoding. PoolQC: Pool quality\n   Ex   Excellent         (4)\n   Gd   Good              (3)\n   TA   Average/Typical   (2)   (TA is never obsevered in pool varibel for traning set)\n   Fa   Fair              (1)\n   NA   No Pool           (0)\ncan be codes as 0-4\nBelowe here i have made list of how the varibels is code. I have made is so no pressent cagories NA is code to 0. Meaning 0 is only a value if the item is not pressent. and otherwise it starts from 1.\nLotShape: General shape of property Reg Regular (4) IR1 Slightly irregular (3) IR2 Moderately Irregular (2) IR3 Irregular (1)\nLandContour: Flatness of the property\n   Lvl  Near Flat/Level                                                     (4)\n   Bnk  Banked - Quick and significant rise from street grade to building   (3)\n   HLS  Hillside - Significant slope from side to side                      (2)\n   Low  Depression                                                          (1)\nLandSlope: Slope of property\n   Gtl  Gentle slope    (3)\n   Mod  Moderate Slope  (2)\n   Sev  Severe Slope    (1)\nExterQual: Evaluates the quality of the material on the exterior\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\n   \nExterCond: Evaluates the present condition of the material on the exterior\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\n   \nBsmtQual: Evaluates the height of the basement\n   Ex   Excellent (100+ inches)   (5)\n   Gd   Good (90-99 inches)       (4)\n   TA   Typical (80-89 inches)    (3)\n   Fa   Fair (70-79 inches)       (2)\n   Po   Poor (&lt;70 inches          (1)\n   NA   No Basement               (0)\nBsmtCond: Evaluates the general condition of the basement\n   Ex   Excellent                                     (5)\n   Gd   Good                                          (4)\n   TA   Typical - slight dampness allowed             (3)\n   Fa   Fair - dampness or some cracking or settling  (2)\n   Po   Poor - Severe cracking, settling, or wetness  (1)\n   NA   No Basement                                   (0)\nBsmtExposure: Refers to walkout or garden level walls\n   Gd   Good Exposure                                                               (4)\n   Av   Average Exposure (split levels or foyers typically score average or above)  (3) \n   Mn   Mimimum Exposure                                                            (2)\n   No   No Exposure                                                                 (1)\n   NA   No Basement                                                                 (0)\nBsmtFinType1: Rating of basement finished area\n   GLQ  Good Living Quarters            (6)\n   ALQ  Average Living Quarters         (5)\n   BLQ  Below Average Living Quarters     (4)\n   Rec  Average Rec Room                (3)\n   LwQ  Low Quality                     (2)\n   Unf  Unfinshed                       (1)\n   NA   No Basement                       (0)\nBsmtFinType2: Rating of basement finished area (if multiple types)\n   GLQ  Good Living Quarters            (6)\n   ALQ  Average Living Quarters         (5)\n   BLQ  Below Average Living Quarters     (4)\n   Rec  Average Rec Room                (3)\n   LwQ  Low Quality                     (2)\n   Unf  Unfinshed                       (1)\n   NA   No Basement                       (0)\nHeatingQC: Heating quality and condition\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\nKitchenQual: Kitchen quality\n   Ex   Excellent       (5)\n   Gd   Good            (4)\n   TA   Average/Typical (3)\n   Fa   Fair            (2)\n   Po   Poor            (1)\nFunctional: Home functionality (Assume typical unless deductions are warranted)\n   Typ  Typical Functionality   (7)\n   Min1 Minor Deductions 1      (6)\n   Min2 Minor Deductions 2      (5)\n   Mod  Moderate Deductions     (4)\n   Maj1 Major Deductions 1      (3)\n   Maj2 Major Deductions 2      (2)\n   Sev  Severely Damaged        (1)\n   Sal  Salvage only            (0)\n   \nFireplaceQu: Fireplace quality\n   Ex   Excellent - Exceptional Masonry Fireplace                                               (5)\n   Gd   Good - Masonry Fireplace in main level                                                  (4)\n   TA   Average - Prefabricated Fireplace in main living area or Masonry Fireplace in basement  (3)\n   Fa   Fair - Prefabricated Fireplace in basement                                              (2)\n   Po   Poor - Ben Franklin Stove                                                               (1)\n   NA   No Fireplace                                                                            (0)\nGarageFinish: Interior finish of the garage\n   Fin  Finished        (3)\n   RFn  Rough Finished  (2) \n   Unf  Unfinished      (1)\n   NA   No Garage         (0)\n   \nGarageQual: Garage quality\n   Ex   Excellent         (5)\n   Gd   Good              (4)\n   TA   Typical/Average   (3)\n   Fa   Fair              (2)\n   Po   Poor              (1)\n   NA   No Garage         (0)\nGarageCond: Garage condition\n   Ex   Excellent         (5)\n   Gd   Good              (4)\n   TA   Typical/Average   (3)\n   Fa   Fair              (2)\n   Po   Poor              (1)\n   NA   No Garage         (0)\nPoolQC: Pool quality\n   Ex   Excellent         (4)\n   Gd   Good              (3)\n   TA   Average/Typical   (2)   (TA is never obsevered in pool varibel for traning set)\n   Fa   Fair              (1)\n   NA   No Pool           (0)\n   \nFence: Fence quality\n   GdPrv    Good Privacy    (4)\n   MnPrv    Minimum Privacy (3)\n   GdWo Good Wood         (2)\n   MnWw Minimum Wood/Wire (1)\n   NA   No Fence            (0)\nTheir properly also some rank to other varibels, varibels like building matrials must have a ranking in terms of price. But i dont have any idear about whese.\n\n\n\nCode\n# Reencoding can be don via match.\n\n# recoding\nrecoding&lt;-function(DF){\n  DF$LotShape&lt;-match(DF$LotShape,c(\"IR3\",\"IR2\",\"IR1\",\"Reg\"))\n  #DF$LandContour&lt;-match(DF$LandContour,c(\"Low\",\"HLS\",\"Bnk\",\"Lvl\"))\n  #DF$LandSlope&lt;-match(DF$LandSlope,c(\"Sev\",\"Mod\",\"Gtl\"))\n  DF$ExterQual&lt;-match(DF$ExterQual,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$ExterCond&lt;-match(DF$ExterCond,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$BsmtQual&lt;-match(DF$BsmtQual,c(\"No Basement\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$BsmtCond&lt;-match(DF$BsmtCond,c(\"No Basement\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$BsmtExposure&lt;-match(DF$BsmtExposure,c(\"No Basement\",\"No\",\"Mn\",\"Av\",\"Gd\"))-1\n  DF$BsmtFinType1&lt;-match(DF$BsmtFinType1,c(\"No Basement\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"))-1\n  DF$BsmtFinType2&lt;-match(DF$BsmtFinType2,c(\"No Basement\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"))-1\n  DF$HeatingQC&lt;-match(DF$HeatingQ,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$KitchenQual&lt;-match(DF$KitchenQual,c(\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))\n  DF$Functional&lt;-match(DF$Functional,c(\"Sal\",\"Sev\",\"Maj2\",\"Maj1\",\"Mod\",\"Min2\",\"Min1\",\"Typ\"))-1\n  DF$FireplaceQu&lt;-match(DF$FireplaceQu,c(\"No Fireplace\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$GarageFinish&lt;-match(DF$GarageFinish,c(\"No Garage\",\"Unf\",\"RFn\",\"Fin\"))-1\n  DF$GarageQual&lt;-match(DF$GarageQual,c(\"No Garage\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$GarageCond&lt;-match(DF$GarageCond,c(\"No Garage\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  DF$PoolQC&lt;-match(DF$PoolQC, c(\"No Pool\",\"Fa\",\"TA\",\"Gd\",\"Ex\"))-1\n  #DF$Fence&lt;-match(DF$Fence,c(\"No Fence\",\"MnWw\",\"GdWo\",\"MnPrv\",\"GdPrv\"))-1\n  return(DF)\n}\n\ndf&lt;-recoding(df)\n#df_test_for_choice&lt;-recoding(df_test_for_choice)\ndf_test_preformence&lt;-recoding(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#feature-transformations",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#feature-transformations",
    "title": "House Pricing Project",
    "section": "Feature transformations",
    "text": "Feature transformations\nIn this section i will list the varibels i have change and what the changes was.\n\n\nClick to expand/collapse varibel Changes\n\nLooking at the description of the data the varibesl with lowe count, is\n\n40: 1-STORY W/FINISHED ATTIC ALL AGES\n45: 1-1/2 STORY - UNFINISHED ALL AGES\n75: 2-1/2 STORY ALL AGES\n85: SPLIT FOYER\n185: PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n\nHere is what i will combine them to.\n40 will be combined with 50 - 50: 1-1/2 STORY FINISHED ALL AGES Making 1-1/2 STORY FINISHED ALL AGES\n85 will be combined with 80 - 80: SPLIT OR MULTI-LEVEL Making Split\n185 will be combined with 160 - 160: 2-STORY PUD - 1946 & NEWER Making PUD - MULTILEVEL\nThis only leaves 45: UNFINISHED ALL AGES\nwith a vary small number of obsevations, in this section we looking at the data but this kind of asumption will affect models choice, what is the value of unfinished house, alle ages. A unfinish house can either be a cheap way to get new house ore a extra expens since it proberly should be removed. Both of these would proberly have lower value than the 1-1/2 STORY FINISHED ALL AGES. So if i inclued them i can drag down the estimat for this groups, and this can be unproporsional if the model is fitted with least sqaur. If i dont comined it with a varibel want to use a interaction effect in linear model I will get model wiht a lot NA, where this the combinations is not represented, i can get around this by tackling overide the model, so basically get to make geuss in whose case, that could be the average house price, something more cleaver, so for now i will leave it in. If i have to do the overird i will tell, this is better than a zero score. In the total linear model this is not a problem, so i may be the case this not problem.\n\nYearBuilt Som of this varibel is allready pressent in MSSubClass\nGarageCars I have think about, it would give nice distribution if it was seplified belove ore equalt 2 cars with true and false. but i think their is more infomation in o zeror so the the varibel vill be zero, one, two, above 2\n\nLooking at this it seam like it would be a god idear to cagaorise/colabse some of these varibel. This is to try to simplify the information. The problem that can arise is that with some of these varibels, they have such ueven distribution that, one can end up fitting a combination to uniqe house. this could be good fine but, in some case by colsaping one can better overall predictiv results. Later I will proberly reduce the dimension based on either spearman ore kendall tau corelation so one would want to ensure that if their is rank in the data is preserved. To give a exampel number of fireplace can be reduces to Yes or No. Yes would stille be higer than No.\n\nfireplace siplified to binary yes and NO\nPoolArea will be change to yes no their is not alot of information -pool yes No\n\nFor these i could it could potensiel be a good idear to eihter make fullbath total Halfbath total. ore colabs them in to yes no\n\nFullbath\nHalfbath\n\nbsmtFullBath\nBsmtHalfBath\nFenche will be change to True ore FAlSE\nSaleCondition will be normal True ore False\nLandContour will be change to level ore not. I will stil count this haveing a order, meaning level wich is equal to one is prefered\nLandSlope will be change to ground level ore not.\n\nFoundation will be combined by combin all other than “PConc” ore “CBlock” to “other”\nLotConfig all other than Corner Inside will be combined\nRoofStyle will be “Gable” or “other”\nThe one data point in MiscFeature that is tenis court will go under other\nIn Condition1 the following RRNe Within 200’ of East-West Railroad RRAe Adjacent to East-West Railroad RRNn Within 200’ of North-South Railroad RRAn Adjacent to North-South Railroad\nWill be combied to (Near Railroad)\nIn the Exterior1st and Exterior2nd. Their is some catagories with single obsevations in, I will put them in the closet cagori.\n\nI have rapt all the change in a function so it esay to aplly to the test dataset aswell\n\n\nCode\nmodify_df &lt;- function(DF){\n  DF$MSSubClass[DF$MSSubClass == 40] &lt;- 50\n  DF$MSSubClass[DF$MSSubClass == 85] &lt;- 80\n  DF$MSSubClass[DF$MSSubClass == 185] &lt;- 160\n  DF$have_Garage&lt;-DF$GarageCars&gt;0\n  DF$GarageCars&lt;-NULL\n  # is Fireplace pressent\n  DF$Fireplaces_present&lt;-DF$Fireplaces&gt;0\n  DF$Fireplaces&lt;-NULL\n  DF$FireplaceQu&lt;-NULL\n  # is pool pressent\n  DF$pool_present&lt;-DF$PoolArea&gt;0\n  DF$PoolArea&lt;-NULL\n  # Full bath pressent above ground\n  DF$FullBath_total&lt;-DF$BsmtFullBath+DF$FullBath\n  DF$BsmtFullBath&lt;-NULL\n  DF$FullBath&lt;-NULL\n  DF$HalfBath_total&lt;-DF$HalfBath+DF$BsmtHalfBath\n  DF$HalfBath&lt;-NULL\n  DF$BsmtHalfBath&lt;-NULL\n  DF$Alley&lt;-NULL\n  DF$Fence&lt;-DF$Fence!=\"No Fence\"\n  DF$SaleCondition&lt;-DF$SaleCondition==\"Normal\"\n  DF$LandContour&lt;-DF$LandContour==\"Lvl\"\n  DF$LandSlope&lt;-DF$LandSlope==\"Gtl\"\n  DF$Foundation[!(DF$Foundation %in% c(\"PConc\", \"CBlock\"))] &lt;- \"Other\"\n  DF$LotConfig[!(DF$LotConfig %in% c(\"Corner\",\"Inside\"))]&lt;-\"Other\"\n  DF$RoofStyle&lt;-DF$RoofStyle!=\"Gable\"\n  DF$MiscFeature[DF$MiscFeature==\"TenC\"]&lt;-\"Othr\"\n  DF$Condition1[DF$Condition1 %in% c(\"RRNe\",\"RRAe\",\"RRNn\",\"RRAn\")]&lt;-\"NR\"\n  DF$Exterior1st[DF$Exterior1st==\"AsphShn\"]&lt;-\"AsbShng\"\n  DF$Exterior1st[DF$Exterior1st==\"CBlock\"]&lt;-\"CemntBd\"\n  DF$Exterior1st[DF$Exterior1st==\"ImStucc\"]&lt;-\"Stucco\"\n  DF$Exterior2nd[DF$Exterior2nd==\"CBlock\"]&lt;-\"CemntBd\"\n  DF$Exterior2nd[DF$Exterior2nd==\"Other\"]&lt;-\"plywood\"\n  return(DF)\n}\n\ndf=modify_df(df)\n#df_test_for_choice&lt;-modify_df(df_test_for_choice)\ndf_test_preformence&lt;-modify_df(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#remove-varibels",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#remove-varibels",
    "title": "House Pricing Project",
    "section": "Remove varibels",
    "text": "Remove varibels\nIn this section, I will list the variables I plan to remove or disregard. The reason for their removal is that their distributions are very skewed compared to other variables, which I deem similar in the information they provide.\nThe main reason for disregarding these variables is that I do not believe they have high predictive strength, and I need to move forward with the data cleaning process. Also, I’m not getting paid for this.\n\n\nClick to expand/collapse varibel removed\n\n\nX3SsnPorch no real information will be removed (removed)\nScreenPorch no information aviable will be removed\nAlley contain no information and will be removed\nstreet contain not information and will be removed\nUtilities contain not information and will be removed\nCondition2 contain not information and will be removed\nRoofMatl contain not information and will be removed will be removed\nHeating contain not information and will be removed will be removed\nCentralAir contain not information and will be removed will be removed\nPoolQC contain not information and will be removed will be removed\nMiscFeature contain not information and will be removed will be removed\nGarageQual\nGarageCond\nGarageType\nElectrical\n\n\n\n\nCode\nremove_varibels&lt;-function(DF){\n  DF$X3SsnPorch&lt;-NULL\n  DF$ScreenPorch&lt;-NULL\n  DF$Alley&lt;-NULL\n  DF$Street&lt;-NULL\n  DF$Utilities&lt;-NULL\n  DF$Condition2&lt;-NULL\n  DF$RoofMatl&lt;-NULL\n  DF$Heating&lt;-NULL\n  DF$CentralAir&lt;-NULL\n  DF$PoolQC&lt;-NULL\n  DF$SaleType&lt;-NULL\n  DF$GarageQual&lt;-NULL\n  DF$GarageCond&lt;-NULL\n  DF$GarageType&lt;-NULL\n  DF$Electrical&lt;-NULL\n  return(DF)\n}\n\ndf=remove_varibels(df)\n#df_test_for_choice&lt;-remove_varibels(df_test_for_choice)\ndf_test_preformence&lt;-remove_varibels(df_test_preformence)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#variables-after-cleanup",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#variables-after-cleanup",
    "title": "House Pricing Project",
    "section": "Variables After Cleanup",
    "text": "Variables After Cleanup\nBelow, I have plotted the variables remaining after the cleanup.\n\n\nCode\nwrite.csv(df, \"data_after_clean.csv\", row.names = FALSE)\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(purrr)\n\nfor (col in names(df)) {\n  if (is.numeric(df[[col]])) {\n    # Numeriske variabler - Lav histogram\n    n_unique &lt;- length(unique(df[[col]]))\n    p &lt;- ggplot(df, aes_string(x = col)) + \n         geom_histogram(bins = min(n_unique, 30), fill = 'skyblue', color = 'black', alpha = 0.7) + \n         labs(title = paste('Histogram of', col), x = col, y = 'Frequency') + \n         theme_minimal()\n  } else {\n    # Ikke-numeriske variabler - Lav bar plot\n    p &lt;- ggplot(df, aes_string(x = col)) + \n         geom_bar(fill = 'orange', color = 'black', alpha = 0.7) + \n         labs(title = paste('Bar Plot of', col), x = col, y = 'Count') + \n         theme_minimal()\n  }\n  print(p)  # Vis plottet\n}\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\nlibrary(shiny)\nlibrary(ggplot2)\n\ndata_before &lt;- read.csv(\"https://raw.githubusercontent.com/Missing-almost-everywhere/Missing-almost-everywhere.io/main/Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/data_after_clean.csv\")\n\nui &lt;- fluidPage(\n  titlePanel(\"House Prices Data Visualization\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"variable\", \"Select Variable:\", choices = names(data_before))\n    ),\n    mainPanel(\n      plotOutput(\"dynamicPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$dynamicPlot &lt;- renderPlot({\n    col &lt;- input$variable\n    \n    if (is.numeric(data_before[[col]])) {\n      ggplot(data_before, aes(x = .data[[col]])) + \n        geom_histogram(bins = 30, fill = 'skyblue', color = 'black', alpha = 0.7) +\n        labs(title = paste('Histogram of', col), x = col, y = 'Frequency') +\n        theme_minimal()\n    } else {\n      ggplot(data_before, aes(x = .data[[col]])) + \n        geom_bar(fill = 'orange', color = 'black', alpha = 0.7) +\n        labs(title = paste('Bar Plot of', col), x = col, y = 'Count') +\n        theme_minimal()\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#looking-into-cross-correlation",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#looking-into-cross-correlation",
    "title": "House Pricing Project",
    "section": "Looking Into Cross-Correlation",
    "text": "Looking Into Cross-Correlation\nBelow, I have plotted the correlation matrix for the variables. Since many of these variables are categorical but have a ranking, I used Spearman correlation.\n\n\nCode\ndf=na.omit(df)\nlibrary(corrplot)\n\n\ncorrplot 0.95 loaded\n\n\nCode\nlibrary(Hmisc)\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nCode\nnumeric_df &lt;- df[sapply(na.omit(df), is.numeric)]\n\n# Compute Spearman correlation matrix\nspearman_corr &lt;- cor(numeric_df, method = \"spearman\")\n\n# Compute p-values (optional)\nlibrary(Hmisc)\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n# Plot the correlation matrix\ncorrplot(spearman_corr, method = \"color\",tl.col = \"black\", tl.srt = 60, insig = \"blank\",tl.cex = 0.5,title = \"Spearman Correlation Plot\")\n\n\n\n\n\n\n\n\n\nNotes on the Plot Overall, the internal correlation is a lot smaller than I thought, which is good. This should make it easier to find good predictors.\nThe third variable from the bottom is the sales price. From this, one can see which variables could be potential predictors. It seems like OverallQual could be a really good predictor.\nHowever, correlation only captures linear effects, so one should be cautious about relying solely on correlation for feature selection. Imagine there was a feature with a sinusoidal curve that perfectly described the data—it would have a correlation of zero."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#reducing-variables-based-on-multicollinearity",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#reducing-variables-based-on-multicollinearity",
    "title": "House Pricing Project",
    "section": "Reducing Variables Based on Multicollinearity",
    "text": "Reducing Variables Based on Multicollinearity\nIn this section, I will reduce some variables, as multicollinearity can pose challenges for certain models. While some partitioning-based models are less sensitive to multicollinearity, linear models can be significantly impacted. In a simple linear model with two inputs, the variance of a parameter can be expressed as:\n\\[\\text{Var}(\\beta_1) = \\frac{\\sigma^2}{1 - \\text{cor}(x_1, x_2)}\\]\nThis means that as the correlation between two variables increases, the variance of the estimate explodes. Consequently, certain models are sensitive to multicollinearity.\nOne approach to addressing this is to remove variables with a correlation above a certain threshold. However, determining the optimal threshold is not straightforward. For a specific model, simulations can be conducted to estimate a suitable value. In this case, I want to perform some reduction before applying any models. I have chosen a cutoff point of (|0.7|). If there is a high correlation between two variables, I will remove the one with the weaker correlation to the log of the sale price.\nSince the variables are not numeric but do have a ranking, PCA is not an option. However, Spearman correlation can still be used.\nFor variable reduction, hierarchical clustering can be applied using the absolute value of the Spearman correlations as a distance measure. Single-linkage clustering can then be employed, with a slight reformulation of the problem. Hierarchical clustering requires a distance metric, and for this purpose, the distance between two variables based on correlation can be defined as:\n\\(d(x_1, x_2) = 1 - |\\text{cor}(x_1, x_2)|\\)\nGiven the cutoff of (|0.7|) for correlation, the corresponding distance after reformulation would be:\n\\(1 - 0.7 = 0.3\\)\n\n\nCode\n# The code here is not optimized but it was fast to write\nnumeric_df &lt;- df[sapply(na.omit(df), is.numeric)]\n\nnumeric_saleprice&lt;-numeric_df$SalePrice\nnumeric_df$SalePrice&lt;-NULL # remove from dataframe\n\nlibrary(Hmisc)\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n\ndist_matrix &lt;- as.dist(1-abs(spearman_corr))\n\nhc &lt;- hclust(dist_matrix, method = \"single\")\n\n\nclusters &lt;- cutree(hc, h = 0.3) #\n\n# Visualize dendrogram\nplot(hc, main = \"Hierarchical Clustering Dendrogram\")\nabline(h = 0.3, col = \"red\") # Add cutoff line\n\n\n\n\n\n\n\n\n\nCode\n# reduce varibels\nnumeric_saleprice_corr &lt;- rcorr(as.matrix(numeric_df), numeric_saleprice, type = \"spearman\")#$r[, \"numeric_saleprice\"]\n\n\n# Step 2: Identify clusters with multiple variables\nduplicates &lt;- table(clusters)[table(clusters) &gt; 1]\nclusters_with_duplicates &lt;- names(duplicates)\n\n\nfor (cluster in clusters_with_duplicates) {\n  clusters_with_duplicates\n  n_cluster=length(names(clusters[clusters==cluster]))\n  spear_var=rep(NA,n_cluster)\n  names_cluster=names(clusters[clusters==cluster])\n  for (i in 1:n_cluster){\n    names(clusters[clusters==cluster])[i]\n    spear_var[i]=rcorr(as.matrix(numeric_df[names(clusters[clusters==cluster])[i]]),log(numeric_saleprice),type=\"spearman\")$r[1,2]\n  }\n  index_of_max &lt;- which.max(spear_var)\n  names_cluster[-index_of_max]\n  #numeric_df[names_cluster[-index_of_max]]&lt;-NULL\n  df[names_cluster[-index_of_max]]&lt;-NULL\n  #df_test_for_choice[names_cluster[-index_of_max]]&lt;-NULL\n  df_test_preformence[names_cluster[-index_of_max]]&lt;-NULL\n}\n\n\nWith the clusters found, for each cluster, the feature with the highest Spearman correlation to the log of the sale price will be chosen.\nAs a result, there was a reduction from 61 to 41 features.\n\n\nCode\nnumeric_df &lt;- df[sapply(na.omit(df), is.numeric)]\n\nnumeric_saleprice&lt;-numeric_df$SalePrice\nnumeric_df$SalePrice&lt;-NULL # remove from dataframe\n\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n\ndist_matrix &lt;- as.dist(1-abs(spearman_corr))\n\nhc &lt;- hclust(dist_matrix, method = \"single\")\n\n\nclusters &lt;- cutree(hc, h = 0.3) #\n\n# Visualize dendrogram\nplot(hc, main = \"Hierarchical Clustering Dendrogram\")\nabline(h = 0.3, col = \"red\") # Add cutoff line\n\n\n\n\n\n\n\n\n\nAs can be seen in the new dendrogram, the dataframe has been reduced to meet the requirements."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#overview-of-variables-with-the-highest-correlation-to-log-of-sale-price",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#overview-of-variables-with-the-highest-correlation-to-log-of-sale-price",
    "title": "House Pricing Project",
    "section": "Overview of Variables with the Highest Correlation to Log of Sale Price",
    "text": "Overview of Variables with the Highest Correlation to Log of Sale Price\nBelow, I have ordered the variables based on the absolute value of their Spearman correlation.\n\n\nCode\n# Required libraries\nlibrary(corrplot)\nlibrary(Hmisc)\n\n# Subset the dataframe to numeric columns\nnumeric_df$log_saleprice&lt;-log(df$SalePrice)\nnumeric_df$SalePrice&lt;-NULL\n# Ensure the target variable is present\ntarget_variable &lt;- \"log_saleprice\"\nif (!target_variable %in% colnames(numeric_df)) {\n  stop(\"Target variable not found in numeric dataframe.\")\n}\n\n# Compute Spearman correlation matrix and p-values\nres &lt;- rcorr(as.matrix(numeric_df), type = \"spearman\")\nspearman_corr &lt;- res$r    # Correlation coefficients\n\n# Extract correlation with the target variable\ntarget_corr &lt;- spearman_corr[, target_variable, drop = FALSE]\ntarget_corr &lt;- as.data.frame(target_corr)\ncolnames(target_corr) &lt;- \"Spearman_Correlation\"\n\n# Add a column indicating if the correlation could be computed\ntarget_corr$Computed &lt;- !is.na(target_corr$Spearman_Correlation)\n\n# Sort variables by Spearman correlation in descending order\nsorted_corr &lt;- target_corr[order(abs(target_corr$Spearman_Correlation), decreasing = TRUE), ]\n\n# Display sorted variables with their correlation and computed status\nprint(sorted_corr[1:10,])\n\n\n               Spearman_Correlation Computed\nlog_saleprice             1.0000000     TRUE\nOverallQual               0.8087095     TRUE\nGrLivArea                 0.7304531     TRUE\nBsmtQual                  0.6760372     TRUE\nGarageArea                0.6477811     TRUE\nFullBath_total            0.6371933     TRUE\nGarageFinish              0.6323531     TRUE\nTotalBsmtSF               0.6023901     TRUE\nYearRemodAdd              0.5688537     TRUE\nHeatingQC                 0.4906598     TRUE\n\n\nLook at the Year sold the collation is really low. the spand of the years it relly lowe, this would idicate that the price do not raise a lot over time in this area. I would have expted some kind of increase to compensate for inflation.\nBelow i have recoded the Time as months since 2006, 1 being january. I also plottet logsale price as agianst the month\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nnumeric_df$time&lt;-NA\nfor (i in 1:length(df$MoSold)){\n  numeric_df$time[i]&lt;-(numeric_df$YrSold[i]-2006)*12+numeric_df$MoSold[i]\n}\n\nyearly_means &lt;- numeric_df %&gt;%\n  group_by(time) %&gt;%\n  summarise(mean_value = mean(log_saleprice))\n\n# Plot residuals against time\nggplot(data=numeric_df[!is.na(numeric_df$log_saleprice),], aes(x=numeric_df$time, y=log_saleprice)) +\n  geom_point(alpha = 0.6, color = \"black\") +  # Individual data points\n  geom_line(data = yearly_means, aes(x = time, y = mean_value, color = \"Mean Sold Price\"), \n            size = 1, show.legend = TRUE) +\n  geom_point(data = yearly_means, aes(x = (time), y = mean_value),\n             color = \"red\", size = 2) +\n  labs(\n    x = \"Month\",\n    y = \"Value\",\n    title = \"Monthly Data with Mean Overlay\"\n  ) +\n  theme_minimal()# caluculate mean \n\n\n\n\n\n\n\n\n\nCode\nmonthly_means &lt;- numeric_df %&gt;%\n  group_by(MoSold) %&gt;%\n  summarise(mean_value = mean(log_saleprice))\n\n\n\nggplot(data=numeric_df[!is.na(numeric_df$log_saleprice),], aes(x=numeric_df$MoSold, y=log_saleprice)) +\n  geom_point(alpha = 0.6, color = \"black\") +  # Individual data points\n  geom_line(data = monthly_means, aes(x = MoSold, y = mean_value, color = \"Mean Sold Price\"), \n            size = 1, show.legend = TRUE) +\n  geom_point(data = monthly_means, aes(x = (MoSold), y = mean_value),\n             color = \"red\", size = 2) +\n  labs(\n    x = \"Month\",\n    y = \"Value\",\n    title = \"Monthly Data with Mean Overlay\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOverall their dont seam to be any big conettiction between pice and time. From the moth plot it clear if ther varinace of the price is dependen on the time ore if the salevolum changes letting to less spread."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#overallqual",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#overallqual",
    "title": "House Pricing Project",
    "section": "OverallQual",
    "text": "OverallQual\nOverallQual has high corelation with saleprice.\n\n\nCode\nggplot(data=df[!is.na(df$SalePrice),], aes(x=factor(OverallQual), y=log(SalePrice)))+\n        geom_point()+ labs(title=\"OverallQual\")\n\n\n\n\n\n\n\n\n\nlog price seam linear related to overall quality."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#ground-lvving-area",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#ground-lvving-area",
    "title": "House Pricing Project",
    "section": "Ground lvving area",
    "text": "Ground lvving area\nGround living area.\n\n\nCode\nggplot(data=df[!is.na(df$SalePrice),], aes(x=factor(GrLivArea), y=log(SalePrice)))+\n        geom_point()+ labs(title=\"GrLivArea\")"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#looking-at-some-no-numeric-varibels",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#looking-at-some-no-numeric-varibels",
    "title": "House Pricing Project",
    "section": "Looking at some no numeric varibels",
    "text": "Looking at some no numeric varibels\nSome of these varibels do not have natrual ordering, wich mean computing spearman corelation is not posibel. In that case the best option (I know) is to plot the target varibels against the cagories in the varibels, theirs differenct forms, such boxbolot ore violin plot. I have chosen violin plot since it hase more resempelens with a historgram, wich i prefere over boxplot.\n\n\nCode\nwrite.csv(df, \"data_for_final_model.csv\", row.names = FALSE)\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nnon_numeric_cols &lt;- df %&gt;%\n  select_if(~!is.numeric(.)) %&gt;%\n  colnames()\n\nfor (col in non_numeric_cols) {\n  p &lt;- ggplot(df, aes_string(x = col, y = \"log(SalePrice)\")) +  \n    geom_violin() +\n    stat_summary(fun = \"mean\", geom = \"point\", color = \"red\", size = 3) +  # Tilføj mean som rødt punkt\n    labs(title = paste(\"Violin plot for\", col, \"vs log(SalePrice)\")) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  \n  print(p)  # Udskriv violin plot for hver iteration\n}\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata_final &lt;- read.csv(\"https://raw.githubusercontent.com/Missing-almost-everywhere/Missing-almost-everywhere.io/main/Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/data_for_final_model.csv\")\n\nui &lt;- fluidPage(\n  titlePanel(\"House Price Analysis\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"variable\", \"Select Variable:\", \n                  choices = names(data_final))\n    ),\n    mainPanel(\n      plotOutput(\"dynamicPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$dynamicPlot &lt;- renderPlot({\n    col &lt;- input$variable\n    \n    if (is.numeric(data_final[[col]])) {\n      if(col != \"SalePrice\") {\n        ggplot(data_final, aes(x = .data[[col]], y = log(SalePrice))) +\n          geom_point(alpha = 0.5) +\n          geom_smooth(method = \"lm\", color = \"red\") +\n          labs(title = paste(col, \"vs log(SalePrice)\")) +\n          theme_minimal()\n      } else {\n        ggplot(data_final, aes(x = .data[[col]])) +\n          geom_histogram(bins = 30, fill = 'skyblue', color = 'black', alpha = 0.7) +\n          labs(title = paste('Histogram of', col)) +\n          theme_minimal()\n      }\n    } else {\n      ggplot(data_final, aes(x = .data[[col]], y = log(SalePrice))) +\n        geom_violin(fill = 'skyblue', alpha = 0.7) +\n        stat_summary(fun = mean, geom = \"point\", color = \"red\", size = 3) +\n        labs(title = paste(col, \"vs log(SalePrice)\")) +\n        theme_minimal() +\n        theme(axis.text.x = element_text(angle = 45, hjust = 1))\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\nSo what I am looking for is something their it seam like the means i sale price is different between the factors. Neighborhood, seam like a good varibel to include, in many intances on can see that the their is not even a overlab in the observed price.\nThe roofing seam like it could have potesial"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#base-line-mocel",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#base-line-mocel",
    "title": "House Pricing Project",
    "section": "Base line mocel",
    "text": "Base line mocel\nAfter Plotting different models and looking the residual i found the below model. Where the saleprice pr sqft of living area is combination the Neighborhood the type of dwelling (MSSubClass)\nwich could be good candidate if one look the residuals being drawn from af t distribusion. So more heavy tale distribution,that a normal distribution. In many ways this model seam resanbull from a intuition point of view.\n\n\nCode\n# Plot original data\n\nmodel_baseline &lt;-lm((SalePrice/GrLivArea) ~ TotalBsmtSF*MSSubClass:Neighborhood:GrLivArea , data = df) # lm((SalePrice / GrLivArea) ~ TotalBsmtSF+MSSubClass+Neighborhood,data=df)\n\n\nplot(df$SalePrice,log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea))\n\n\n\n\n\n\n\n\n\nCode\nhist(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea),breaks=50)\n\n\n\n\n\n\n\n\n\nCode\n# The histogram almost look like a t distribution\nlibrary(MASS)\nfitdistr(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea), \"t\",start = list(m=mean(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea)),s=sd(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea)), df=2),lower=c(-1, 0.001,1))\n\n\n        m              s              df     \n  -0.009049068    0.156394619    4.526172860 \n ( 0.004804025) ( 0.005221797) ( 0.565497406)\n\n\nCode\nlibrary(Dowd)\nTQQPlot(log(df$SalePrice)-log(model_baseline$fitted.value*df$GrLivArea),df=4.5 )"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#a-linear-interaction-model.",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#a-linear-interaction-model.",
    "title": "House Pricing Project",
    "section": "A linear interaction model.",
    "text": "A linear interaction model.\nBelove I have fitted a interaction model where log saleprice is a interactiv funtion between The type of house, the neighborhood and the size of the dewelling+ sum effect from the size of the basement. It seam reasonbell and is somewat how i would estimate a type of house for the overall price.\n\n\nCode\nModel_second_baseline&lt;- lm(log(SalePrice)~TotalBsmtSF+MSSubClass:GrLivArea:Neighborhood,data=df)\n\n\nlibrary(Dowd)\nTQQPlot(Model_second_baseline$residuals,df=4.5 )\n\n\n\n\n\n\n\n\n\nCode\n# Define the custom metric for CV evaluation\nlog_diff_metric_model_baseline &lt;- function(data, label) {\n  groups=unique(label)\n  groups_score=rep(NA,length(groups))\n  \n  for (i in 1:length(groups)){\n    groupe_for_test=groups[i]\n    partion_vector_train &lt;- which(label!= groupe_for_test)\n    partion_vector_test &lt;- which(label== groupe_for_test)\n    train_data &lt;- data[partion_vector_train, ] \n    test_data &lt;- data[partion_vector_test, ] \n    # fit model \n    model_baseline &lt;-lm(log(SalePrice)~TotalBsmtSF+MSSubClass:GrLivArea:Neighborhood,data=train_data)\n    # get predition \n    predicted_SalePrice &lt;- predict(model_baseline, newdata = test_data) \n    # get log diff\n    log_diff &lt;- log(test_data$SalePrice) - (predicted_SalePrice)\n    # save results\n    groups_score[i]&lt;-sum(log_diff**2)\n  }\n  \n  return(sum(groups_score)/length(data$SalePrice))\n}\n\n# make labels\n\nn_cv=50\n\nlabels=sample(rep(seq(1,n_cv),floor(length(df$Id)/n_cv)))\nif(length(df$Id)-floor(length(df$Id)/n_cv)*n_cv!=0){\n  labels&lt;-c(labels,seq(1:(length(df$Id)-floor(length(df$Id)/n_cv)*n_cv)))\n}\nlabels=sample(labels)\n\n\nprint(\"RMSE of log prices\")\n\n\n[1] \"RMSE of log prices\"\n\n\nCode\nprint(log_diff_metric_model_baseline(df,labels))\n\n\n[1] 0.06415234\n\n\nOver all the predicative stregth look good the reisudall of the model looks more simullaro to t distribuin than a normal."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#lasso",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#lasso",
    "title": "House Pricing Project",
    "section": "Lasso",
    "text": "Lasso\nIn this chunck of code i fit lasso model for the dataframe, by asuming a linear model over all the vearibels in the datafram penlized by the size of beta. The Lasso solves the following\n\\[\\underset{\\beta}{argmin} ||log(Y)-\\beta X||_{2}^2+\\lambda||\\beta||_1\\].\n“It penalizes the size of the coefficients (beta). Since the penalty is based on the \\(L_1\\) -norm, it tends to set some coefficients to zero. This encourages sparse solutions, where only the most relevant features are retained.\n\n\nCode\nlibrary(glmnet)\n\n# Prepare the data\ndf_lasso &lt;- df\nlog_saleprice &lt;- log(df_lasso$SalePrice)\ndf_lasso$SalePrice &lt;- NULL\ndf_lasso$log_saleprice &lt;- log_saleprice  # Add log_saleprice to df_lasso\ndf_lasso_id&lt;-df_lasso$Id\ndf_lasso$Id&lt;-NULL\n# Remove rows with NA values\ndf_lasso &lt;- na.omit(df_lasso)\n\n# Split data into folds\nk &lt;- 10\nfolds &lt;- sample(1:k, size = nrow(df_lasso), replace = TRUE)\n\n# Initialize a vector to store MSEs for each fold\ncv_mse &lt;- numeric(k)\nbest_lambda_vector &lt;- rep(NA, k)\n\nfor (i in 1:k) {\n  # Split data into training and test sets\n  train_indices &lt;- which(folds != i)\n  test_indices &lt;- which(folds == i)\n  \n  train_data &lt;- df_lasso[train_indices, ]\n  test_data &lt;- df_lasso[test_indices, ]\n  \n  # Create design matrices\n  X_train &lt;- model.matrix(log_saleprice ~ . , data = train_data)\n  X_test &lt;- model.matrix(log_saleprice ~ . , data = test_data)\n  \n  # Fix column mismatch between X_train and X_test\n  common_columns &lt;- intersect(colnames(X_train), colnames(X_test))\n  X_test &lt;- X_test[, common_columns, drop = FALSE]\n  X_train &lt;- X_train[, common_columns, drop = FALSE]\n  \n  # Fit LASSO model with cross-validation to choose lambda\n  lasso_model &lt;- cv.glmnet(X_train, train_data$log_saleprice, alpha = 1, family = \"gaussian\")\n  best_lambda &lt;- lasso_model$lambda.min\n  best_lambda_vector[i] &lt;- best_lambda\n  \n  # Predict on test data\n  predictions &lt;- predict(lasso_model, s = best_lambda, newx = X_test)\n  \n  # Compute MSE for this fold\n  cv_mse[i] &lt;- mean((test_data$log_saleprice - predictions)^2)\n}\n\n# Average MSE across all folds\nmean_cv_mse &lt;- mean(cv_mse)\ncat(\"Mean Cross-Validated MSE:\", mean_cv_mse, \"\\n\")\n\n\nMean Cross-Validated MSE: 0.02193118 \n\n\nCode\n# Fit final LASSO model with the average best lambda\nmean_best_lambda &lt;- mean(best_lambda_vector, na.rm = TRUE)  # Calculate mean lambda\nX_full &lt;- model.matrix(log_saleprice ~ . , data = df_lasso)\nfinal_lasso_model &lt;- glmnet(X_full, log_saleprice, alpha = 1, lambda = mean_best_lambda, family = \"gaussian\")\n\n# Print final model coefficients\nprint(mean_cv_mse)\n\n\n[1] 0.02193118\n\n\nAs observed, the predictive strength of the model is better than the baseline model, but not by a lot in terms of predictive performance."
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#xgboost",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#xgboost",
    "title": "House Pricing Project",
    "section": "XGBoost",
    "text": "XGBoost\nXGBoost is an ensemble learning method that uses repeated decision trees, where each tree is fitted on the residuals of the previous one. Essentially, it’s a sophisticated method of partitioning the dataset.\nThis approach has gained significant popularity, so I decided to give it a try. Since XGBoost relies on tree structures for partitioning, the data must have an inherent ordering. To include categorical variables, I applied one-hot encoding to transform them into a suitable format.\n\n\nCode\nlibrary(xgboost)\nlibrary(data.table)\n\n# Prepare the dataset\ndf_xgb &lt;- df\ntarget &lt;- log(df_xgb$SalePrice)  # Log-transform the target variable\ndf_xgb$SalePrice &lt;- NULL         # Remove the target variable from features\ndf_xgb_Id &lt;- df_xgb$Id\ndf_xgb$Id &lt;- NULL\n\n# One-hot encode categorical variables\ndf_xgb &lt;- as.data.table(df_xgb)\ndf_xgb &lt;- model.matrix(~ . - 1, data = df_xgb)  # Perform one-hot encoding and remove intercept\n\n# Create DMatrix\nddata &lt;- xgb.DMatrix(data = df_xgb, label = target)\n\n# Set parameters for the XGBoost model\nparams &lt;- list(\n  objective = \"reg:squarederror\",  # Regression task\n  eta = 0.1,                       # Learning rate\n  max_depth = 6,                   # Maximum depth of trees\n  subsample = 0.8,                 # Subsampling ratio\n  colsample_bytree = 0.8           # Feature subsampling ratio\n)\n\n# Perform 10-fold cross-validation\nset.seed(123)  # For reproducibility\ncv_results &lt;- xgb.cv(\n  params = params,\n  data = ddata,\n  nrounds = 100,                    # Number of boosting rounds\n  nfold = 10,                       # Number of folds for cross-validation\n  metrics = \"rmse\",                 # Evaluation metric\n  early_stopping_rounds = 10,       # Stop early if no improvement\n  print_every_n = 10,               # Print progress every 10 rounds\n  verbose = TRUE\n)\n\n\n[1] train-rmse:10.379960+0.002592   test-rmse:10.379961+0.026888 \nMultiple eval metrics are present. Will use test_rmse for early stopping.\nWill train until test_rmse hasn't improved in 10 rounds.\n\n[11]    train-rmse:3.637180+0.001203    test-rmse:3.637569+0.017285 \n[21]    train-rmse:1.286535+0.000669    test-rmse:1.287011+0.011197 \n[31]    train-rmse:0.469571+0.000640    test-rmse:0.476945+0.007929 \n[41]    train-rmse:0.190574+0.000775    test-rmse:0.215138+0.012590 \n[51]    train-rmse:0.102444+0.000572    test-rmse:0.147398+0.018300 \n[61]    train-rmse:0.075914+0.000609    test-rmse:0.132825+0.019453 \n[71]    train-rmse:0.065099+0.000603    test-rmse:0.129387+0.019381 \n[81]    train-rmse:0.057990+0.001078    test-rmse:0.128064+0.019680 \n[91]    train-rmse:0.052257+0.001140    test-rmse:0.127166+0.019725 \n[100]   train-rmse:0.047739+0.001064    test-rmse:0.126684+0.019586 \n\n\nCode\n# Extract the best number of boosting rounds\nbest_nrounds &lt;- cv_results$best_iteration\ncat(\"Best number of rounds:\", best_nrounds, \"\\n\")\n\n\nBest number of rounds: 98 \n\n\nCode\n# Train final model using the best number of rounds\nfinal_model &lt;- xgb.train(\n  params = params,\n  data = ddata,\n  nrounds = best_nrounds\n)\n\n# Evaluate final model on entire dataset (or split as desired)\npredictions &lt;- predict(final_model, ddata)\noverall_rmse &lt;- sqrt(mean((target - predictions)^2))\ncat(\"RMSE on entire dataset:\", overall_rmse, \"\\n\")\n\n\nRMSE on entire dataset: 0.05299007"
  },
  {
    "objectID": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#prepration-of-test-set.",
    "href": "Projects/house-prices-advanced-regression-techniqueshouse-prices-advanced-regression-techniques-data/testap.html#prepration-of-test-set.",
    "title": "House Pricing Project",
    "section": "Prepration of test set.",
    "text": "Prepration of test set.\nIn this section, I analyze the features in the test set. There are some missing values, which I address using the K-Nearest Neighbors (KNN) algorithm. This method is particularly effective for handling missing values in categorical variables.\n\n\nCode\nprint(\"df_test_preformence\")\n\n\n[1] \"df_test_preformence\"\n\n\nCode\ntable(df_test_preformence$MSSubClass)\n\n\n\n 20  30  45  50  60  70  75  80  90 120 150 160 180 190 \n543  70   6 145 276  68   7  88  57  95   1  65   7  31 \n\n\nCode\nprint(\"df\")\n\n\n[1] \"df\"\n\n\nCode\ntable(df$MSSubClass)\n\n\n\n 20  30  45  50  60  70  75  80  90 120 160 180 190 \n532  67  12 148 296  60  16  78  52  86  63  10  30 \n\n\n\n\nCode\ndf_test_preformence$MSSubClass[df_test_preformence$MSSubClass==150]=160\n\n\nmissing values\n\n\nCode\nvis_miss(df_test_preformence)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(VIM)\n\n\nWarning: package 'VIM' was built under R version 4.4.2\n\n\nLoading required package: colorspace\n\n\nLoading required package: grid\n\n\nVIM is ready to use.\n\n\nSuggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues\n\n\n\nAttaching package: 'VIM'\n\n\nThe following object is masked from 'package:bootstrap':\n\n    diabetes\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\n\nCode\n# Impute missing values using KNN\nimputed_data &lt;- kNN(df_test_preformence, k = 5)\n\n# the imputed_data ad a new colum to say what vaules is imputed i am remvoving them.\ndf_test_preformence&lt;-imputed_data[,1:dim(df_test_preformence)[2]]\n\n# Check the updated dataframe\ndim(df_test_preformence)\n\n\n[1] 1459   54\n\n\nTheir is one Plywood in test set\n\n\nCode\nsetdiff(unique(df$Exterior2nd),unique(df_test_preformence$Exterior2nd))\n\n\n[1] \"plywood\"\n\n\nCode\ntable(df_test_preformence$Exterior2nd)\n\n\n\nAsbShng AsphShn Brk Cmn BrkFace CemntBd CmentBd HdBoard ImStucc MetalSd Plywood \n     18       1      15      22       2      66     199       5     233     128 \n  Stone  Stucco VinylSd Wd Sdng Wd Shng \n      1      21     511     194      43 \n\n\nCode\ntable(df$Exterior2nd)\n\n\n\nAsbShng AsphShn Brk Cmn BrkFace CemntBd CmentBd HdBoard ImStucc MetalSd plywood \n     20       3       7      25       1      58     207      10     213       1 \nPlywood   Stone  Stucco VinylSd Wd Sdng Wd Shng \n    142       4      26     499     196      38 \n\n\n\n\nCode\nX_full &lt;- model.matrix(log_saleprice ~ . , data = df_lasso)\nfinal_lasso_model &lt;- glmnet(X_full, df_lasso$log_saleprice, alpha = 1, lambda = mean_best_lambda, family = \"gaussian\")\n\ndf_lasso_evalatio &lt;- df_test_preformence\ndf_lasso_evalatio_id&lt;-df_lasso_evalatio$Id\ndf_lasso_evalatio$Id&lt;-NULL\n# Create design matrix for prediction from test data\nX_predict &lt;- model.matrix(~., data = df_lasso_evalatio)\n\n# Identify any missing columns between X_full and the prediction data (X_predict)\nmissing_cols &lt;- setdiff(colnames(X_full), colnames(X_predict))\n\n# Add missing columns to X_predict with zero values (align columns)\nX_predict &lt;- cbind(X_predict, matrix(0, nrow = nrow(X_predict), ncol = length(missing_cols),\n                                     dimnames = list(NULL, missing_cols)))\n\n# Ensure column order matches X_full\nX_predict &lt;- X_predict[, colnames(X_full)]\n\n# Predict log_saleprice for the test dataset\npredictions &lt;- predict(final_lasso_model, newx = X_predict)\n\n# Convert log predictions to SalePrice scale (exp of log predictions)\nSalePrice_pred &lt;- exp(c(predictions))\n\n# Create a data frame with Id and SalePrice\nprediction_df &lt;- data.frame(Id = df_lasso_evalatio_id, SalePrice = SalePrice_pred)\n\n# Write predictions to a CSV file\n#write.csv(prediction_df, \"sale_prices.csv\", row.names = FALSE)\n\n# Check the length of SalePrice predictions\nlength(prediction_df$SalePrice)\n\n\n[1] 1459"
  }
]